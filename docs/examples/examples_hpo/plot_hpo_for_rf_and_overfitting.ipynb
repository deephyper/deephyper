{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Hyperparameter optimization and overfitting\n\nIn this example, you will learn how to treat the choice of a learning method as just another\nhyperparameter. We consider the [Random Forest (RF)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\nand [Gradient Boosting (GB)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\nclassifiers from [Scikit-Learn](https://scikit-learn.org/stable/) on the Airlines dataset.\n\nEach classifier has both unique and shared hyperparameters.\nWe use [ConfigSpace](https://automl.github.io/ConfigSpace/latest/), a Python package for defining conditional hyperparameters and more, to model them.\n\nBy using, the objective of hyperparameter properly, and considering hyperparameter optimization as an optimized model selection method, you will also learn how to fight overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation and imports\n\nInstalling dependencies with the `pip installation <install-pip>` is recommended. It requires **Python >= 3.10**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\npip install \"deephyper[ray] openml==0.15.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Import statements\nfrom inspect import signature\n\nimport ConfigSpace as cs\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport openml\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import check_random_state, resample\n\nfrom deephyper.analysis.hpo import plot_search_trajectory_single_objective_hpo\nfrom deephyper.evaluator import Evaluator\nfrom deephyper.evaluator.callback import TqdmCallback\nfrom deephyper.hpo import CBO, HpProblem\n\nWIDTH_PLOTS = 8\nHEIGHT_PLOTS = WIDTH_PLOTS / 1.618"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by creating a function which loads the data of interest. Here we use the [\"Airlines\" dataset from\nOpenML](https://www.openml.org/search?type=data&sort=runs&id=1169&status=active) where the task is to\npredict whether a given flight will be delayed, given the information of the scheduled departure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Loading the data\ndef load_data(\n    random_state=42,\n    verbose=False,\n    test_size=0.33,\n    valid_size=0.33,\n    categoricals_to_integers=False,\n):\n    \"\"\"Load the \"Airlines\" dataset from OpenML.\n\n    Args:\n        random_state (int, optional): A numpy `RandomState`. Defaults to 42.\n        verbose (bool, optional): Print informations about the dataset. Defaults to False.\n        test_size (float, optional): The proportion of the test dataset out of the whole data. Defaults to 0.33.\n        valid_size (float, optional): The proportion of the train dataset out of the whole data without the test data. Defaults to 0.33.\n        categoricals_to_integers (bool, optional): Convert categoricals features to integer values. Defaults to False.\n\n    Returns:\n        tuple: Numpy arrays as, `(X_train, y_train), (X_valid, y_valid), (X_test, y_test)`.\n    \"\"\"\n    random_state = (\n        np.random.RandomState(random_state) if type(random_state) is int else random_state\n    )\n\n    dataset = openml.datasets.get_dataset(\n        dataset_id=1169,\n        download_data=True,\n        download_qualities=True,\n        download_features_meta_data=True,\n    )\n\n    if verbose:\n        print(\n            f\"This is dataset '{dataset.name}', the target feature is \"\n            f\"'{dataset.default_target_attribute}'\"\n        )\n        print(f\"URL: {dataset.url}\")\n        print(dataset.description[:500])\n\n    X, y, categorical_indicator, ft_names = dataset.get_data(\n        target=dataset.default_target_attribute\n    )\n\n    # encode categoricals as integers\n    if categoricals_to_integers:\n        for ft_ind, ft_name in enumerate(ft_names):\n            if categorical_indicator[ft_ind]:\n                labenc = LabelEncoder().fit(X[ft_name])\n                X[ft_name] = labenc.transform(X[ft_name])\n                n_classes = len(labenc.classes_)\n            else:\n                n_classes = -1\n            categorical_indicator[ft_ind] = (\n                categorical_indicator[ft_ind],\n                n_classes,\n            )\n\n    X, y = X.to_numpy(), y.to_numpy()\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, shuffle=True, random_state=random_state\n    )\n\n    # relative valid_size on Train set\n    r_valid_size = valid_size / (1.0 - test_size)\n    X_train, X_valid, y_train, y_valid = train_test_split(\n        X_train,\n        y_train,\n        test_size=r_valid_size,\n        shuffle=True,\n        random_state=random_state,\n    )\n\n    return (X_train, y_train), (X_valid, y_valid), (X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we create a mapping to record the classification algorithms of interest:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "CLASSIFIERS = {\n    \"RandomForest\": RandomForestClassifier,\n    \"GradientBoosting\": GradientBoostingClassifier,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a baseline code to test the accuracy of each candidate model with its default hyperparameters:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Evaluate baseline models\ndef evaluate_baseline():\n    rs_clf = check_random_state(42)\n    rs_data = check_random_state(42)\n\n    ratio_test = 0.33\n    ratio_valid = (1 - ratio_test) * 0.33\n\n    train, valid, test = load_data(\n        random_state=rs_data,\n        test_size=ratio_test,\n        valid_size=ratio_valid,\n        categoricals_to_integers=True,\n    )\n\n    for clf_name, clf_class in CLASSIFIERS.items():\n        print(\"Scoring model:\", clf_name)\n\n        clf = clf_class(random_state=rs_clf)\n\n        clf.fit(*train)\n\n        acc_train = clf.score(*train)\n        acc_valid = clf.score(*valid)\n        acc_test = clf.score(*test)\n\n        print(f\"\\tAccuracy on Training: {acc_train:.3f}\")\n        print(f\"\\tAccuracy on Validation: {acc_valid:.3f}\")\n        print(f\"\\tAccuracy on Testing: {acc_test:.3f}\\n\")\n\n\nevaluate_baseline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The accuracy values show that the RandomForest classifier with default hyperparameters results in overfitting\nand therefore poor generalization (i.e., high accuracy on training data but not on the validation or test data).\nOn the contrary GradientBoosting does not show any sign of overfitting and has a better accuracy on the validation\nand testing set, which shows a better generalization than RandomForest (for the default hyperparameters).\n\nThen, we optimize the hyperparameters, where we seek to find the best classifier and its corresponding best hyperparameters\nto improve the accuracy on the vaidation and test data.\nWe create a ``load_subsampled_data`` function to load and return subsampled training and validation data in order to\nspeed up the evaluation of candidate models and hyperparameters:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def load_subsampled_data(verbose=0, subsample=True, random_state=None):\n    # In this case passing a random state is critical to make sure\n    # that the same data are loaded all the time and that the test set\n    # is not mixed with either the training or validation set.\n    # It is important to not avoid setting a global seed for safety reasons.\n    random_state = np.random.RandomState(random_state)\n\n    # Proportion of the test set on the full dataset\n    ratio_test = 0.33\n\n    # Proportion of the valid set on \"dataset \\ test set\"\n    # here we want the test and validation set to have same number of elements\n    ratio_valid = (1 - ratio_test) * 0.33\n\n    # The 3rd result is ignored with \"_\" because it corresponds to the test set\n    # which is not interesting for us now.\n    (X_train, y_train), (X_valid, y_valid), _ = load_data(\n        random_state=42,\n        test_size=ratio_test,\n        valid_size=ratio_valid,\n        categoricals_to_integers=True,\n    )\n\n    # Uncomment the next line if you want to sub-sample the training data to speed-up\n    # the search, \"n_samples\" controls the size of the new training data\n    if subsample:\n        X_train, y_train = resample(X_train, y_train, n_samples=int(1e4))\n\n    if verbose:\n        print(f\"X_train shape: {np.shape(X_train)}\")\n        print(f\"y_train shape: {np.shape(y_train)}\")\n        print(f\"X_valid shape: {np.shape(X_valid)}\")\n        print(f\"y_valid shape: {np.shape(y_valid)}\")\n\n    return (X_train, y_train), (X_valid, y_valid)\n\n\nprint(\"Without subsampling\")\n_ = load_subsampled_data(verbose=1, subsample=False)\nprint()\nprint(\"With subsampling\")\n_ = load_subsampled_data(verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we create a ``run`` function to train and evaluate a given hyperparameter configuration. This function has to return a scalar value (typically, a validation accuracy) that is maximized by the hyperparameter optimization algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Utility function that filters a dictionnary based on the signature of an object\ndef filter_parameters(obj, config: dict) -> dict:\n    \"\"\"Filter the incoming configuration dict based on the signature of obj.\n\n    Args:\n        obj (Callable): the object for which the signature is used.\n        config (dict): the configuration to filter.\n\n    Returns:\n        dict: the filtered configuration dict.\n    \"\"\"\n    sig = signature(obj)\n    clf_allowed_params = list(sig.parameters.keys())\n    clf_params = {(k[2:] if k.startswith(\"p:\") else k): v for k, v in config.items()}\n    clf_params = {\n        k: v\n        for k, v in clf_params.items()\n        if (k in clf_allowed_params and (v not in [\"nan\", \"NA\"]))\n    }\n    return clf_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run(job) -> float:\n    config = job.parameters.copy()\n    config[\"random_state\"] = check_random_state(42)\n\n    (X_train, y_train), (X_valid, y_valid) = load_subsampled_data(subsample=True)\n\n    clf_class = CLASSIFIERS[config[\"classifier\"]]\n\n    # keep parameters possible for the current classifier\n    config[\"n_jobs\"] = 4\n    clf_params = filter_parameters(clf_class, config)\n\n    try:  # good practice to manage the fail value yourself...\n        clf = clf_class(**clf_params)\n\n        clf.fit(X_train, y_train)\n\n        fit_is_complete = True\n    except Exception:\n        fit_is_complete = False\n\n    if fit_is_complete:\n        y_pred = clf.predict(X_valid)\n        acc = accuracy_score(y_valid, y_pred)\n    else:\n        acc = \"F_fit_failed\"\n\n    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we create the ``HpProblem`` to define the search space of hyperparameters for each model.\n\nThe first hyperparameter is ``\"classifier\"``, the selected model.\n\nThen, we use ``Condition`` and ``Forbidden`` to define constraints on the hyperparameters.\n\nDefault values are very important when adding ``Condition`` and ``Forbidden`` clauses.\nOtherwise, the creation of the problem can fail if the default configuration is not acceptable.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = HpProblem()\n\nclassifier = problem.add_hyperparameter(\n    [\"RandomForest\", \"GradientBoosting\"], \"classifier\", default_value=\"RandomForest\"\n)\n\n# For both\nproblem.add_hyperparameter((1, 1000, \"log-uniform\"), \"n_estimators\")\nproblem.add_hyperparameter((1, 50), \"max_depth\")\nproblem.add_hyperparameter((2, 10), \"min_samples_split\")\nproblem.add_hyperparameter((1, 10), \"min_samples_leaf\")\ncriterion = problem.add_hyperparameter(\n    [\"friedman_mse\", \"squared_error\", \"gini\", \"entropy\"],\n    \"criterion\",\n    default_value=\"gini\",\n)\n\n# GradientBoosting\nloss = problem.add_hyperparameter([\"log_loss\", \"exponential\"], \"loss\")\nlearning_rate = problem.add_hyperparameter((0.01, 1.0), \"learning_rate\")\nsubsample = problem.add_hyperparameter((0.01, 1.0), \"subsample\")\n\ngradient_boosting_hp = [loss, learning_rate, subsample]\nfor hp_i in gradient_boosting_hp:\n    problem.add_condition(cs.EqualsCondition(hp_i, classifier, \"GradientBoosting\"))\n\nforbidden_criterion_rf = cs.ForbiddenAndConjunction(\n    cs.ForbiddenEqualsClause(classifier, \"RandomForest\"),\n    cs.ForbiddenInClause(criterion, [\"friedman_mse\", \"squared_error\"]),\n)\nproblem.add_forbidden_clause(forbidden_criterion_rf)\n\nforbidden_criterion_gb = cs.ForbiddenAndConjunction(\n    cs.ForbiddenEqualsClause(classifier, \"GradientBoosting\"),\n    cs.ForbiddenInClause(criterion, [\"gini\", \"entropy\"]),\n)\nproblem.add_forbidden_clause(forbidden_criterion_gb)\n\nproblem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we create an ``Evaluator`` object using the ``ray`` backend to distribute the evaluation of the run-function defined previously.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator.create(\n    run,\n    method=\"ray\",\n    method_kwargs={\n        \"num_cpus_per_task\": 1,\n        \"callbacks\": [TqdmCallback()],\n    },\n)\n\nprint(\"Number of workers: \", evaluator.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, you can define a Bayesian optimization search called ``CBO`` (for Centralized Bayesian Optimization) and link to it the defined ``problem`` and ``evaluator``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_evals = 100\n\nsearch = CBO(\n    problem,\n    evaluator,\n    acq_func=\"UCBd\",\n    acq_func_optimizer=\"mixedga\",\n    acq_optimizer_freq=1,\n    multi_point_strategy=\"qUCBd\",\n    objective_scaler=\"identity\",\n    random_state=42,\n)\nresults = search.search(max_evals=max_evals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the search is over, a file named ``results.csv`` is saved in the current directory.\nThe same dataframe is returned by the ``search.search(...)`` call.\nIt contains the hyperparameters configurations evaluated during the search and their corresponding ``objective``\nvalue (i.e, validation accuracy), ``timestamp_submit`` the time when the evaluator submitted the configuration\nto be evaluated and ``timestamp_gather`` the time when the evaluator received the configuration once evaluated\n(both are relative times with respect to the creation of the ``Evaluator`` instance).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Plot results from hyperparameter optimization\nfig, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\nplot_search_trajectory_single_objective_hpo(results, mode=\"max\", ax=ax)\n_ = plt.title(\"Search Trajectory\")\n\n# Remember that these results only used a subsample of the training data!\n# The baseline with the full dataset reached about the same performance, 0.64 in validation accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can now look at the Top-3 configuration of hyperparameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results.nlargest(n=3, columns=\"objective\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us define a test to evaluate the best configuration on the training, validation and test data sets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate_config(config):\n    config[\"random_state\"] = check_random_state(42)\n\n    rs_data = check_random_state(42)\n\n    ratio_test = 0.33\n    ratio_valid = (1 - ratio_test) * 0.33\n\n    train, valid, test = load_data(\n        random_state=rs_data,\n        test_size=ratio_test,\n        valid_size=ratio_valid,\n        categoricals_to_integers=True,\n    )\n\n    print(\"Scoring model:\", config[\"p:classifier\"])\n    clf_class = CLASSIFIERS[config[\"p:classifier\"]]\n    config[\"n_jobs\"] = 4\n    clf_params = filter_parameters(clf_class, config)\n\n    clf = clf_class(**clf_params)\n\n    clf.fit(*train)\n\n    acc_train = clf.score(*train)\n    acc_valid = clf.score(*valid)\n    acc_test = clf.score(*test)\n\n    print(f\"\\tAccuracy on Training: {acc_train:.3f}\")\n    print(f\"\\tAccuracy on Validation: {acc_valid:.3f}\")\n    print(f\"\\tAccuracy on Testing: {acc_test:.3f}\")\n\n\nconfig = results.iloc[results.objective.argmax()][:-2].to_dict()\nprint(f\"Best config is:\\n {config}\")\nevaluate_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In conclusion, compared to the default configuration, we can see the accuracy improvement \nfrom 0.619 to 0.666 on test data and we can also see the reduction of overfitting between \nthe training and  the validation/test data sets. It was 0.879 training accuracy to 0.619 test accuracy \nfor baseline RandomForest). It is now 0.750 training accuracy to 0.666 test accuracy with the best \nhyperparameters that selected the RandomForest classifier.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}