
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/examples_hpo/plot_hpo_text_classification.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_examples_hpo_plot_hpo_text_classification.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_examples_hpo_plot_hpo_text_classification.py:


Hyperparameter search for text classification
=============================================

**Author(s)**: Romain Egele, Brett Eiffert.

 
In this tutorial we present how to use hyperparameter optimization on a text classification analysis example from the Pytorch documentation.
 
**Reference**:
This tutorial is based on materials from the Pytorch Documentation: [Text classification with the torchtext library](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)

.. GENERATED FROM PYTHON SOURCE LINES 15-21

.. code-block:: bash

    %%bash
    pip install deephyper
    pip install ray
    pip install torch torchtext torchdata

.. GENERATED FROM PYTHON SOURCE LINES 24-26

Imports
~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 28-44

.. code-block:: Python

    import ray
    import json
    import pandas as pd
    from functools import partial

    import torch

    from torchtext.data.utils import get_tokenizer
    from torchtext.data.functional import to_map_style_dataset
    from torchtext.vocab import build_vocab_from_iterator

    from torch.utils.data import DataLoader
    from torch.utils.data.dataset import random_split

    from torch import nn





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    A module that was compiled using NumPy 1.x cannot be run in
    NumPy 2.2.3 as it may crash. To support both 1.x and 2.x
    versions of NumPy, modules must be compiled with NumPy 2.0.
    Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

    If you are a user of the module, the easiest solution will be to
    downgrade to 'numpy<2' or try to upgrade the affected module.
    We expect that some modules will need time to support NumPy 2.

    Traceback (most recent call last):  File "/Users/35e/mamba/envs/deephyper_dev/bin/sphinx-build", line 8, in <module>
        sys.exit(main())
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx/cmd/build.py", line 491, in main
        return make_mode.run_make_mode(argv[1:])
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx/cmd/make_mode.py", line 223, in run_make_mode
        return make.run_generic_build(builder_name)
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx/cmd/make_mode.py", line 206, in run_generic_build
        return build_main(args + self.opts)
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx/cmd/build.py", line 414, in build_main
        app = Sphinx(
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx/application.py", line 332, in __init__
        self._init_builder()
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx/application.py", line 414, in _init_builder
        self.events.emit('builder-inited')
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx/events.py", line 404, in emit
        results.append(listener.handler(self.app, *args))
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py", line 806, in generate_gallery_rst
        ) = generate_dir_rst(src_dir, target_dir, gallery_conf, seen_backrefs)
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py", line 606, in generate_dir_rst
        results = parallel(
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py", line 607, in <genexpr>
        p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py", line 1374, in generate_file_rst
        output_blocks, time_elapsed = execute_script(
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py", line 1192, in execute_script
        execute_code_block(
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py", line 1048, in execute_code_block
        is_last_expr, mem_max = _exec_and_get_memory(
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py", line 876, in _exec_and_get_memory
        mem_max, _ = call_memory(
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py", line 1725, in _sg_call_memory_noop
        return 0.0, func()
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py", line 794, in __call__
        exec(self.code, self.fake_main.__dict__)
      File "/Users/35e/Projects/DeepHyper/deephyper/examples/examples_hpo/plot_hpo_text_classification.py", line 33, in <module>
        import torch
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
        from .functional import *  # noqa: F403
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
        import torch.nn.functional as F
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
        from .modules import *  # noqa: F403
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
        from .transformer import TransformerEncoder, TransformerDecoder, \
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
        device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
    /Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)
      device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),




.. GENERATED FROM PYTHON SOURCE LINES 45-46

.. note::The following can be used to detect if <b>CUDA</b> devices are available on the current host. Therefore, this notebook will automatically adapt the parallel execution based on the ressources available locally. However, it will not be the case if many compute nodes are requested.

.. GENERATED FROM PYTHON SOURCE LINES 48-51

.. code-block:: Python

    is_gpu_available = torch.cuda.is_available()
    n_gpus = torch.cuda.device_count()








.. GENERATED FROM PYTHON SOURCE LINES 52-57

The dataset
~~~~~~~~~~~ 

The torchtext library provides a few raw dataset iterators, which yield the raw text strings. For example, the :code:`AG_NEWS` dataset iterators yield the raw data as a tuple of label and text. It has four labels (1 : World 2 : Sports 3 : Business 4 : Sci/Tec).


.. GENERATED FROM PYTHON SOURCE LINES 59-71

.. code-block:: Python

    from torchtext.datasets import AG_NEWS

    def load_data(train_ratio):
        train_iter, test_iter = AG_NEWS()
        train_dataset = to_map_style_dataset(train_iter)
        test_dataset = to_map_style_dataset(test_iter)
        num_train = int(len(train_dataset) * train_ratio)
        split_train, split_valid = \
            random_split(train_dataset, [num_train, len(train_dataset) - num_train])

        return split_train, split_valid, test_dataset








.. GENERATED FROM PYTHON SOURCE LINES 72-94

Preprocessing pipelines and Batch generation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in
factory function :code:`build_vocab_from_iterator` which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the
vocabulary.

The vocabulary block converts a list of tokens into integers.

.. code-block:: python
vocab(['here', 'is', 'an', 'example'])
>>> [475, 21, 30, 5286]


The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. The label pipeline converts the label into integers. For example,

.. code-block:: python
text_pipeline('here is the an example')
>>> [475, 21, 2, 30, 5286]
label_pipeline('10')
>>> 9


.. GENERATED FROM PYTHON SOURCE LINES 96-125

.. code-block:: Python

    train_iter = AG_NEWS(split='train')
    num_class = 4

    tokenizer = get_tokenizer('basic_english')

    def yield_tokens(data_iter):
        for _, text in data_iter:
            yield tokenizer(text)

    vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=["<unk>"])
    vocab.set_default_index(vocab["<unk>"])
    vocab_size = len(vocab)

    text_pipeline = lambda x: vocab(tokenizer(x))
    label_pipeline = lambda x: int(x) - 1


    def collate_batch(batch, device):
        label_list, text_list, offsets = [], [], [0]
        for (_label, _text) in batch:
            label_list.append(label_pipeline(_label))
            processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
            text_list.append(processed_text)
            offsets.append(processed_text.size(0))
        label_list = torch.tensor(label_list, dtype=torch.int64)
        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
        text_list = torch.cat(text_list)
        return label_list.to(device), text_list.to(device), offsets.to(device)








.. GENERATED FROM PYTHON SOURCE LINES 126-128

.. note:: The :code:`collate_fn` function works on a batch of samples generated from :code:`DataLoader`. The input to :code:`collate_fn` is a batch of data with the batch size in :code:`DataLoader`, and :code:`collate_fn` processes them according to the data processing pipelines declared previously.


.. GENERATED FROM PYTHON SOURCE LINES 130-134

Define the model
~~~~~~~~~~~~~~~~

The model is composed of the `nn.EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>`_ layer plus a linear layer for the classification purpose.

.. GENERATED FROM PYTHON SOURCE LINES 136-154

.. code-block:: Python

    class TextClassificationModel(nn.Module):

        def __init__(self, vocab_size, embed_dim, num_class):
            super(TextClassificationModel, self).__init__()
            self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)
            self.fc = nn.Linear(embed_dim, num_class)
            self.init_weights()

        def init_weights(self):
            initrange = 0.5
            self.embedding.weight.data.uniform_(-initrange, initrange)
            self.fc.weight.data.uniform_(-initrange, initrange)
            self.fc.bias.data.zero_()

        def forward(self, text, offsets):
            embedded = self.embedding(text, offsets)
            return self.fc(embedded)








.. GENERATED FROM PYTHON SOURCE LINES 155-157

Define functions to train the model and evaluate results.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 159-181

.. code-block:: Python

    def train(model, criterion, optimizer, dataloader):
        model.train()

        for _, (label, text, offsets) in enumerate(dataloader):
            optimizer.zero_grad()
            predicted_label = model(text, offsets)
            loss = criterion(predicted_label, label)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
            optimizer.step()

    def evaluate(model, dataloader):
        model.eval()
        total_acc, total_count = 0, 0

        with torch.no_grad():
            for _, (label, text, offsets) in enumerate(dataloader):
                predicted_label = model(text, offsets)
                total_acc += (predicted_label.argmax(1) == label).sum().item()
                total_count += label.size(0)
        return total_acc/total_count








.. GENERATED FROM PYTHON SOURCE LINES 182-192

Define the run-function
~~~~~~~~~~~~~~~~~~~~~~~ 

The run-function defines how the objective that we want to maximize is computed. It takes a :code:`config` dictionary as input and often returns a scalar value that we want to maximize. The :code:`config` contains a sample value of hyperparameters that we want to tune. In this example we will search for:

* :code:`num_epochs` (default value: :code:`10`)
* :code:`batch_size` (default value: :code:`64`)
* :code:`learning_rate` (default value: :code:`5`)

A hyperparameter value can be acessed easily in the dictionary through the corresponding key, for example :code:`config["units"]`.

.. GENERATED FROM PYTHON SOURCE LINES 194-219

.. code-block:: Python

    def get_run(train_ratio=0.95):
      def run(config: dict):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        embed_dim = 64
    
        collate_fn = partial(collate_batch, device=device)
        split_train, split_valid, _ = load_data(train_ratio)
        train_dataloader = DataLoader(split_train, batch_size=int(config["batch_size"]),
                                    shuffle=True, collate_fn=collate_fn)
        valid_dataloader = DataLoader(split_valid, batch_size=int(config["batch_size"]),
                                    shuffle=True, collate_fn=collate_fn)

        model = TextClassificationModel(vocab_size, int(embed_dim), num_class).to(device)
      
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.SGD(model.parameters(), lr=config["learning_rate"])

        for _ in range(1, int(config["num_epochs"]) + 1):
            train(model, criterion, optimizer, train_dataloader)
    
        accu_test = evaluate(model, valid_dataloader)
        return accu_test
      return run








.. GENERATED FROM PYTHON SOURCE LINES 220-221

We create two versions of :code:`run`, one quicker to evaluate for the seacrh, with a small training dataset, and another one, for performance evaluation, which uses a normal training/validation ratio.

.. GENERATED FROM PYTHON SOURCE LINES 223-226

.. code-block:: Python

    quick_run = get_run(train_ratio=0.3)
    perf_run = get_run(train_ratio=0.95)








.. GENERATED FROM PYTHON SOURCE LINES 227-230

.. note:: The objective maximised by DeepHyper is the scalar value returned by the :code:`run`-function.

In this tutorial it corresponds to the validation accuracy of the model after training.

.. GENERATED FROM PYTHON SOURCE LINES 232-242

Define the Hyperparameter optimization problem
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

Hyperparameter ranges are defined using the following syntax:

* Discrete integer ranges are generated from a tuple :code:`(lower: int, upper: int)`
* Continuous prarameters are generated from a tuple :code:`(lower: float, upper: float)`
* Categorical or nonordinal hyperparameter ranges can be given as a list of possible values :code:`[val1, val2, ...]`

We provide the default configuration of hyperparameters as a starting point of the problem.

.. GENERATED FROM PYTHON SOURCE LINES 244-257

.. code-block:: Python

    from deephyper.hpo import HpProblem

    problem = HpProblem()

    # Discrete hyperparameter (sampled with uniform prior)
    problem.add_hyperparameter((5, 20), "num_epochs", default_value=10)

    # Discrete and Real hyperparameters (sampled with log-uniform)
    problem.add_hyperparameter((8, 512, "log-uniform"), "batch_size", default_value=64)
    problem.add_hyperparameter((0.1, 10, "log-uniform"), "learning_rate", default_value=5)

    problem





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Configuration space object:
      Hyperparameters:
        batch_size, Type: UniformInteger, Range: [8, 512], Default: 64, on log-scale
        learning_rate, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.0, on log-scale
        num_epochs, Type: UniformInteger, Range: [5, 20], Default: 10




.. GENERATED FROM PYTHON SOURCE LINES 258-262

Evaluate a default configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We evaluate the performance of the default set of hyperparameters provided in the Pytorch tutorial.

.. GENERATED FROM PYTHON SOURCE LINES 262-279

.. code-block:: Python


    #We launch the Ray run-time and execute the `run` function
    #with the default configuration
    if is_gpu_available:
        if not(ray.is_initialized()):
            ray.init(num_cpus=n_gpus, num_gpus=n_gpus, log_to_driver=False)
    
        run_default = ray.remote(num_cpus=1, num_gpus=1)(perf_run)
        objective_default = ray.get(run_default.remote(problem.default_configuration))
    else:
        if not(ray.is_initialized()):
            ray.init(num_cpus=1, log_to_driver=False)
        run_default = perf_run
        objective_default = run_default(problem.default_configuration)

    print(f"Accuracy Default Configuration:  {objective_default:.3f}")



.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "/Users/35e/Projects/DeepHyper/deephyper/examples/examples_hpo/plot_hpo_text_classification.py", line 275, in <module>
        objective_default = run_default(problem.default_configuration)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/Users/35e/Projects/DeepHyper/deephyper/examples/examples_hpo/plot_hpo_text_classification.py", line 213, in run
        train(model, criterion, optimizer, train_dataloader)
      File "/Users/35e/Projects/DeepHyper/deephyper/examples/examples_hpo/plot_hpo_text_classification.py", line 167, in train
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
      File "/Users/35e/mamba/envs/deephyper_dev/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py", line 55, in clip_grad_norm_
        norms.extend(torch._foreach_norm(grads, norm_type))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    NotImplementedError: Could not run 'aten::_foreach_norm.Scalar' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_norm.Scalar' is only available for these backends: [CPU, MPS, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

    CPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:31357 [kernel]
    MPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:75 [backend fallback]
    Meta: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]
    BackendSelect: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
    Python: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]
    FuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]
    Functionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback]
    Named: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]
    Conjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]
    Negative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]
    ZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]
    ADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]
    AutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    AutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]
    Tracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:17346 [kernel]
    AutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:378 [backend fallback]
    AutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:244 [backend fallback]
    FuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]
    BatchedNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]
    FuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
    Batched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]
    VmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
    FuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]
    PythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]
    FuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]
    PreDispatch: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]
    PythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]





.. GENERATED FROM PYTHON SOURCE LINES 280-286

Define the evaluator object
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

The :code:`Evaluator` object allows to change the parallelization backend used by DeepHyper.  
It is a standalone object which schedules the execution of remote tasks. All evaluators needs a :code:`run_function` to be instantiated.  
Then a keyword :code:`method` defines the backend (e.g., :code:`"ray"`) and the :code:`method_kwargs` corresponds to keyword arguments of this chosen :code:`method`.

.. GENERATED FROM PYTHON SOURCE LINES 288-290

.. code-block:: python
  evaluator = Evaluator.create(run_function, method, method_kwargs)

.. GENERATED FROM PYTHON SOURCE LINES 292-295

Once created the :code:`evaluator.num_workers` gives access to the number of available parallel workers.

Finally, to submit and collect tasks to the evaluator one just needs to use the following interface:

.. GENERATED FROM PYTHON SOURCE LINES 297-303

.. code-block:: python
  configs = [...]
  evaluator.submit(configs)
  ...
  tasks_done = evaluator.get("BATCH", size=1) # For asynchronous
  tasks_done = evaluator.get("ALL") # For batch synchronous

.. GENERATED FROM PYTHON SOURCE LINES 305-306

.. warning:: Each `Evaluator` saves its own state, therefore it is crucial to create a new evaluator when launching a fresh search.

.. GENERATED FROM PYTHON SOURCE LINES 308-338

.. code-block:: Python

    from deephyper.evaluator import Evaluator
    from deephyper.evaluator.callback import TqdmCallback

    def get_evaluator(run_function):
        # Default arguments for Ray: 1 worker and 1 worker per evaluation
        method_kwargs = {
            "num_cpus": 1, 
            "num_cpus_per_task": 1,
            "callbacks": [TqdmCallback()]
        }

        # If GPU devices are detected then it will create 'n_gpus' workers
        # and use 1 worker for each evaluation
        if is_gpu_available:
            method_kwargs["num_cpus"] = n_gpus
            method_kwargs["num_gpus"] = n_gpus
            method_kwargs["num_cpus_per_task"] = 1
            method_kwargs["num_gpus_per_task"] = 1

        evaluator = Evaluator.create(
            run_function, 
            method="ray", 
            method_kwargs=method_kwargs
        )
        print(f"Created new evaluator with {evaluator.num_workers} worker{'s' if evaluator.num_workers > 1 else ''} and config: {method_kwargs}", )
    
        return evaluator

    evaluator_1 = get_evaluator(quick_run)


.. GENERATED FROM PYTHON SOURCE LINES 339-343

Define and run the Centralized Bayesian Optimization search (CBO)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

We create the CBO using the :code:`problem` and :code:`evaluator` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 345-347

.. code-block:: Python

    from deephyper.hpo import CBO


.. GENERATED FROM PYTHON SOURCE LINES 348-349

Instanciate the search with the problem and a specific evaluator

.. GENERATED FROM PYTHON SOURCE LINES 349-351

.. code-block:: Python

    search = CBO(problem, evaluator_1)


.. GENERATED FROM PYTHON SOURCE LINES 352-357

.. note:: All DeepHyper's search algorithm have two stopping criteria:

* :code:`max_evals (int)`: Defines the maximum number of evaluations that we want to perform. Default to :code:`-1` for an infinite number.
* :code:`timeout (int)`: Defines a time budget (in seconds) before stopping the search. Default to :code:`None` for an infinite time budget.


.. GENERATED FROM PYTHON SOURCE LINES 359-361

.. code-block:: Python

    results = search.search(max_evals=30)


.. GENERATED FROM PYTHON SOURCE LINES 362-368

The returned `results` is a Pandas Dataframe where columns are hyperparameters and information stored by the evaluator:

* :code:`job_id` is a unique identifier corresponding to the order of creation of tasks
* :code:`objective` is the value returned by the run-function
* :code:`timestamp_submit` is the time (in seconds) when the hyperparameter configuration was submitted by the :code:`Evaluator` relative to the creation of the evaluator.
* code:`timestamp_gather` is the time (in seconds) when the hyperparameter configuration was collected by the :code:`Evaluator` relative to the creation of the evaluator.

.. GENERATED FROM PYTHON SOURCE LINES 370-372

.. code-block:: Python

    results


.. GENERATED FROM PYTHON SOURCE LINES 373-377

Evaluate the best configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now that the search is over, let us print the best configuration found during this run and evaluate it on the full training dataset.

.. GENERATED FROM PYTHON SOURCE LINES 379-389

.. code-block:: Python

    i_max = results.objective.argmax()
    best_config = results.iloc[i_max][:-3].to_dict()
    best_config = {k[2:]: v for k, v in best_config.items() if k.startswith("p:")}

    print(f"The default configuration has an accuracy of {objective_default:.3f}. \n" 
          f"The best configuration found by DeepHyper has an accuracy {results['objective'].iloc[i_max]:.3f}, \n" 
          f"finished after {results['timestamp_gather'].iloc[i_max]:.2f} secondes of search.\n")

    print(json.dumps(best_config, indent=4))


.. GENERATED FROM PYTHON SOURCE LINES 390-394

.. code-block:: Python

    objective_best = perf_run(best_config)
    print(f"Accuracy Best Configuration:  {objective_best:.3f}")




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 15.129 seconds)


.. _sphx_glr_download_examples_examples_hpo_plot_hpo_text_classification.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_hpo_text_classification.ipynb <plot_hpo_text_classification.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_hpo_text_classification.py <plot_hpo_text_classification.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_hpo_text_classification.zip <plot_hpo_text_classification.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
