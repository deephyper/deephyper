
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/examples_hpo/plot_hpo_text_classification_with_stopper.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_examples_hpo_plot_hpo_text_classification_with_stopper.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_examples_hpo_plot_hpo_text_classification_with_stopper.py:


Hyperparameter Search for Text Classification with the Stopper Class
====================================================================

**Author(s)**: Romain Egele, Brett Eiffert.

 In this example, we will edit the DeepHyper Hyperparameter Search for Text Classification example to use the deephyper.stopper class. The Stopper class is 
 used to check if training per job/evaluation can be ended early and save run time if the stopper algorithm determines that
 no more training is needed. Read more about the Stopper class `here <https://deephyper.readthedocs.io/en/stable/_autosummary/deephyper.stopper.html>`_
 
**Reference**:
This example is based on materials from the Pytorch Documentation: `Text classification with the torchtext library <https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html>`_

.. GENERATED FROM PYTHON SOURCE LINES 16-20

.. code-block:: bash

    %%bash
    pip install deephyper ray numpy==1.26.4 torch torchtext==0.17.2 torchdata==0.7.1 'portalocker>=2.0.0'

.. GENERATED FROM PYTHON SOURCE LINES 23-27

Imports
-------

All imports used in the tutorial are declared at the top of the file.

.. GENERATED FROM PYTHON SOURCE LINES 27-47

.. dropdown:: Code (Imports)

    .. code-block:: Python


        from deephyper.evaluator import RunningJob

        import ray
        import json
        from functools import partial

        import torch

        from torchtext.data.utils import get_tokenizer
        from torchtext.data.functional import to_map_style_dataset
        from torchtext.vocab import build_vocab_from_iterator
        from torchtext.datasets import AG_NEWS

        from torch.utils.data import DataLoader
        from torch.utils.data.dataset import random_split

        from torch import nn








.. GENERATED FROM PYTHON SOURCE LINES 48-51

.. note::
  The following can be used to detect if **CUDA** devices are available on the current host. Therefore, this notebook will automatically adapt the parallel execution based on the resources available locally. However, it will not be the case if many compute nodes are requested.


.. GENERATED FROM PYTHON SOURCE LINES 53-54

If GPU is available, this code will enabled the tutorial to use the GPU for pytorch operations.

.. GENERATED FROM PYTHON SOURCE LINES 55-60

.. dropdown:: Code (Code to check if using CPU or GPU)

    .. code-block:: Python


        is_gpu_available = torch.cuda.is_available()
        n_gpus = torch.cuda.device_count()








.. GENERATED FROM PYTHON SOURCE LINES 61-66

The dataset
-----------

The torchtext library provides a few raw dataset iterators, which yield the raw text strings. For example, the :code:`AG_NEWS` dataset iterators yield the raw data as a tuple of label and text. It has four labels (1 : World 2 : Sports 3 : Business 4 : Sci/Tec).


.. GENERATED FROM PYTHON SOURCE LINES 66-84

.. dropdown:: Code (Loading the data)

    .. code-block:: Python


        def load_data(train_ratio, fast=False):
            train_iter, test_iter = AG_NEWS()
            train_dataset = to_map_style_dataset(train_iter)
            test_dataset = to_map_style_dataset(test_iter)
            num_train = int(len(train_dataset) * train_ratio)
            split_train, split_valid = \
                random_split(train_dataset, [num_train, len(train_dataset) - num_train])
    
            ## downsample
            if fast:
                split_train, _ = random_split(split_train, [int(len(split_train)*.05), int(len(split_train)*.95)])
                split_valid, _ = random_split(split_valid, [int(len(split_valid)*.05), int(len(split_valid)*.95)])
                test_dataset, _ = random_split(test_dataset, [int(len(test_dataset)*.05), int(len(test_dataset)*.95)])

            return split_train, split_valid, test_dataset








.. GENERATED FROM PYTHON SOURCE LINES 85-93

Preprocessing pipelines and Batch generation
--------------------------------------------

Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in
factory function :code:`build_vocab_from_iterator` which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the
vocabulary.

The vocabulary block converts a list of tokens into integers.

.. GENERATED FROM PYTHON SOURCE LINES 95-99

.. code-block:: python

  vocab(['here', 'is', 'an', 'example'])
  >>> [475, 21, 30, 5286]

.. GENERATED FROM PYTHON SOURCE LINES 101-102

The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. The label pipeline converts the label into integers. For example,

.. GENERATED FROM PYTHON SOURCE LINES 104-110

.. code-block:: python

  text_pipeline('here is the an example')
  >>> [475, 21, 2, 30, 5286]
  label_pipeline('10')
  >>> 9 

.. GENERATED FROM PYTHON SOURCE LINES 110-141

.. dropdown:: Code (Code to tokenize and build vocabulary for text processing)

    .. code-block:: Python


        train_iter = AG_NEWS(split='train')
        num_class = 4

        tokenizer = get_tokenizer('basic_english')

        def yield_tokens(data_iter):
            for _, text in data_iter:
                yield tokenizer(text)

        vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=["<unk>"])
        vocab.set_default_index(vocab["<unk>"])
        vocab_size = len(vocab)

        text_pipeline = lambda x: vocab(tokenizer(x))
        label_pipeline = lambda x: int(x) - 1


        def collate_batch(batch, device):
            label_list, text_list, offsets = [], [], [0]
            for (_label, _text) in batch:
                label_list.append(label_pipeline(_label))
                processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
                text_list.append(processed_text)
                offsets.append(processed_text.size(0))
            label_list = torch.tensor(label_list, dtype=torch.int64)
            offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
            text_list = torch.cat(text_list)
            return label_list.to(device), text_list.to(device), offsets.to(device)








.. GENERATED FROM PYTHON SOURCE LINES 142-144

.. note:: The :code:`collate_fn` function works on a batch of samples generated from :code:`DataLoader`. The input to :code:`collate_fn` is a batch of data with the batch size in :code:`DataLoader`, and :code:`collate_fn` processes them according to the data processing pipelines declared previously.


.. GENERATED FROM PYTHON SOURCE LINES 146-150

Define the model
----------------

The model is composed of the `nn.EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>`_ layer plus a linear layer for the classification purpose.

.. GENERATED FROM PYTHON SOURCE LINES 150-170

.. dropdown:: Code (Defining the Text Classification model)

    .. code-block:: Python


        class TextClassificationModel(nn.Module):

            def __init__(self, vocab_size, embed_dim, num_class):
                super().__init__()
                self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)
                self.fc = nn.Linear(embed_dim, num_class)
                self.init_weights()

            def init_weights(self):
                initrange = 0.5
                self.embedding.weight.data.uniform_(-initrange, initrange)
                self.fc.weight.data.uniform_(-initrange, initrange)
                self.fc.bias.data.zero_()

            def forward(self, text, offsets):
                embedded = self.embedding(text, offsets)
                return self.fc(embedded)








.. GENERATED FROM PYTHON SOURCE LINES 171-173

Define functions to train the model and evaluate results.
---------------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 173-197

.. dropdown:: Code (Define the training and evaluation of the Text Classification model)

    .. code-block:: Python


        def train(model, criterion, optimizer, dataloader):
            model.train()

            for _, (label, text, offsets) in enumerate(dataloader):
                optimizer.zero_grad()
                predicted_label = model(text, offsets)
                loss = criterion(predicted_label, label)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
                optimizer.step()

        def evaluate(model, dataloader):
            model.eval()
            total_acc, total_count = 0, 0

            with torch.no_grad():
                for _, (label, text, offsets) in enumerate(dataloader):
                    predicted_label = model(text, offsets)
                    total_acc += (predicted_label.argmax(1) == label).sum().item()
                    total_count += label.size(0)
            return total_acc/total_count








.. GENERATED FROM PYTHON SOURCE LINES 198-213

Define the run-function
-----------------------

The run-function defines how the objective that we want to maximize is computed. It takes a :code:`config` dictionary as input and often returns a scalar value that we want to maximize. The :code:`config` contains a sample value of hyperparameters that we want to tune. In this example we will search for:

* :code:`num_epochs` (default value: :code:`10`)
* :code:`batch_size` (default value: :code:`64`)
* :code:`learning_rate` (default value: :code:`5`)

A hyperparameter value can be accessed easily in the dictionary through the corresponding key, for example :code:`config["units"]`.

When a Stopper is defined and set as a parameter in a search (below :code:`CBO()``), 
the run function must invoke methods :code:`job.record()` and :code:`job.stopped()`. 
:code:`job.record()` tells the Stopper which values to watch so it knows to stop 
and then :code:`job.stopped()` is a state the stopper uses to exit the specific job in the search earlier than expected.

.. GENERATED FROM PYTHON SOURCE LINES 213-245

.. code-block:: Python


    def get_run(train_ratio=0.95):
      def run(job: RunningJob):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        embed_dim = 64
        num_epochs = 100
    
        collate_fn = partial(collate_batch, device=device)
        split_train, split_valid, _ = load_data(train_ratio, fast=True) # set fast=false for longer running, more accurate example
        train_dataloader = DataLoader(split_train, batch_size=int(job["batch_size"]),
                                    shuffle=True, collate_fn=collate_fn)
        valid_dataloader = DataLoader(split_valid, batch_size=int(job["batch_size"]),
                                    shuffle=True, collate_fn=collate_fn)

        model = TextClassificationModel(vocab_size, int(embed_dim), num_class).to(device)
      
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.SGD(model.parameters(), lr=job["learning_rate"])

        accu_list = []
        for i in range(1, num_epochs + 1):
            train(model, criterion, optimizer, train_dataloader)
            accu_list.append(evaluate(model, valid_dataloader))
            job.record(budget = i + 1, objective=evaluate(model, valid_dataloader))
            if job.stopped():
                break
    
        accu_test = evaluate(model, valid_dataloader)
        return {"objective": accu_test, "metadata": {"index_stopped": i, "accu_list": accu_list}}
      return run








.. GENERATED FROM PYTHON SOURCE LINES 246-247

We create two versions of :code:`run`, one quicker to evaluate for the search, with a small training dataset, and another one, for performance evaluation, which uses a normal training/validation ratio.

.. GENERATED FROM PYTHON SOURCE LINES 249-252

.. code-block:: Python

    quick_run = get_run(train_ratio=0.3)
    perf_run = get_run(train_ratio=0.95)








.. GENERATED FROM PYTHON SOURCE LINES 253-256

.. note:: The objective maximised by DeepHyper is the scalar value returned by the :code:`run`-function.

In this tutorial it corresponds to the validation accuracy of the model after training.

.. GENERATED FROM PYTHON SOURCE LINES 258-268

Define the Hyperparameter optimization problem
---------------------------------------------- 

Hyperparameter ranges are defined using the following syntax:

* Discrete integer ranges are generated from a tuple :code:`(lower: int, upper: int)`
* Continuous prarameters are generated from a tuple :code:`(lower: float, upper: float)`
* Categorical or nonordinal hyperparameter ranges can be given as a list of possible values :code:`[val1, val2, ...]`

We provide the default configuration of hyperparameters as a starting point of the problem.

.. GENERATED FROM PYTHON SOURCE LINES 270-280

.. code-block:: Python

    from deephyper.hpo import HpProblem

    problem = HpProblem()

    # Discrete and Real hyperparameters (sampled with log-uniform)
    problem.add_hyperparameter((8, 512, "log-uniform"), "batch_size", default_value=64)
    problem.add_hyperparameter((0.1, 10, "log-uniform"), "learning_rate", default_value=5)

    problem





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Configuration space object:
      Hyperparameters:
        batch_size, Type: UniformInteger, Range: [8, 512], Default: 64, on log-scale
        learning_rate, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.0, on log-scale




.. GENERATED FROM PYTHON SOURCE LINES 281-285

Evaluate a default configuration
--------------------------------

We evaluate the performance of the default set of hyperparameters provided in the Pytorch tutorial.

.. GENERATED FROM PYTHON SOURCE LINES 285-305

.. dropdown:: Code (Imports)

    .. code-block:: Python


        #We launch the Ray run-time and execute the `run` function
        #with the default configuration

        if is_gpu_available:
            if not(ray.is_initialized()):
                ray.init(num_cpus=n_gpus, num_gpus=n_gpus, log_to_driver=False)
    
            run_default = ray.remote(num_cpus=1, num_gpus=1)(perf_run)
            objective_default = ray.get(run_default.remote(RunningJob(parameters=problem.default_configuration)))
        else:
            if not(ray.is_initialized()):
                ray.init(num_cpus=1, log_to_driver=False)
            run_default = perf_run
            objective_default = run_default(RunningJob(parameters=problem.default_configuration))
            print(problem.default_configuration)

        print(f"Accuracy Default Configuration:  {objective_default["objective"]:.3f}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    2025-08-18 14:34:14,988 INFO worker.py:1843 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265 
    {'batch_size': 64, 'learning_rate': 5.0}
    Accuracy Default Configuration:  0.850




.. GENERATED FROM PYTHON SOURCE LINES 306-312

Define the evaluator object
---------------------------

The :code:`Evaluator` object allows to change the parallelization backend used by DeepHyper.  
It is a standalone object which schedules the execution of remote tasks. All evaluators needs a :code:`run_function` to be instantiated.  
Then a keyword :code:`method` defines the backend (e.g., :code:`"ray"`) and the :code:`method_kwargs` corresponds to keyword arguments of this chosen :code:`method`.

.. GENERATED FROM PYTHON SOURCE LINES 314-317

.. code-block:: python

  evaluator = Evaluator.create(run_function, method, method_kwargs)

.. GENERATED FROM PYTHON SOURCE LINES 319-322

Once created the :code:`evaluator.num_workers` gives access to the number of available parallel workers.

Finally, to submit and collect tasks to the evaluator one just needs to use the following interface:

.. GENERATED FROM PYTHON SOURCE LINES 324-331

.. code-block:: python

 	configs = [...]
 	evaluator.submit(configs)
	...
	tasks_done = evaluator.get("BATCH", size=1) # For asynchronous
	tasks_done = evaluator.get("ALL") # For batch synchronous

.. GENERATED FROM PYTHON SOURCE LINES 333-334

.. warning:: Each `Evaluator` saves its own state, therefore it is crucial to create a new evaluator when launching a fresh search.

.. GENERATED FROM PYTHON SOURCE LINES 334-367

.. dropdown:: Code (Imports)

    .. code-block:: Python


        from deephyper.evaluator import Evaluator
        from deephyper.evaluator.callback import TqdmCallback


        def get_evaluator(run_function):
            # Default arguments for Ray: 1 worker and 1 worker per evaluation
            method_kwargs = {
                "num_cpus": 1, 
                "num_cpus_per_task": 1,
                "callbacks": [TqdmCallback()]
            }

            # If GPU devices are detected then it will create 'n_gpus' workers
            # and use 1 worker for each evaluation
            if is_gpu_available:
                method_kwargs["num_cpus"] = n_gpus
                method_kwargs["num_gpus"] = n_gpus
                method_kwargs["num_cpus_per_task"] = 1
                method_kwargs["num_gpus_per_task"] = 1

            evaluator = Evaluator.create(
                run_function, 
                method="ray", 
                method_kwargs=method_kwargs
            )
            print(f"Created new evaluator with {evaluator.num_workers} worker{'s' if evaluator.num_workers > 1 else ''} and config: {method_kwargs}", )
    
            return evaluator

        evaluator = get_evaluator(quick_run)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Created new evaluator with 1 worker and config: {'num_cpus': 1, 'num_cpus_per_task': 1, 'callbacks': [<deephyper.evaluator.callback.TqdmCallback object at 0x13e0e7da0>]}




.. GENERATED FROM PYTHON SOURCE LINES 368-374

Define and run the Centralized Bayesian Optimization search (CBO)
-----------------------------------------------------------------

We create the CBO using the :code:`problem` and :code:`evaluator` defined above.

A Stopper is also defined and passed as an argument to the CBO. This Stopper controls the :code:`job.observe()` and :code:`job.stopped()` functions.

.. GENERATED FROM PYTHON SOURCE LINES 376-379

.. code-block:: Python

    from deephyper.hpo import CBO
    from deephyper.stopper import SuccessiveHalvingStopper








.. GENERATED FROM PYTHON SOURCE LINES 380-381

Instantiate the search with the problem and a specific evaluator

.. GENERATED FROM PYTHON SOURCE LINES 381-384

.. code-block:: Python

    stopper = SuccessiveHalvingStopper(min_steps=1, max_steps=100)
    search = CBO(problem, stopper=stopper, log_dir="stopper-log-files")








.. GENERATED FROM PYTHON SOURCE LINES 385-390

.. note:: 
  All DeepHyper's search algorithm have two stopping criteria:
      * :code:`max_evals (int)`: Defines the maximum number of evaluations that we want to perform. Default to :code:`-1` for an infinite number.
      * :code:`timeout (int)`: Defines a time budget (in seconds) before stopping the search. Default to :code:`None` for an infinite time budget.


.. GENERATED FROM PYTHON SOURCE LINES 392-394

.. code-block:: Python

    results = search.search(evaluator, max_evals=30)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/30 [00:00<?, ?it/s]      3%|▎         | 1/30 [00:00<00:00, 3880.02it/s, failures=0, objective=0.806]      7%|▋         | 2/30 [00:00<00:11,  2.49it/s, failures=0, objective=0.806]        7%|▋         | 2/30 [00:00<00:11,  2.49it/s, failures=0, objective=0.806]     10%|█         | 3/30 [00:01<00:14,  1.80it/s, failures=0, objective=0.806]     10%|█         | 3/30 [00:01<00:14,  1.80it/s, failures=0, objective=0.806]     13%|█▎        | 4/30 [00:21<03:22,  7.77s/it, failures=0, objective=0.806]     13%|█▎        | 4/30 [00:21<03:22,  7.77s/it, failures=0, objective=0.811]     17%|█▋        | 5/30 [00:22<02:15,  5.42s/it, failures=0, objective=0.811]     17%|█▋        | 5/30 [00:22<02:15,  5.42s/it, failures=0, objective=0.811]     20%|██        | 6/30 [00:23<01:34,  3.93s/it, failures=0, objective=0.811]     20%|██        | 6/30 [00:23<01:34,  3.93s/it, failures=0, objective=0.811]     23%|██▎       | 7/30 [00:24<01:09,  3.03s/it, failures=0, objective=0.811]     23%|██▎       | 7/30 [00:24<01:09,  3.03s/it, failures=0, objective=0.811]     27%|██▋       | 8/30 [00:25<00:53,  2.42s/it, failures=0, objective=0.811]     27%|██▋       | 8/30 [00:25<00:53,  2.42s/it, failures=0, objective=0.811]     30%|███       | 9/30 [00:31<01:15,  3.60s/it, failures=0, objective=0.811]     30%|███       | 9/30 [00:31<01:15,  3.60s/it, failures=0, objective=0.811]     33%|███▎      | 10/30 [00:32<00:55,  2.79s/it, failures=0, objective=0.811]     33%|███▎      | 10/30 [00:32<00:55,  2.79s/it, failures=0, objective=0.811]     37%|███▋      | 11/30 [00:34<00:44,  2.35s/it, failures=0, objective=0.811]     37%|███▋      | 11/30 [00:34<00:44,  2.35s/it, failures=0, objective=0.811]     40%|████      | 12/30 [00:35<00:34,  1.93s/it, failures=0, objective=0.811]     40%|████      | 12/30 [00:35<00:34,  1.93s/it, failures=0, objective=0.811]     43%|████▎     | 13/30 [00:36<00:28,  1.71s/it, failures=0, objective=0.811]     43%|████▎     | 13/30 [00:36<00:28,  1.71s/it, failures=0, objective=0.811]     47%|████▋     | 14/30 [00:37<00:23,  1.47s/it, failures=0, objective=0.811]     47%|████▋     | 14/30 [00:37<00:23,  1.47s/it, failures=0, objective=0.811]     50%|█████     | 15/30 [00:38<00:20,  1.35s/it, failures=0, objective=0.811]     50%|█████     | 15/30 [00:38<00:20,  1.35s/it, failures=0, objective=0.811]     53%|█████▎    | 16/30 [00:39<00:18,  1.30s/it, failures=0, objective=0.811]     53%|█████▎    | 16/30 [00:39<00:18,  1.30s/it, failures=0, objective=0.811]     57%|█████▋    | 17/30 [00:40<00:15,  1.19s/it, failures=0, objective=0.811]     57%|█████▋    | 17/30 [00:40<00:15,  1.19s/it, failures=0, objective=0.811]     60%|██████    | 18/30 [00:41<00:13,  1.12s/it, failures=0, objective=0.811]     60%|██████    | 18/30 [00:41<00:13,  1.12s/it, failures=0, objective=0.811]     63%|██████▎   | 19/30 [00:42<00:11,  1.09s/it, failures=0, objective=0.811]     63%|██████▎   | 19/30 [00:42<00:11,  1.09s/it, failures=0, objective=0.811]     67%|██████▋   | 20/30 [00:43<00:10,  1.06s/it, failures=0, objective=0.811]     67%|██████▋   | 20/30 [00:43<00:10,  1.06s/it, failures=0, objective=0.811]     70%|███████   | 21/30 [00:44<00:09,  1.10s/it, failures=0, objective=0.811]     70%|███████   | 21/30 [00:44<00:09,  1.10s/it, failures=0, objective=0.811]     73%|███████▎  | 22/30 [00:45<00:08,  1.08s/it, failures=0, objective=0.811]     73%|███████▎  | 22/30 [00:45<00:08,  1.08s/it, failures=0, objective=0.811]     77%|███████▋  | 23/30 [00:46<00:07,  1.07s/it, failures=0, objective=0.811]     77%|███████▋  | 23/30 [00:46<00:07,  1.07s/it, failures=0, objective=0.811]     80%|████████  | 24/30 [00:47<00:06,  1.06s/it, failures=0, objective=0.811]     80%|████████  | 24/30 [00:47<00:06,  1.06s/it, failures=0, objective=0.811]     83%|████████▎ | 25/30 [00:48<00:05,  1.05s/it, failures=0, objective=0.811]     83%|████████▎ | 25/30 [00:48<00:05,  1.05s/it, failures=0, objective=0.811]     87%|████████▋ | 26/30 [00:49<00:04,  1.06s/it, failures=0, objective=0.811]     87%|████████▋ | 26/30 [00:49<00:04,  1.06s/it, failures=0, objective=0.811]     90%|█████████ | 27/30 [01:18<00:28,  9.34s/it, failures=0, objective=0.811]     90%|█████████ | 27/30 [01:18<00:28,  9.34s/it, failures=0, objective=0.816]     93%|█████████▎| 28/30 [02:05<00:41, 20.76s/it, failures=0, objective=0.816]     93%|█████████▎| 28/30 [02:05<00:41, 20.76s/it, failures=0, objective=0.816]     97%|█████████▋| 29/30 [02:09<00:15, 15.70s/it, failures=0, objective=0.816]     97%|█████████▋| 29/30 [02:09<00:15, 15.70s/it, failures=0, objective=0.816]    100%|██████████| 30/30 [02:10<00:00, 11.31s/it, failures=0, objective=0.816]    100%|██████████| 30/30 [02:10<00:00, 11.31s/it, failures=0, objective=0.816]    100%|██████████| 30/30 [02:10<00:00,  4.36s/it, failures=0, objective=0.816]




.. GENERATED FROM PYTHON SOURCE LINES 395-401

The returned :code:`results` is a Pandas Dataframe where columns are hyperparameters and information stored by the evaluator:

* :code:`job_id` is a unique identifier corresponding to the order of creation of tasks
* :code:`objective` is the value returned by the run-function
* :code:`timestamp_submit` is the time (in seconds) when the hyperparameter configuration was submitted by the :code:`Evaluator` relative to the creation of the evaluator.
* :code:`timestamp_gather` is the time (in seconds) when the hyperparameter configuration was collected by the :code:`Evaluator` relative to the creation of the evaluator.

.. GENERATED FROM PYTHON SOURCE LINES 403-406

Show results. As shown by the :code:`index_stopped` column, even there were 100 epochs per job, not all jobs used all 100 epochs.
The power of a Stopper is shown as it can reduce runtime significantly as the Stopper and jobs become "smart" and decide to end early
because the Stopper algorithm determined it was unnecessary to move forward in the search for that job.

.. GENERATED FROM PYTHON SOURCE LINES 406-409

.. code-block:: Python

    results







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>p:batch_size</th>
          <th>p:learning_rate</th>
          <th>objective</th>
          <th>job_id</th>
          <th>job_status</th>
          <th>m:timestamp_submit</th>
          <th>m:index_stopped</th>
          <th>m:accu_list</th>
          <th>m:timestamp_gather</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>89</td>
          <td>0.936261</td>
          <td>0.806429</td>
          <td>0</td>
          <td>DONE</td>
          <td>0.873358</td>
          <td>99</td>
          <td>[0.29, 0.33, 0.3597619047619048, 0.38928571428...</td>
          <td>24.567496</td>
        </tr>
        <tr>
          <th>1</th>
          <td>462</td>
          <td>0.127722</td>
          <td>0.256667</td>
          <td>1</td>
          <td>DONE</td>
          <td>24.590855</td>
          <td>1</td>
          <td>[0.25666666666666665]</td>
          <td>25.379413</td>
        </tr>
        <tr>
          <th>2</th>
          <td>68</td>
          <td>0.248208</td>
          <td>0.273333</td>
          <td>2</td>
          <td>DONE</td>
          <td>25.389952</td>
          <td>1</td>
          <td>[0.2733333333333333]</td>
          <td>26.149187</td>
        </tr>
        <tr>
          <th>3</th>
          <td>122</td>
          <td>6.620931</td>
          <td>0.811190</td>
          <td>3</td>
          <td>DONE</td>
          <td>26.160535</td>
          <td>99</td>
          <td>[0.34, 0.4742857142857143, 0.48095238095238096...</td>
          <td>46.049277</td>
        </tr>
        <tr>
          <th>4</th>
          <td>14</td>
          <td>0.455865</td>
          <td>0.290952</td>
          <td>4</td>
          <td>DONE</td>
          <td>46.059753</td>
          <td>1</td>
          <td>[0.29095238095238096]</td>
          <td>47.064833</td>
        </tr>
        <tr>
          <th>5</th>
          <td>226</td>
          <td>5.175847</td>
          <td>0.271429</td>
          <td>5</td>
          <td>DONE</td>
          <td>47.294174</td>
          <td>1</td>
          <td>[0.2714285714285714]</td>
          <td>47.999485</td>
        </tr>
        <tr>
          <th>6</th>
          <td>143</td>
          <td>6.786172</td>
          <td>0.382619</td>
          <td>6</td>
          <td>DONE</td>
          <td>48.232269</td>
          <td>2</td>
          <td>[0.38976190476190475, 0.38261904761904764]</td>
          <td>49.135595</td>
        </tr>
        <tr>
          <th>7</th>
          <td>122</td>
          <td>5.489615</td>
          <td>0.309048</td>
          <td>7</td>
          <td>DONE</td>
          <td>49.501916</td>
          <td>1</td>
          <td>[0.30904761904761907]</td>
          <td>50.226595</td>
        </tr>
        <tr>
          <th>8</th>
          <td>82</td>
          <td>6.303539</td>
          <td>0.805714</td>
          <td>8</td>
          <td>DONE</td>
          <td>50.455649</td>
          <td>26</td>
          <td>[0.3788095238095238, 0.4830952380952381, 0.447...</td>
          <td>56.438881</td>
        </tr>
        <tr>
          <th>9</th>
          <td>118</td>
          <td>6.322114</td>
          <td>0.328571</td>
          <td>9</td>
          <td>DONE</td>
          <td>56.670742</td>
          <td>1</td>
          <td>[0.32857142857142857]</td>
          <td>57.395846</td>
        </tr>
        <tr>
          <th>10</th>
          <td>121</td>
          <td>6.612655</td>
          <td>0.451905</td>
          <td>10</td>
          <td>DONE</td>
          <td>57.824365</td>
          <td>2</td>
          <td>[0.36428571428571427, 0.4519047619047619]</td>
          <td>58.737630</td>
        </tr>
        <tr>
          <th>11</th>
          <td>89</td>
          <td>1.066949</td>
          <td>0.309762</td>
          <td>11</td>
          <td>DONE</td>
          <td>58.977306</td>
          <td>1</td>
          <td>[0.30976190476190474]</td>
          <td>59.708176</td>
        </tr>
        <tr>
          <th>12</th>
          <td>78</td>
          <td>5.803714</td>
          <td>0.342381</td>
          <td>12</td>
          <td>DONE</td>
          <td>59.936309</td>
          <td>2</td>
          <td>[0.39976190476190476, 0.3423809523809524]</td>
          <td>60.896947</td>
        </tr>
        <tr>
          <th>13</th>
          <td>129</td>
          <td>6.773631</td>
          <td>0.321190</td>
          <td>13</td>
          <td>DONE</td>
          <td>61.127136</td>
          <td>1</td>
          <td>[0.3211904761904762]</td>
          <td>61.837705</td>
        </tr>
        <tr>
          <th>14</th>
          <td>81</td>
          <td>9.214344</td>
          <td>0.264762</td>
          <td>14</td>
          <td>DONE</td>
          <td>62.157754</td>
          <td>1</td>
          <td>[0.26476190476190475]</td>
          <td>62.898223</td>
        </tr>
        <tr>
          <th>15</th>
          <td>82</td>
          <td>6.868379</td>
          <td>0.395476</td>
          <td>15</td>
          <td>DONE</td>
          <td>63.133662</td>
          <td>2</td>
          <td>[0.3545238095238095, 0.3954761904761905]</td>
          <td>64.081785</td>
        </tr>
        <tr>
          <th>16</th>
          <td>189</td>
          <td>0.941383</td>
          <td>0.242857</td>
          <td>16</td>
          <td>DONE</td>
          <td>64.311886</td>
          <td>1</td>
          <td>[0.24285714285714285]</td>
          <td>65.019591</td>
        </tr>
        <tr>
          <th>17</th>
          <td>153</td>
          <td>3.548180</td>
          <td>0.306429</td>
          <td>17</td>
          <td>DONE</td>
          <td>65.249049</td>
          <td>1</td>
          <td>[0.30642857142857144]</td>
          <td>65.967033</td>
        </tr>
        <tr>
          <th>18</th>
          <td>39</td>
          <td>0.796331</td>
          <td>0.265476</td>
          <td>18</td>
          <td>DONE</td>
          <td>66.196620</td>
          <td>1</td>
          <td>[0.2654761904761905]</td>
          <td>66.990159</td>
        </tr>
        <tr>
          <th>19</th>
          <td>82</td>
          <td>0.926323</td>
          <td>0.268571</td>
          <td>19</td>
          <td>DONE</td>
          <td>67.222382</td>
          <td>1</td>
          <td>[0.26857142857142857]</td>
          <td>67.968843</td>
        </tr>
        <tr>
          <th>20</th>
          <td>82</td>
          <td>6.288300</td>
          <td>0.459524</td>
          <td>20</td>
          <td>DONE</td>
          <td>68.235045</td>
          <td>2</td>
          <td>[0.38571428571428573, 0.4595238095238095]</td>
          <td>69.176954</td>
        </tr>
        <tr>
          <th>21</th>
          <td>124</td>
          <td>6.642127</td>
          <td>0.325952</td>
          <td>21</td>
          <td>DONE</td>
          <td>69.492247</td>
          <td>1</td>
          <td>[0.32595238095238094]</td>
          <td>70.213134</td>
        </tr>
        <tr>
          <th>22</th>
          <td>82</td>
          <td>6.306742</td>
          <td>0.338810</td>
          <td>22</td>
          <td>DONE</td>
          <td>70.529433</td>
          <td>1</td>
          <td>[0.3388095238095238]</td>
          <td>71.268248</td>
        </tr>
        <tr>
          <th>23</th>
          <td>89</td>
          <td>0.936794</td>
          <td>0.303333</td>
          <td>23</td>
          <td>DONE</td>
          <td>71.572120</td>
          <td>1</td>
          <td>[0.30333333333333334]</td>
          <td>72.305645</td>
        </tr>
        <tr>
          <th>24</th>
          <td>90</td>
          <td>0.934473</td>
          <td>0.283810</td>
          <td>24</td>
          <td>DONE</td>
          <td>72.597020</td>
          <td>1</td>
          <td>[0.2838095238095238]</td>
          <td>73.326124</td>
        </tr>
        <tr>
          <th>25</th>
          <td>83</td>
          <td>6.303264</td>
          <td>0.287619</td>
          <td>25</td>
          <td>DONE</td>
          <td>73.668412</td>
          <td>1</td>
          <td>[0.2876190476190476]</td>
          <td>74.405132</td>
        </tr>
        <tr>
          <th>26</th>
          <td>34</td>
          <td>6.297783</td>
          <td>0.816429</td>
          <td>26</td>
          <td>DONE</td>
          <td>74.632590</td>
          <td>99</td>
          <td>[0.465, 0.5776190476190476, 0.5757142857142857...</td>
          <td>103.068003</td>
        </tr>
        <tr>
          <th>27</th>
          <td>10</td>
          <td>8.341522</td>
          <td>0.813333</td>
          <td>27</td>
          <td>DONE</td>
          <td>103.303074</td>
          <td>80</td>
          <td>[0.55, 0.7361904761904762, 0.7595238095238095,...</td>
          <td>150.467412</td>
        </tr>
        <tr>
          <th>28</th>
          <td>19</td>
          <td>9.167084</td>
          <td>0.790238</td>
          <td>28</td>
          <td>DONE</td>
          <td>150.792730</td>
          <td>8</td>
          <td>[0.4533333333333333, 0.638095238095238, 0.745,...</td>
          <td>154.363692</td>
        </tr>
        <tr>
          <th>29</th>
          <td>35</td>
          <td>5.982289</td>
          <td>0.311429</td>
          <td>29</td>
          <td>DONE</td>
          <td>154.608246</td>
          <td>1</td>
          <td>[0.31142857142857144]</td>
          <td>155.419572</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 410-416

Visualizing the Stopper
-----------------------
This graph shows the same information as described above but in a visual form.
Each of the 30 jobs and the rate at which they learned against the validation dataset is shown here. 
As shown above, not all job lines will show 100 epochs because the Stopper determined the jobs did not 
need to run the full time to converge on a solution.

.. GENERATED FROM PYTHON SOURCE LINES 418-434

.. code-block:: Python

    import numpy as np
    import matplotlib.pyplot as plt

    i = 0
    for row in results.iterrows():
        y = row[1]["m:accu_list"]
        x = np.arange(i+1, i+1+len(y))
        plt.plot(x, y, label=row[1]["job_id"])
        i += len(y)

    plt.xlabel('Epoch')
    plt.ylabel('Validation accuracy')
    plt.title("Validation Accuracies during training")

    plt.show()




.. image-sg:: /examples/examples_hpo/images/sphx_glr_plot_hpo_text_classification_with_stopper_001.png
   :alt: Validation Accuracies during training
   :srcset: /examples/examples_hpo/images/sphx_glr_plot_hpo_text_classification_with_stopper_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 435-439

Evaluate the best configuration
-------------------------------

Now that the search is over, let us print the best configuration found during this run and evaluate it on the full training dataset.

.. GENERATED FROM PYTHON SOURCE LINES 441-443

Show the job with best configuration and compare this with the graph above. The result of the comparison should be intuitive -
the job with the best objective in the graph should match :code:`i_max`.

.. GENERATED FROM PYTHON SOURCE LINES 443-446

.. code-block:: Python

    i_max = results.objective.argmax()
    i_max





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    26



.. GENERATED FROM PYTHON SOURCE LINES 447-456

.. code-block:: Python

    best_config = results.iloc[i_max][:-3].to_dict()
    best_config = {k[2:]: v for k, v in best_config.items() if k.startswith("p:")}

    print(f"The default configuration has an accuracy of {objective_default["objective"]:.3f}. \n" 
          f"The best configuration found by DeepHyper has an accuracy {results['objective'].iloc[i_max]:.3f}, \n" 
          f"finished after {results['m:timestamp_gather'].iloc[i_max]:.2f} seconds of search.\n")

    print(json.dumps(best_config, indent=4))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The default configuration has an accuracy of 0.850. 
    The best configuration found by DeepHyper has an accuracy 0.816, 
    finished after 103.07 seconds of search.

    {
        "batch_size": 34,
        "learning_rate": 6.297783280051977
    }




.. GENERATED FROM PYTHON SOURCE LINES 457-459

.. code-block:: Python

    objective_best = perf_run(RunningJob(parameters=best_config))
    print(f"Accuracy Best Configuration:  {objective_best["objective"]:.3f}")




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Accuracy Best Configuration:  0.857





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (4 minutes 34.607 seconds)


.. _sphx_glr_download_examples_examples_hpo_plot_hpo_text_classification_with_stopper.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_hpo_text_classification_with_stopper.ipynb <plot_hpo_text_classification_with_stopper.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_hpo_text_classification_with_stopper.py <plot_hpo_text_classification_with_stopper.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_hpo_text_classification_with_stopper.zip <plot_hpo_text_classification_with_stopper.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
