{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Mutli-Objective Black-Box Optimization\n\nIn this tutorial, we will explore how to run black-box multi-objective optimization (MOO). In this setting, the goal is to resolve the following problem:\n\n\\begin{align}\\text{max}_x (f_0(x), f_1(x), ..., f_n(x))\\end{align}\n\nwhere $x$ is the set of optimized variables and $f_i$ are the different objectives. In DeepHyper, we use scalarization to transform such multi-objective problem into a single-objective problem:\n\n\\begin{align}\\text{max}_x s_w((f_0(x), f_1(x), ..., f_n(x)))\\end{align}\n\nwhere $w$ is a set of weights which manages the trade-off between objectives and $s_w : \\mathbb{R}^n \\rightarrow \\mathbb{R}$. The weight vector $w$ is randomized and re-sampled for each new batch of suggestion from the optimizer.\n\nWe will look at the DTLZ benchmark suite, a classic in multi-objective optimization (MOO) litterature. This benchmark exibit some characteristic cases of MOO. By default, this tutorial is loading the DTLZ-II benchmark which exibit a Pareto-Front with a concave shape.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation and imports\n\nInstalling dependencies with the `pip installation <install-pip>` is recommended. It requires **Python >= 3.10**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\npip install deephyper\npip install -e \"git+https://github.com/deephyper/benchmark.git@main#egg=deephyper-benchmark\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Import statements\nimport os\n\nimport matplotlib.pyplot as plt\n\nWIDTH_PLOTS = 8\nHEIGHT_PLOTS = WIDTH_PLOTS / 1.618\n\nn_objectives = 2\n\n# Configuration of the DTLZ Benchmark\nos.environ[\"DEEPHYPER_BENCHMARK_DTLZ_PROB\"] = str(2)\nos.environ[\"DEEPHYPER_BENCHMARK_NDIMS\"] = str(8)\nos.environ[\"DEEPHYPER_BENCHMARK_NOBJS\"] = str(n_objectives)\nos.environ[\"DEEPHYPER_BENCHMARK_DTLZ_OFFSET\"] = str(0.6)\nos.environ[\"DEEPHYPER_BENCHMARK_FAILURES\"] = str(0)\n\n# Loading the DTLZ Benchmark\nimport deephyper_benchmark as dhb; dhb.load(\"DTLZ\");\nfrom deephyper_benchmark.lib.dtlz import hpo, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can display the variable search space of the benchmark we just loaded:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hpo.problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To define a black-box for multi-objective optimization it is very similar to single-objective optimization at the difference that the ``objective`` can now be a list of values. A first possibility is:\n\n```python\ndef run(job):\n    ...\n    return objective_0, objective_1, ..., objective_n\n```\nwhich just returns the objectives to optimize as a tuple. If additionnal metadata are interesting to gather for each evaluation it is also possible to return them by following this format:\n\n```python\ndef run(job):\n    ...\n    return {\n        \"objective\": [objective_0, objective_1, ..., objective_n],\n        \"metadata\": {\n            \"flops\": ...,\n            \"memory_footprint\": ...,\n            \"duration\": ...,\n         }\n     }\n```\neach of the metadata needs to be JSON serializable and will be returned in the final results with a column name formatted as ``m:metadata_key`` such as ``m:duration``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can load Centralized Bayesian Optimization search:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.hpo import CBO\nfrom deephyper.evaluator import Evaluator\nfrom deephyper.evaluator.callback import TqdmCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interface to submit/gather parallel evaluations of the black-box function.\nThe method argument is used to specify the parallelization method, in our case we use threads.\nThe method_kwargs argument is used to specify the number of workers and the callbacks.\nThe TqdmCallback is used to display a progress bar during the search.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator.create(\n    hpo.run,\n    method=\"thread\",\n    method_kwargs={\"num_workers\": 4, \"callbacks\": [TqdmCallback()]},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Search algorithm\nThe acq_func argument is used to specify the acquisition function.\nThe multi_point_strategy argument is used to specify the multi-point strategy,\nin our case we use qUCB instead of the default cl_max (constant-liar) to reduce overheads.\nThe update_prior argument is used to specify whether the sampling-prior should\nbe updated during the search.\nThe update_prior_quantile argument is used to specify the quantile of the lower-bound\nused to update the sampling-prior.\nThe moo_scalarization_strategy argument is used to specify the scalarization strategy.\nChebyshev is capable of generating a diverse set of solutions for non-convex problems.\nThe moo_scalarization_weight argument is used to specify the weight of the scalarization.\nrandom is used to generate a random weight vector for each iteration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search = CBO(\n    hpo.problem,\n    evaluator,\n    acq_func=\"UCBd\",\n    multi_point_strategy=\"qUCB\",\n    acq_optimizer=\"ga\",\n    acq_optimizer_freq=1,\n    moo_scalarization_strategy=\"AugChebyshev\",\n    moo_scalarization_weight=\"random\",\n    objective_scaler=\"identity\",\n    n_jobs=-1,\n    verbose=1,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Launch the search for a given number of evaluations\nother stopping criteria can be used (e.g. timeout, early-stopping/convergence)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = search.search(max_evals=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A Pandas table of results is returned by the search and also saved at ``./results.csv``. An other location can be specified by using ``CBO(..., log_dir=...)``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this table we retrieve:\n\n- columns starting by ``p:`` which are the optimized variables.\n- the ``objective_{i}`` are the objectives returned by the black-box function.\n- the ``job_id`` is the identifier of the executed evaluations.\n- columns starting by ``m:`` are metadata returned by the black-box function.\n- ``pareto_efficient`` is a column only returned for MOO which specify if the evaluation is part of the set of optimal solutions.\n\nLet us use this table to visualized evaluated objectives:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Plot evaluated objectives\nfig, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS), tight_layout=True)\n_ = ax.plot(\n    -results[~results[\"pareto_efficient\"]][\"objective_0\"],\n    -results[~results[\"pareto_efficient\"]][\"objective_1\"],\n    \"o\",\n    color=\"blue\",\n    alpha=0.7,\n    label=\"Non Pareto-Efficient\",\n)\n_ = ax.plot(\n    -results[results[\"pareto_efficient\"]][\"objective_0\"],\n    -results[results[\"pareto_efficient\"]][\"objective_1\"],\n    \"o\",\n    color=\"red\",\n    alpha=0.7,\n    label=\"Pareto-Efficient\",\n)\n_ = ax.grid()\n_ = ax.legend()\n_ = ax.set_xlabel(\"Objective 0\")\n_ = ax.set_ylabel(\"Objective 1\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}