{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Black-Box Optimization\n\n**Author(s)**: Romain Egele, Brett Eiffert.\n\nIn this tutorial, we introduce you to the notion of [black-box optimization (Wikipedia)](https://en.wikipedia.org/wiki/Derivative-free_optimization) (a.k.a., derivative-free optimization) with DeepHyper.\n\nBlack-box optimization is a field of optimization research where an objective function $f(x) = y \\in \\mathbb{R}$ is optimized only based on input-output observations $\\{ (x_1,y_1), \\ldots, (x_n, y_n) \\}$.\n \nLet's start by installing DeepHyper!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\npip install deephyper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can import it and check the installed version:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deephyper\nprint(deephyper.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization Problem\n\nThe optimization problem is based  on two components:\n\n1. The black-box function that we want to optimize.\n2. The search space or domain of input variables over which we want to optimize.\n\n### Black-Box Function\n\nDeepHyper is developed to optimize black-box functions.\nHere, we define the function $f(x) = - x ^ 2$ that we want to maximise (the maximum being $f(x=0) = 0$ on $I_x = [-10;10]$). The black-box function ``f`` takes as input a ``job`` that follows a dictionary interface from which we retrieve the variables of interest.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def f(job):\n    return -job.parameters[\"x\"] ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Search Space of Input Variables\n\nIn this example, we have only one variable $x$ for the black-box functin $f$. We empirically decide to optimize this variable $x$ on the interval $I_x = [-10;10]$. To do so we use the :class:`deephyper.hpo.HpProblem` from DeepHyper and add a **real** hyperparameter by using a tuple of two floats.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.hpo import HpProblem\n\n\nproblem = HpProblem()\n\n# Define the variable you want to optimize\nproblem.add_hyperparameter((-10.0, 10.0), \"x\")\n\nproblem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluator Interface\n\nDeepHyper uses an API called :class:`deephyper.evaluator.Evaluator` to distribute the computation of black-box functions and adapt to different backends (e.g., threads, processes, MPI, Ray). An ``Evaluator`` object wraps the black-box function ``f`` that we want to optimize. Then a ``method`` parameter is used to select the backend and ``method_kwargs`` defines some available options of this backend.\n\n\n.. hint:: The ``method=\"thread\"`` provides parallel computation only if the black-box is releasing the global interpretor lock (GIL). Therefore, if you want parallelism in Jupyter notebooks you should use the Ray evaluator (``method=\"ray\"``) after installing Ray with ``pip install ray``.\n\nIt is possible to define callbacks to extend the behaviour of ``Evaluator`` each time a function-evaluation is launched or completed. In this example we use the :class:`deephyper.evaluator.callback.TqdmCallback` to follow the completed evaluations and the evolution of the objective with a progress-bar.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.evaluator import Evaluator\nfrom deephyper.evaluator.callback import TqdmCallback\n\n\n# define the evaluator to distribute the computation\nevaluator = Evaluator.create(\n    f,\n    method=\"thread\",\n    method_kwargs={\n        \"num_workers\": 4,\n        \"callbacks\": [TqdmCallback()]\n    },\n)\n\nprint(f\"Evaluator has {evaluator.num_workers} available worker{'' if evaluator.num_workers == 1 else 's'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Search Algorithm\n\nThe next step is to define the search algorithm that we want to use. Here, we choose :class:`deephyper.hpo.CBO` (Centralized Bayesian Optimization) which is a sampling based Bayesian optimization strategy. This algorithm has the advantage of being asynchronous thanks to a constant liar strategy which is crutial to keep a good utilization of the resources when the number of available workers increases.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.hpo import CBO\n\n# define your search\nsearch = CBO(\n    problem,\n    evaluator,\n    acq_func=\"UCB\",  # Acquisition function to Upper Confidence Bound\n    multi_point_strategy=\"qUCB\",  # Fast Multi-point strategy with q-Upper Confidence Bound\n    n_jobs=2,  # Number of threads to fit surrogate models in parallel\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can execute the search for a given number of iterations by using the ``search.search(max_evals=...)``. It is also possible to use the ``timeout`` parameter if one needs a specific time budget (e.g., restricted computational time in machine learning competitions, allocation time in HPC).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = search.search(max_evals=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let us visualize the results. The ``search(...)`` returns a DataFrame also saved locally under ``results.csv`` (in case of crash we don't want to lose the possibly expensive evaluations already performed).\n\nThe DataFrame contains as columns:\n\n1. the optimized hyperparameters: such as $x$ with name ``p:x``.\n2. the ``objective`` **maximised** which directly match the results of the $f$ function in our example.\n3. the ``job_id`` of each evaluated function (increased incrementally following the order of created evaluations).\n4. the time of creation/collection of each task ``timestamp_submit`` and ``timestamp_gather`` respectively (in secondes, since the creation of the Evaluator).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the parameters at the observed maximum value we can use the :func:`deephyper.analysis.hpo.parameters_at_max`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.analysis.hpo import parameters_at_max\n\n\nparameters, objective = parameters_at_max(results)\nprint(\"\\nOptimum values\")\nprint(\"x:\", parameters[\"x\"])\nprint(\"objective:\", objective)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also plot the evolution of the objective to verify that we converge correctly toward $0$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom deephyper.analysis.hpo import plot_search_trajectory_single_objective_hpo\n\n\nWIDTH_PLOTS = 8\nHEIGHT_PLOTS = WIDTH_PLOTS / 1.618\n\nfig, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\nplot_search_trajectory_single_objective_hpo(results, mode=\"min\", ax=ax)\n_ = plt.title(\"Search Trajectory\")\n_ = plt.yscale(\"log\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}