{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Constrained Black-Box Optimization with Custom Chained Sampler\n\n**Author(s)**: Romain Egele.\n\nThis tutorial demonstrates how to solve *constrained*\n[black-box optimization](https://en.wikipedia.org/wiki/Derivative-free_optimization)\nusing **DeepHyper**, focusing on how to encode structural constraints directly\nin the **sampling strategy** of the search algorithm.\n\nBlack-box optimization aims to optimize an unknown function\n$f(x) = y \\in \\mathbb{R}$ using only input\u2013output evaluations\n$\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$.\nNo analytical gradients or structural properties of $f$ are required.\n\nIn *constrained* settings, the search is further restricted to parameters that\nsatisfy one or more feasibility rules. Constraints can significantly reshape\nthe search space and modify the behavior of the optimizer.\n\n## Problem Setting\nIn this example, we consider a *discrete*, *ordered* search space of\ndimension $N$.\nEach variable $x_i$ must satisfy the **monotonicity constraint**\n\n\\begin{align}x_0 < x_1 < \\cdots < x_{N-1}.\\end{align}\n\nEach $x_i$ is bounded between $i$ and $m - N + i$.\nThis could tipically represent the layer indexes to drop in Depth pruning of Large language models.\nThe objective is to **maximize** the sum\n\n\\begin{align}f(x) = \\sum_{i=0}^{N-1} x_i.\\end{align}\n\nSince the optimal strategy is to push every variable as high as possible while\nrespecting monotonicity, the theoretical optimum is:\n\n\\begin{align}\\sum_{i=0}^{N-1} (m - i).\\end{align}\n\nDeepHyper offers several ways to incorporate constraints:\n\n#. **Custom chained sampler** *(this tutorial)*: constraints are enforced\n   directly when generating new candidate points.\n\n#. **Rejection sampling**:\n   see `Constrained Black-Box Optimization with Rejection Sampling <sphx_glr_examples_examples_bbo_plot_constrained_black_box_optimization.py>`.\n\n#. **Learning to avoid failures** (CBO auto-handles failed evaluations):\n   see this tutorial and also\n   `Learn to Avoid Failures with Bayesian Optimization <sphx_glr_examples_examples_bbo_plot_notify_failures_hpo.py>`.\n\n#. **Multi-objective approach** where the constraint becomes an additional\n   objective (tutorial forthcoming).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nimport numpy as np\nimport pandas as pd\n\nfrom deephyper.analysis.hpo import (\n    plot_search_trajectory_single_objective_hpo,\n    parameters_at_max,\n    filter_failed_objectives,\n)\nfrom deephyper.hpo import HpProblem, CBO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Chained Sampler\n\nBecause every $x_i$ must be strictly larger than $x_{i-1}$, the\nusual independent sampling over each variable would frequently violate the\nconstraint.\n\nInstead, we implement a *chained* sampler:\neach $x_k$ is sampled conditionally so that enough \"room\" remains for\nfuture variables. This ensures that:\n\n- all generated samples satisfy $x_i < x_{i+1}$ by construction;\n- the sampler focuses on the feasible region, avoiding wasted evaluations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = 10\nm = 32\n\nprint(\"optimum:\", sum([m - i - 1 for i in range(n)]))\n\npb = HpProblem()\nfor i in range(n):\n    pb.add((i, m - n + i - 1), f\"x{i}\")\n\n\ndef sampling_fn(size: int) -> list[dict]:\n    def sample_chain():\n        # Chain the sampling\n        vals = []\n        lo = 0\n        for k in range(n):\n            low = max(k, lo + 1 if k > 0 else 0)\n            high = m - n + k\n            v = np.random.randint(low, high - (n - 1 - k))  # keep room for future vars\n            vals.append(v)\n            lo = v\n        return {k: v for k, v in zip(pb.hyperparameter_names, vals)}\n\n    return [sample_chain() for _ in range(size)]\n\n\npb.set_sampling_fn(sampling_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constraint Function\n\nAlthough the sampler already generates feasible points, we explicitly define a\n``constraint_fn``. This allows DeepHyper to properly handle *failed* trials\n(e.g., from manually constructed parameter sets or mutation-based acquisition\noptimizers).\nNot only that, this will help report non-feasible points using ``\"F_constraint\"``\nin the objective function so that CBO learns to avoid them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def constraint_fn(df: pd.DataFrame) -> pd.Series:\n    accept = pd.Series(np.ones((len(df)), dtype=bool))\n    for i in range(n - 1):\n        accept = accept & (df[f\"x{i}\"] < df[f\"x{i + 1}\"])\n    return accept\n\n\npb.set_constraint_fn(constraint_fn)\n\n\ndef f(job):\n    \"\"\"Objective function: maximize sum(x_i).\"\"\"\n    df = pd.DataFrame([job.parameters])\n    accept = constraint_fn(df)\n    if all(accept):\n        return sum(job.parameters.values())\n    else:\n        return \"F_constraint\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian Optimization with Mixed-GA Acquisition Optimization\n\nWe run a **Centralized Bayesian Optimization (CBO)** search using:\n\n- Ensemble of Trees surrogate model (``\"ET\"``).\n- A **mixed genetic algorithm** (``\"mixedga\"``) to optimize the acquisition\n  function.\n- A **periodically decaying scheduler** on the exploration parameter ``kappa``.\n\nThis setup is well suited for discrete, irregularly constrained spaces.\n\nThe search runs for ``max_evals=300`` iterations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search = CBO(\n    pb,\n    surrogate_model=\"ET\",\n    surrogate_model_kwargs={\"max_features\": \"sqrt\"},\n    acq_optimizer=\"mixedga\",\n    acq_optimizer_kwargs={\n        \"n_points\": 1_000,\n        \"acq_optimizer_freq\": 2,\n        \"filter_failures\": \"mean\",\n    },\n    acq_func_kwargs={\n        # Exploration/Exploitation mechanism\n        \"kappa\": 10.0,\n        \"scheduler\": {\n            \"type\": \"periodic-exp-decay\",\n            \"period\": 20,\n            \"kappa_final\": 0.1,\n        },\n    },\n    verbose=1,\n)\nresults = search.search(f, max_evals=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting the Best Parameters\nTo recover the parameters corresponding to the best observed objective value,\nwe can use :func:`deephyper.analysis.hpo.parameters_at_max`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parameters, objective = parameters_at_max(results)\nprint(\"\\nOptimum values\")\nfor i in range(n):\n    print(f\"x{i}: {parameters[f'x{i}']:.3f}\")\nprint(\"objective:\", objective)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\nWe conclude with:\n\n- a **search trajectomakery plot** showing the best objective value over time,\n  where the periodic exploration schedule is clearly visible;\n\n- a **feasible-space evaluation plot** showing all sampled curves\n  $i \\mapsto x_i$ (each curve is one evaluation), colored by objective\n  value.\n\nThese visualizations confirm that the optimizer progressively learns the\nstructure of the monotonic constraint and approaches the theoretical optimum.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "WIDTH_PLOTS = 8\nHEIGHT_PLOTS = WIDTH_PLOTS / 1.618\n\nfig, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\nplot_search_trajectory_single_objective_hpo(results, mode=\"max\", ax=ax)\n_ = plt.title(\"Search Trajectory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Feasible Region and Evaluations\nWe now plot all evaluated points in the (x, y) plane, color-coded by\nobjective value, along with the constraint boundary ``x + y = 10``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results, _ = filter_failed_objectives(results)\n\np_columns = [col for col in results.columns if col.startswith(\"p:\")]\n\n# Create a normalizer over the objective range\nobj_vals = results[\"objective\"]\nnorm = colors.Normalize(vmin=obj_vals.min(), vmax=obj_vals.max())\n\n# Choose a colormap (viridis is a good default)\ncmap = plt.get_cmap(\"viridis\")\n\nfig, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\n\nfor i, row in results.iterrows():\n    x_values = row[p_columns].values\n    y_values = np.arange(n)\n    obj_value = row[\"objective\"]\n\n    color = cmap(norm(obj_value))  # map objective \u2192 color\n    ax.plot(x_values, y_values, color=color, alpha=0.9)\n\n# Optionally add a colorbar\nsm = cm.ScalarMappable(norm=norm, cmap=cmap)\ncbar = fig.colorbar(sm, ax=ax)\ncbar.set_label(\"Objective value\")\nax.grid()\nax.set_ylim(0, n - 1)\nax.set_xlim(0, m)\nax.set_ylabel(r\"$i$\")\nax.set_xlabel(r\"$x_i$\")\nax.set_yticks(list(range(n)), [str(i) for i in range(n)])\nax.set_xticks(list(range(0, m, 2)), [str(i) for i in range(0, m, 2)])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}