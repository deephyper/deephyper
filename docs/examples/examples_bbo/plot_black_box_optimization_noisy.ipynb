{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Noisy Black-Box Optimization\n\n**Author(s)**: Romain Egele.\n\nIn this tutorial, we show you how to manage **noisy** [black-box optimization (Wikipedia)](https://en.wikipedia.org/wiki/Derivative-free_optimization) (a.k.a., derivative-free optimization) with DeepHyper.\n\nBlack-box optimization is a field of optimization research where an objective function $f(x) = y \\in \\mathbb{R}$ is optimized only based on input-output observations $\\{ (x_1,y_1), \\ldots, (x_n, y_n) \\}$.\n \nLet's start by installing DeepHyper!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\npip install deephyper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization Problem\n\nThe optimization problem consists of two components:\n\n1. The *black-box function* that we aim to optimize.\n2. The *search space* (or domain) of input variables over which the optimization is performed.\n\n### Black-Box Function\n\nIn DeepHyper, black-box optimization is performed on user-defined functions that can be noisy or stochastic.\nBelow, we define a noisy black-box function `f` that depends on a single variable $x$ in the domain\n$I_x = [-10, 10]$.\n\nThe noisy black-box function is defined as:\n\n\\begin{align}f(x) = \\text{Binomial}(n=1, p(x))\\end{align}\n\nwhere the probability of success is:\n\n\\begin{align}p(x) = \\frac{100 - x^2}{100}.\\end{align}\n\nThis means that for each evaluation, `f(x)` returns a random binary value (0 or 1) with probability `p(x)` of success.\nThe maximum expected value of $f(x)$ is obtained at $x = 0$, where $p(0) = 1$.\n\nThe function `f` takes as input a `job`, which behaves like a dictionary.\nThe variable of interest `x` is accessed via `job.parameters[\"x\"]`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\n\ndef f(job):\n    p = (100 - job.parameters[\"x\"] ** 2) / 100\n    obs = np.random.binomial(n=1, p=p)\n    return obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Search Space of Input Variables\n\nIn this example, we have only one variable $x$ for the black-box functin $f$. We empirically decide to optimize this variable $x$ on the interval $I_x = [-10;10]$. To do so we use the :class:`deephyper.hpo.HpProblem` from DeepHyper and add a **real** hyperparameter by using a tuple of two floats.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.hpo import HpProblem\n\n\nproblem = HpProblem()\n\n# Define the variable you want to optimize\nproblem.add_hyperparameter((-10.0, 10.0), \"x\")\n\nproblem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluator Interface\n\nDeepHyper uses an API called :class:`deephyper.evaluator.Evaluator` to distribute the computation of black-box functions and adapt to different backends (e.g., threads, processes, MPI, Ray). An ``Evaluator`` object wraps the black-box function ``f`` that we want to optimize. Then a ``method`` parameter is used to select the backend and ``method_kwargs`` defines some available options of this backend.\n\n\n.. hint:: The ``method=\"thread\"`` provides parallel computation only if the black-box is releasing the global interpretor lock (GIL). Therefore, if you want parallelism in Jupyter notebooks you should use the Ray evaluator (``method=\"ray\"``) after installing Ray with ``pip install ray``.\n\nIt is possible to define callbacks to extend the behaviour of ``Evaluator`` each time a function-evaluation is launched or completed. In this example we use the :class:`deephyper.evaluator.callback.TqdmCallback` to follow the completed evaluations and the evolution of the objective with a progress-bar.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.evaluator import Evaluator\nfrom deephyper.evaluator.callback import TqdmCallback\n\n\n# define the evaluator to distribute the computation\nevaluator = Evaluator.create(\n    f,\n    method=\"thread\",\n    method_kwargs={\n        \"num_workers\": 1,\n        \"callbacks\": [TqdmCallback()]\n    },\n)\n\nprint(f\"Evaluator has {evaluator.num_workers} available worker{'' if evaluator.num_workers == 1 else 's'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Search Algorithm\n\nThe next step is to define the search algorithm that we want to use. Here, we choose :class:`deephyper.hpo.CBO` (Centralized Bayesian Optimization) which is a sampling based Bayesian optimization strategy. \nThis algorithm has the advantage of being asynchronous which is crutial to keep a good utilization of the resources when the number of available workers increases.\nWe also choose, how to optimize the acquisition function of the Bayesian optimization with ``\"ga\"`` (i.e., continuous Genetic Algorithm).\n\nThen, we setup a solution selection method. Here we use :class:`deephyper.hpo.ArgMaxEstSelection`, that will select the optimum based on the estimated maximum of a surrogate model.\nThe ``model_grid_search=True`` activates the auto-tuning of the surrogate model every 100 observations by default.\nThe ``noisy_objective=True`` sets the default configuration of the surrogate model for a noisy objective.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.hpo import CBO, ArgMaxEstSelection\n\n\ndef create_search():\n    search = CBO(\n        problem,\n        acq_optimizer=\"ga\",\n        solution_selection=ArgMaxEstSelection(\n            problem,\n            model_grid_search=True,\n            noisy_objective=True,\n        ),\n    )\n    return search\n\nmax_evals = 300\nsearch = create_search()\nresults = search.search(evaluator, max_evals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let us visualize the results. The ``search(...)`` returns a DataFrame also saved locally under ``results.csv`` (in case of crash we don't want to lose the possibly expensive evaluations already performed).\n\nThe DataFrame contains the usual columns:\n\n1. the optimized hyperparameters: such as $x$ with name ``p:x``.\n2. the ``objective`` **maximised** which directly match the results of the $f$ function in our example.\n3. the ``job_id`` of each evaluated function (increased incrementally following the order of created evaluations).\n4. the time of creation/collection of each task ``timestamp_submit`` and ``timestamp_gather`` respectively (in secondes, since the creation of the Evaluator).\n\nIn addition, it now also contains the new columns:\n1. the estimated solution parameter ``sol.p:x``.\n2. the estimated solution objective ``sol.objective``.\n3. the estimated solution objective aleatoric uncertainty ``sol.objective_std_al``.\n4. the estimated solution objective epistemic uncertainty ``sol.objective_std_ep``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the parameters at the observed maximum value we can use the :func:`deephyper.analysis.hpo.parameters_at_max`:\nWe make sure to select the right column and prefix for parameters.\nAlso, we prefer to select the solution amoung the ``n_last=20`` rows to avoid selecting noisy observations at the beginning.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.analysis.hpo import parameters_at_max\n\n\nparameters, objective = parameters_at_max(results, column=\"sol.objective\", prefix=\"sol.p:\", n_last=20)\nprint(\"\\nEstimated Optimum values\")\nprint(\"x:\", parameters[\"x\"])\nprint(\"objective:\", objective)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also plot the evolution of the estimated solution value of $x$ to verify that we converge correctly toward $x=0$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom deephyper.analysis.hpo import plot_search_trajectory_single_objective_hpo\n\n\nWIDTH_PLOTS = 8\nHEIGHT_PLOTS = WIDTH_PLOTS / 1.618\n\nfig, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\nplot_search_trajectory_single_objective_hpo(results, column=\"sol.p:x\", mode=\"max\", ax=ax)\n_ = ax.set_ylabel(r\"Estimated solution $x$\")\n_ = ax.set_ylim(-10, 10)\n_ = plt.title(\"Search Trajectory\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}