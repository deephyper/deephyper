{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Applying Transfer Learning to Black-Box Optimization\n\n**Author(s)**: Romain Egele.\n\nIn this example, we demonstrate how to leverage transfer learning for hyperparameter optimization. Imagine you are working on multiple related tasks, such as optimizing the hyperparameters of neural networks for various datasets. It's reasonable to expect that similar hyperparameter configurations might perform well across these datasets, even if some minor adjustments are needed to fine-tune performance.\n\nBy conducting a thorough (and potentially expensive) search on one task, you can reuse the resulting hyperparameter set to guide and accelerate optimization for subsequent tasks. This approach reduces computational costs while maintaining high performance.\n\nTo illustrate, we will use a simple and computationally inexpensive example: minimizing the function $f(x) = \\sum_{i=0}^\n{n-1}$. Here, the difficulty of the problem is defined by the number of variables $n$. We will start by optimizing the small problem where $n=5$. Then, we will apply transfer learning to optimize a larger problem where $n=10$, comparing the results with and without transfer learning to highlight the benefits.\n\nLet's begin by defining the run-functions for both the small-scale and large-scale problems:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Import statements\nimport functools\n\nimport matplotlib.pyplot as plt\n\nfrom deephyper.analysis import figure_size\nfrom deephyper.analysis.hpo import plot_search_trajectory_single_objective_hpo\nfrom deephyper.evaluator import Evaluator\nfrom deephyper.evaluator.callback import TqdmCallback\nfrom deephyper.hpo import CBO, HpProblem\n\nWIDTH_PLOTS = 8\nHEIGHT_PLOTS = WIDTH_PLOTS / 1.618"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run(job, N: int) -> float:\n    # Definition of the function to minimize\n    y = sum([job.parameters[f\"x{i}\"] ** 2 for i in range(N)])\n    return -y  # Use the `-` sign to perform minimization\n\n\nn_small = 5\nn_large = 10\nrun_small = functools.partial(run, N=n_small)\nrun_large = functools.partial(run, N=n_large)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can define the hyperparameter problem space based on $n$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_problem(n):\n    problem = HpProblem()\n    for i in range(n):\n        problem.add_hyperparameter((-10.0, 10.0), f\"x{i}\")\n    return problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem_small = create_problem(n_small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem_large = create_problem(n_large)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the parameters of the search:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search_kwargs = {\n    \"surrogate_model\": \"ET\",  # Use Extra Trees as surrogate model\n    \"surrogate_model_kwargs\": {\n        \"n_estimators\": 25,  # Relatively small number of trees in the surrogate to make it \"fast\"\n        \"min_samples_split\": 4,  # Larger number to avoid small leaf nodes (smoothing the objective response)\n    },\n    \"acq_optimizer\": \"ga\", # Optimizing the acquisition function with countinuous genetic algorithm\n    \"acq_optimizer_freq\": 1,\n    \"filter_duplicated\": False,  # Deactivate filtration of duplicated new points\n    \"kappa\": 10.0,  # Initial value of exploration-exploitation parameter for the acquisition function\n    \"scheduler\": {  # Scheduler for the exploration-exploitation parameter \"kappa\"\n        \"type\": \"periodic-exp-decay\",  # Periodic exponential decay\n        \"period\": 25,  # Period over which the decay is applied. It is useful to escape local solutions.\n        \"kappa_final\": 0.01,  # Value of kappa at the end of each \"period\"\n    },\n    \"objective_scaler\": \"identity\",\n    \"random_state\": 42,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a dictionnary that will store the results of each experiment and also fix the number of\nevaluation of the search to 200.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = {}\nmax_evals = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we run the search for each problem. We start with the small problem:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluator_small = Evaluator.create(\n    run_small, \n    method=\"thread\", \n    method_kwargs={\"callbacks\": [TqdmCallback(\"HPO - Small Problem\")]},\n)\n\nsearch_small = CBO(\n    problem_small, \n    evaluator_small, \n    n_initial_points=2 * n_small + 1, \n    **search_kwargs,\n)\nresults_small = search_small.search(max_evals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We run the search on the large problem without transfer learning:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluator_large = Evaluator.create(\n    run_large,\n    method=\"thread\",\n    method_kwargs={\"callbacks\": [TqdmCallback(\"HPO - Large Problem\")]},\n)\nsearch_large = CBO(\n    problem_large, \n    evaluator_large,\n    n_initial_points=2 * n_large + 1,\n    **search_kwargs,\n)\nresults[\"Large\"] = search_large.search(max_evals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we run the search on the large problem with transfer learning from the results\nof the small problem that we computed first:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluator_large_tl = Evaluator.create(\n    run_large,\n    method=\"thread\",\n    method_kwargs={\"callbacks\": [TqdmCallback(\"HPO - Large Problem with TL\")]},\n)\nsearch_large_tl = CBO(\n    problem_large, \n    evaluator_large_tl, \n    n_initial_points=2 * n_large + 1, \n    **search_kwargs,\n)\n\n# This is where transfer learning happens\nsearch_large_tl.fit_generative_model(results_small)\n\nresults[\"Large+TL\"] = search_large_tl.search(max_evals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we compare the results and quickly see that transfer-learning\nprovided a consequant speed-up for the search:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS), tight_layout=True)\n\nfor i, (strategy, df) in enumerate(results.items()):\n    plot_search_trajectory_single_objective_hpo(\n        df,\n        show_failures=False,\n        mode=\"min\",\n        ax=ax,\n        label=strategy,\n        plot_kwargs={\"color\": f\"C{i}\"},\n        scatter_success_kwargs={\"c\": f\"C{i}\"},\n    )\n\nax.set_xlabel\nax.set_xlabel(\"Time (sec.)\")\nax.set_ylabel(\"Objective\")\nax.set_yscale(\"log\")\nax.grid(visible=True, which=\"minor\", linestyle=\":\")\nax.grid(visible=True, which=\"major\", linestyle=\"-\")\nax.legend()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}