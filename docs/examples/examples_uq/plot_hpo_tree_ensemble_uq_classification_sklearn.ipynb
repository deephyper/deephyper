{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Hyperparameter Optimized Ensemble of Random Decision Trees with Uncertainty for Classification\n\n**Author(s)**: Romain Egele.\n\nIn this tutorial, you will learn about how to use hyperparameter optimization to generate ensemble of [Scikit-Learn](https://scikit-learn.org/stable/) models that can be used for uncertainty quantification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation and imports\n\nInstalling dependencies with the `pip installation <install-pip>` is recommended. It requires **Python >= 3.10**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\npip install \"deephyper[ray]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Import statements\nimport pathlib\nimport pickle\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.calibration import CalibrationDisplay\nfrom sklearn.datasets import make_moons\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\nWIDTH_PLOTS = 8\nHEIGHT_PLOTS = WIDTH_PLOTS / 1.618"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Synthetic data generation\n\nFor the data, we use the :func:`sklearn.datasets.make_moons` functionality from Scikit-Learn to have a synthetic binary-classification problem with two moons.\nThe input data $x$ are two dimensionnal and the target data $y$ are binary values.\nWe randomly flip 10% of the labels to generate artificial noise that should later be estimated by what we call \"aleatoric uncertainty\" (a.k.a., intrinsic random noise).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Loading synthetic data\ndef flip_binary_labels(y, ratio, random_state=None):\n    \"\"\"Increase the variance of P(Y|X) by ``ratio``\"\"\"\n    y_flipped = np.zeros(np.shape(y))\n    y_flipped[:] = y[:]\n    rs = np.random.RandomState(random_state)\n    idx = np.arange(len(y_flipped))\n    idx = rs.choice(idx, size=int(ratio * len(y_flipped)), replace=False)\n    y_flipped[idx] = 1 - y_flipped[idx]\n    return y_flipped\n\n\ndef load_data(noise=0.1, n=1_000, ratio_flipped=0.1, test_size=0.33, valid_size=0.33, random_state=42):\n    rng = np.random.RandomState(random_state)\n    max_int = np.iinfo(np.int32).max\n\n    test_size = int(test_size * n)\n    valid_size = int(valid_size * n)\n\n    X, y = make_moons(n_samples=n, noise=noise, shuffle=True, random_state=rng.randint(max_int))\n    X = X - np.mean(X, axis=0)\n\n    y = flip_binary_labels(y, ratio=ratio_flipped, random_state=rng.randint(max_int))\n    y = y.astype(np.int64)\n\n    train_X, test_X, train_y, test_y = train_test_split(\n        X, \n        y, \n        test_size=test_size,\n        random_state=rng.randint(max_int),\n        stratify=y,\n    )\n\n    train_X, valid_X, train_y, valid_y = train_test_split(\n        train_X,\n        train_y, \n        test_size=valid_size, \n        random_state=rng.randint(max_int), \n        stratify=train_y,\n    )\n\n    return (train_X, train_y), (valid_X, valid_y), (test_X, test_y)\n\n(x, y), (vx, vy), (tx, ty) = load_data()\n\n_ = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS), tight_layout=True)\n_ = plt.scatter(\n    x[:, 0].reshape(-1), x[:, 1].reshape(-1), c=y, label=\"train\", alpha=0.8\n)\n_ = plt.scatter(\n    vx[:, 0].reshape(-1),\n    vx[:, 1].reshape(-1),\n    c=vy,\n    marker=\"s\",\n    label=\"valid\",\n    alpha=0.8,\n)\n_ = plt.ylabel(\"$x1$\", fontsize=12)\n_ = plt.xlabel(\"$x0$\", fontsize=12)\n_ = plt.legend(loc=\"upper center\", ncol=3, fontsize=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a Decision Tree\n\nWe focus on the class of random decision tree models. \nWe define a function that trains and evaluate a random decision tree from given parameters ``job.parameters``.\nThese parameters will be optimized in the next steps by DeepHyper.\n\nThe score we minimize with respect to hyperparameters $\\theta$ is the validation log loss (a.k.a., binary cross entropy) as we want to have calibrated uncertainty estimates of $P(Y|X=x)$ and $1-P(Y|X=x)$:\n\n\\begin{align}L_\\text{BCE}(x, y;\\theta) = y \\cdot \\log\\left(p(y|x;\\theta)\\right) + (1 - y) \\cdot \\log\\left(1 - p(y|x\\theta)\\right)\\end{align}\n\nwhere $p(y|x;\\theta)$ is the predited probability of a tree with hyperparameters $\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Plot decision boundary\ndef plot_decision_boundary_decision_tree(dataset, labels, model, steps=1000, color_map=\"viridis\", ax=None):\n    color_map = plt.get_cmap(color_map)\n    # Define region of interest by data limits\n    xmin, xmax = dataset[:, 0].min() - 1, dataset[:, 0].max() + 1\n    ymin, ymax = dataset[:, 1].min() - 1, dataset[:, 1].max() + 1\n    x_span = np.linspace(xmin, xmax, steps)\n    y_span = np.linspace(ymin, ymax, steps)\n    xx, yy = np.meshgrid(x_span, y_span)\n\n    # Make predictions across region of interest\n    labels_predicted = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n\n    # Plot decision boundary in region of interest\n    z = labels_predicted[:, 1].reshape(xx.shape)\n\n    ax.contourf(xx, yy, z, cmap=color_map, alpha=0.5)\n\n    # Get predicted labels on training data and plot\n    ax.scatter(\n        dataset[:, 0],\n        dataset[:, 1],\n        c=labels,\n        # cmap=color_map,\n        lw=0,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``run`` function takes a ``job`` object as input suggested by DeepHyper.\nWe use it to pass the ``job.parameters`` and create the decision tree ``model``. \nThen, we fit the model on the data on compute its log-loss score on the validation dataset.\nIn case of unexpected error we return a special value ``F_fit`` so that our hyperparameter optimization can learn to avoid these unexepected failures.\nWe checkpoint the model on disk as ``model_*.pkl`` files.\nFinally, we return all of our scores, the ``\"objective\"`` is the value maximized by DeepHyper. Other scores are returned as metadata for further analysis (e.g., overfitting, underfitting, etc.).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hpo_dir = \"hpo_sklearn_classification\"\nmodel_checkpoint_dir = os.path.join(hpo_dir, \"models\")\n\n\ndef run(job, model_checkpoint_dir=\".\", verbose=True, show_plots=False):\n\n    (x, y), (vx, vy), (tx, ty) = load_data()\n\n    model = DecisionTreeClassifier(**job.parameters)\n\n    try:\n        model.fit(x, y)\n        vy_pred_proba = model.predict_proba(vx)\n        val_cce = log_loss(vy, vy_pred_proba)\n    except:\n        return \"F_fit\"\n\n    # Saving the model\n    with open(os.path.join(model_checkpoint_dir, f\"model_{job.id}.pkl\"), \"wb\") as f:\n        pickle.dump(model, f)\n\n    if verbose:\n        print(f\"{job.id}: {val_cce=:.3f}\")\n\n    if show_plots:\n        fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(WIDTH_PLOTS, HEIGHT_PLOTS*2), tight_layout=True)\n        plot_decision_boundary_decision_tree(tx, ty, model, steps=1000, color_map=\"viridis\", ax=axes[0])\n        disp = CalibrationDisplay.from_predictions(ty, model.predict_proba(tx)[:, 1], ax=axes[1])\n\n    test_cce = log_loss(ty, model.predict_proba(tx))\n    test_acc = accuracy_score(ty, model.predict(tx))\n\n    # The score is negated for maximization\n    # The score is -Categorical Cross Entropy/LogLoss\n    return {\n        \"objective\": -val_cce,\n        \"metadata\": {\"test_cce\": test_cce, \"test_acc\": test_acc},\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is important to note that we did not fix the random state of the random decision tree.\nThe hyperparameter optimization takes into consideration the fact that the observed objective is noisy and of course this can be tuned.\nFor example, as the default surrogate model of DeepHyper is itself a randomized forest, increasing the number of samples in leaf nodes would have the effect of averaging out the prediction of the surrogate.\n\nAlso, the point of ensembling randomized decision trees is to build a model with lower variance (i.e., variability of the score when fitting it) than its base estimators.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter search space\n\nWe define the hyperparameter search space for decision trees.\nThis tells to DeepHyper the hyperparameter values it can use for the optimization.\nTo define these hyperparameters we look at the [DecisionTreeClassifier API Reference](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.hpo import HpProblem\n\n\ndef create_hpo_problem():\n\n    problem = HpProblem()\n\n    problem.add_hyperparameter([\"gini\", \"entropy\", \"log_loss\"], \"criterion\")\n    problem.add_hyperparameter([\"best\", \"random\"], \"splitter\")\n    problem.add_hyperparameter((10, 1000, \"log-uniform\"), \"max_depth\", default_value=1000)\n    problem.add_hyperparameter((2, 20), \"min_samples_split\", default_value=2)\n    problem.add_hyperparameter((1, 20), \"min_samples_leaf\", default_value=1)\n    problem.add_hyperparameter((0.0, 0.5), \"min_weight_fraction_leaf\", default_value=0.0)\n\n    return problem\n\nproblem = create_hpo_problem()\nproblem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation of the baseline\n\nWe previously defined ``default_value=...`` for each hyperparameter. These values corresponds to the default hyperparameters used in Scikit-Learn. We now test them to have a base performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.evaluator import RunningJob\n\n\ndef evaluate_decision_tree(problem):\n\n    model_checkpoint_dir = \"models_sklearn_test\"\n    pathlib.Path(model_checkpoint_dir).mkdir(parents=True, exist_ok=True)\n\n    default_parameters = problem.default_configuration\n    print(f\"{default_parameters=}\")\n    \n    output = run(\n        RunningJob(id=\"test\", parameters=default_parameters),\n        model_checkpoint_dir=model_checkpoint_dir,\n        show_plots=True,\n    )\n    return output\n\nbaseline_output = evaluate_decision_tree(problem)\nbaseline_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The accuracy is great, but the uncertainty is not well calibrated.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Optimization\n\nIn DeepHyper, instead of just performing sequential Bayesian optimization we provide asynchronous parallelisation for\nBayesian optimization (and other methods). This allows to execute multiple evaluation function in parallel to collect \nobservations of objectives faster.\n\nIn this example, we will focus on using centralized Bayesian optimization (CBO). In this setting, we have one main process that runs the\nBayesian optimization algorithm and we have multiple worker processes that run evaluation functions. The class we use for this is\n:class:`deephyper.hpo.CBO`.\n\nLet us start by explaining import configuration parameters of :class:`deephyper.hpo.CBO`:\n\n- ``initial_points``: is a list of initial hyperparameter configurations to test, we add the baseline hyperparameters as we want to be at least better than this configuration.\n- ``surrogate_model_*``: are parameters related to the surrogate model we use, here ``\"ET\"`` is an alias for the Extremely Randomized Trees regression model.\n- ``multi_point_strategy``: is the strategy we use for parallel suggestion of hyperparameters, here we use the ``qUCBd`` that will sample for each new parallel configuration a different $\\kappa^j_i$ value from an exponential with mean $\\kappa_i$ where $j$ is the index in the current generated parallel batch and $i$ is the iteration of the Bayesian optimization loop. ``UCB`` corresponds to the Upper Confidence Bound acquisition function. Finally the ``\"d\"`` postfix in ``qUCBd`` means that we will only consider the epistemic component of the uncertainty returned by the surrogate model.\n- ``acq_optimizer_*``: are parameters related to optimization of the previously defined acquisition function.\n- ``kappa`` and ``scheduler``: are the parameters that define the schedule of $\\kappa^j_i$ previously mentionned.\n- ``objective_scaler``: is a parameter that can be used to rescale the observed objectives (e.g., identity, min-max, log).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search_kwargs = {\n    \"initial_points\": [problem.default_configuration],\n    \"n_initial_points\": 2 * len(problem) + 1,  # Number of initial random points\n    \"surrogate_model\": \"ET\",  # Use Extra Trees as surrogate model\n    \"surrogate_model_kwargs\": {\n        \"n_estimators\": 50,  # Relatively small number of trees in the surrogate to make it \"fast\"\n        \"min_samples_split\": 8,  # Larger number to avoid small leaf nodes (smoothing the objective response)\n    },\n    \"multi_point_strategy\": \"qUCBd\",  # Multi-point strategy for asynchronous batch generations (explained later)\n    \"acq_optimizer\": \"sampling\",  # Use random sampling for the acquisition function optimizer\n    \"filter_duplicated\": False,  # Deactivate filtration of duplicated new points\n    \"kappa\": 10.0,  # Initial value of exploration-exploitation parameter for the acquisition function\n    \"scheduler\": {  # Scheduler for the exploration-exploitation parameter \"kappa\"\n        \"type\": \"periodic-exp-decay\",  # Periodic exponential decay\n        \"period\": 50,  # Period over which the decay is applied. It is useful to escape local solutions.\n        \"kappa_final\": 0.001,  # Value of kappa at the end of each \"period\"\n    },\n    \"objective_scaler\": \"identity\",\n    \"random_state\": 42,  # Random seed\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we can run the optimization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.hpo import CBO\nfrom deephyper.evaluator import Evaluator\nfrom deephyper.evaluator.callback import TqdmCallback\n\n\ndef run_hpo(problem):\n\n    pathlib.Path(model_checkpoint_dir).mkdir(parents=True, exist_ok=True)\n\n    evaluator = Evaluator.create(\n        run,\n        method=\"ray\",\n        method_kwargs={\n            \"num_cpus_per_task\": 1,\n            \"run_function_kwargs\": {\n                \"model_checkpoint_dir\": model_checkpoint_dir,\n                \"verbose\": False,\n            },\n            \"callbacks\": [TqdmCallback()]\n        },\n    )\n    search = CBO(\n        problem,\n        evaluator,\n        log_dir=hpo_dir,\n        **search_kwargs,\n    )\n\n    results = search.search(max_evals=1_000)\n\n    return results\n\nresults = run_hpo(problem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of the results\n\nThe results of the HPO is a dataframe.\nThe columns starting with ``p:`` are the hyperparameters.\nThe columns starting with ``m:`` are the metadata.\nThere are also special columns: ``objective``, ``job_id`` and ``job_status``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evolution of the objective\n\nWe use :func:`deephyper.analysis.hpo.plot_search_trajectory_single_objective_hpo` to look at the evolution of the objective during the search.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Plot search trajectory\nfrom deephyper.analysis.hpo import plot_search_trajectory_single_objective_hpo\n\n\n_, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS), tight_layout=True)\n_ = plot_search_trajectory_single_objective_hpo(results, mode=\"min\", ax=ax)\nax.axhline(-baseline_output[\"objective\"], linestyle=\"--\", color=\"red\", label=\"baseline\")\nax.set_yscale(\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dashed red horizontal line corresponds to the baseline performance.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Worker utilization\n\nWe use :func:`deephyper.analysis.hpo.plot_worker_utilization` to look at the number of active workers over the search.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Plot worker utilization\nfrom deephyper.analysis.hpo import plot_worker_utilization\n\n_, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS), tight_layout=True)\n_ = plot_worker_utilization(results, ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best decision tree\n\nThen, we look indivudualy at the performance of the top 5 models by using :func:`deephyper.analysis.hpo.parameters_from_row`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.analysis.hpo import parameters_from_row\n\n\ntopk_rows = results.nlargest(5, \"objective\").reset_index(drop=True)\n\nfor i, row in topk_rows.iterrows():\n    parameters = parameters_from_row(row)\n    obj = row[\"objective\"]\n    print(f\"Top-{i+1} -> {obj=:.3f}: {parameters}\")\n    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we just plot the decision boundary and calibration plots of the best model we can\nobserve a significant improvement over the baseline with log-loss values around 0.338 when it\nwas previously around 6.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_job = topk_rows.iloc[0]\nhpo_dir = \"hpo_sklearn_classification\"\nmodel_checkpoint_dir = os.path.join(hpo_dir, \"models\")\nwith open(os.path.join(model_checkpoint_dir, f\"model_0.{best_job.job_id}.pkl\"), \"rb\") as f:\n    best_model = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Plot decision boundary and calibration\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(WIDTH_PLOTS, HEIGHT_PLOTS*2), tight_layout=True)\nplot_decision_boundary_decision_tree(tx, ty, best_model, steps=1000, color_map=\"viridis\", ax=axes[0])\ndisp = CalibrationDisplay.from_predictions(ty, best_model.predict_proba(tx)[:, 1], ax=axes[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble of decision trees\n\nWe now move to ensembling checkpointed models and we start by importing utilities from :module:`deephyper.ensemble` and `deephyper.predictor`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.ensemble import EnsemblePredictor\nfrom deephyper.ensemble.aggregator import MixedCategoricalAggregator\nfrom deephyper.ensemble.loss import CategoricalCrossEntropy \nfrom deephyper.ensemble.selector import GreedySelector, TopKSelector\nfrom deephyper.predictor.sklearn import SklearnPredictorFileLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Plot decision boundary and uncertainty\ndef plot_decision_boundary_and_uncertainty(\n    dataset, labels, model, steps=1000, color_map=\"viridis\", s=5\n):\n\n    fig, axs = plt.subplots(\n        3, sharex=\"all\", sharey=\"all\", figsize=(WIDTH_PLOTS, HEIGHT_PLOTS * 2), tight_layout=True,\n    )\n\n    # Define region of interest by data limits\n    xmin, xmax = dataset[:, 0].min() - 1, dataset[:, 0].max() + 1\n    ymin, ymax = dataset[:, 1].min() - 1, dataset[:, 1].max() + 1\n    x_span = np.linspace(xmin, xmax, steps)\n    y_span = np.linspace(ymin, ymax, steps)\n    xx, yy = np.meshgrid(x_span, y_span)\n\n    # Make predictions across region of interest\n    y_pred = model.predict(np.c_[xx.ravel(), yy.ravel()].astype(np.float32))\n    y_pred_proba = y_pred[\"loc\"]\n    y_pred_aleatoric = y_pred[\"uncertainty_aleatoric\"]\n    y_pred_epistemic = y_pred[\"uncertainty_epistemic\"]\n\n    # Plot decision boundary in region of interest\n\n    # 1. MODE\n    color_map = plt.get_cmap(\"viridis\")\n    z = y_pred_proba[:, 1].reshape(xx.shape)\n\n    cont = axs[0].contourf(xx, yy, z, cmap=color_map, vmin=0, vmax=1, alpha=0.5)\n\n    # Get predicted labels on training data and plot\n    axs[0].scatter(\n        dataset[:, 0],\n        dataset[:, 1],\n        c=labels,\n        cmap=color_map,\n        s=s,\n        lw=0,\n    )\n    plt.colorbar(cont, ax=axs[0], label=\"Probability of class 1\")\n\n    # 2. ALEATORIC\n    color_map = plt.get_cmap(\"plasma\")\n    z = y_pred_aleatoric.reshape(xx.shape)\n\n    cont = axs[1].contourf(xx, yy, z, cmap=color_map, vmin=0, vmax=0.69, alpha=0.5)\n\n    # Get predicted labels on training data and plot\n    axs[1].scatter(\n        dataset[:, 0],\n        dataset[:, 1],\n        c=labels,\n        cmap=color_map,\n        s=s,\n        lw=0,\n    )\n    plt.colorbar(cont, ax=axs[1], label=\"Aleatoric uncertainty\")\n\n    # 3. EPISTEMIC\n    z = y_pred_epistemic.reshape(xx.shape)\n\n    cont = axs[2].contourf(xx, yy, z, cmap=color_map, vmin=0, vmax=0.69, alpha=0.5)\n\n    # Get predicted labels on training data and plot\n    axs[2].scatter(\n        dataset[:, 0],\n        dataset[:, 1],\n        c=labels,\n        cmap=color_map,\n        s=s,\n        lw=0,\n    )\n    plt.colorbar(cont, ax=axs[2], label=\"Epistemic uncertainty\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a function that will create an ensemble with TopK or Greedy selection strategies.\nThis function also has a parameter ``k`` that sets the number of unique member in the ensemble.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_ensemble_from_checkpoints(ensemble_selector: str = \"topk\", k=20):\n\n    # 0. Load data\n    _, (vx, vy), _ = load_data()\n\n    # !1.3 SKLEARN EXAMPLE\n    predictor_files = SklearnPredictorFileLoader.find_predictor_files(\n        model_checkpoint_dir\n    )\n    predictor_loaders = [SklearnPredictorFileLoader(f) for f in predictor_files]\n    predictors = [p.load() for p in predictor_loaders]\n\n    # 2. Build an ensemble\n    ensemble = EnsemblePredictor(\n        predictors=predictors,\n        aggregator=MixedCategoricalAggregator(\n            uncertainty_method=\"entropy\",\n            decomposed_uncertainty=True,\n        ),\n        # You can specify parallel backends for the evaluation of the ensemble\n        evaluator={\n            \"method\": \"ray\",\n            \"method_kwargs\": {\"num_cpus_per_task\": 1},\n        },\n    )\n    y_predictors = ensemble.predictions_from_predictors(\n        vx, predictors=ensemble.predictors\n    )\n\n    # Use TopK or Greedy/Caruana\n    if ensemble_selector == \"topk\":\n        selector = TopKSelector(\n            loss_func=CategoricalCrossEntropy(),\n            k=20,\n        )\n    elif ensemble_selector == \"greedy\":\n        selector = GreedySelector(\n            loss_func=CategoricalCrossEntropy(),\n            aggregator=MixedCategoricalAggregator(),\n            k=20,\n            k_init=5,\n            max_it=100,\n            early_stopping=False,\n            bagging=True,\n            eps_tol=1e-5,\n        )\n    else:\n        raise ValueError(f\"Unknown ensemble_selector: {ensemble_selector}\")\n\n    selected_predictors_indexes, selected_predictors_weights = selector.select(\n        vy, y_predictors\n    )\n    print(f\"{selected_predictors_indexes=}\")\n    print(f\"{selected_predictors_weights=}\")\n\n    ensemble.predictors = [ensemble.predictors[i] for i in selected_predictors_indexes]\n    ensemble.weights = selected_predictors_weights\n\n    return ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by testing the Topk strategy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ensemble = create_ensemble_from_checkpoints(\"topk\")\nty_pred = ensemble.predict(tx)[\"loc\"]\ncce = log_loss(ty, ty_pred)\nacc = accuracy_score(ty, np.argmax(ty_pred, axis=1))\nprint(f\"Test scores: {cce=:.3f}, {acc=:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Plot decision boundary and uncertainty for ensemble\nplot_decision_boundary_and_uncertainty(tx, ty, ensemble, steps=1000, color_map=\"viridis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Plot calibration for ensemble\nfig, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS), tight_layout=True)\ndisp = CalibrationDisplay.from_predictions(ty, ty_pred[:, 1], ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We do the same for the Greedy strategy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ensemble = create_ensemble_from_checkpoints(\"greedy\")\nty_pred = ensemble.predict(tx)[\"loc\"]\ncce = log_loss(ty, ty_pred)\nacc = accuracy_score(ty, np.argmax(ty_pred, axis=1))\nprint(f\"Test scores: {cce=:.3f}, {acc=:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sphinx_gallery_thumbnail_number = 8\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Plot decision boundary and uncertainty for ensemble\nplot_decision_boundary_and_uncertainty(tx, ty, ensemble, steps=1000, color_map=\"viridis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Plot calibration for ensemble\nfig, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS), tight_layout=True)\ndisp = CalibrationDisplay.from_predictions(ty, ty_pred[:, 1], ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In conclusion, the improvement over the default hyperparameters is significant.\n\nFor CCE, we improved from about 6 to 0.4.\n\nFor Accuracy, we improved from 0.82 to 0.87.\n\nNot only that we have disentangled uncertainty estimates. The epistemic uncertainty is informative of locations where we are missing data and the aleatoric uncertainty is informative of the noise level in the labels.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}