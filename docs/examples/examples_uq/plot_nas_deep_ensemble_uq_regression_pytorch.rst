
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/examples_uq/plot_nas_deep_ensemble_uq_regression_pytorch.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_examples_uq_plot_nas_deep_ensemble_uq_regression_pytorch.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_examples_uq_plot_nas_deep_ensemble_uq_regression_pytorch.py:


Neural Architecture Search and Deep Ensemble with Uncertainty Quantification for Regression (Pytorch)
=====================================================================================================

**Author(s)**: Romain Egele, Brett Eiffert.

In this tutorial, you will learn how to perform **Neural Architecture Search (NAS)** and use it to construct a diverse deep ensemble with disentangled **aleatoric** and **epistemic uncertainty**.
 
NAS is the idea of automatically optimizing the architecture of deep neural networks to solve a given task. Here, we will use **hyperparameter optimization (HPO)** algorithms to guide the NAS process.

Specifically, in this tutorial you will learn how to:

1.      **Define a customizable PyTorch module** that exposes neural architecture hyperparameters.
2.      **Define constraints** on the neural architecture hyperparameters to reduce redundancies and improve efficiency of the optimization.

This tutorial will provide a hands-on approach to leveraging NAS for robust regression models with well-calibrated uncertainty estimates.

.. GENERATED FROM PYTHON SOURCE LINES 20-29

Installation and imports
------------------------

Installing dependencies with the :ref:`pip installation <install-pip>` is recommended. It requires **Python >= 3.10**.

.. code-block:: bash

    %%bash
    pip install "deephyper[ray,torch]"

.. GENERATED FROM PYTHON SOURCE LINES 31-47

.. dropdown:: Code (Import statements)

    .. code-block:: Python


        import json
        import os
        import pathlib

        import matplotlib.pyplot as plt
        import numpy as np
        import pandas as pd

        from sklearn.model_selection import train_test_split
        from tqdm import tqdm

        WIDTH_PLOTS = 8
        HEIGHT_PLOTS = WIDTH_PLOTS / 1.618








.. GENERATED FROM PYTHON SOURCE LINES 48-63

Synthetic data generation
-------------------------

We generate synthetic data from a 1D scalar function :math:`Y = f(X) + \epsilon(X)`, where :math:`X,Y` are random variables with support :math:`\mathbb{R}`.

The training data are drown uniformly from :math:`X \sim U([-30,-15] \cup [15,30])` with:

.. math::

    f(x) = \cos(x/2) + 2 \cdot \sin(x/10) + x/100

and :math:`\epsilon(X) \sim \mathcal{N}(0, \sigma(X))` with:

- :math:`\sigma(x) = 0.5` if :math:`x \in [-30,-15]`
- :math:`\sigma(x) = 1.0` if :math:`x \in [15,30]`

.. GENERATED FROM PYTHON SOURCE LINES 63-122

.. dropdown:: Code (Loading synthetic data)

    .. code-block:: Python


        def load_data(
            developement_size=500,
            test_size=200,
            random_state=42,
            x_min=-50,
            x_max=50,
        ):
            rs = np.random.RandomState(random_state)

            def f(x):
                return np.cos(x / 2) + 2 * np.sin(x / 10) + x / 100

            x_1 = rs.uniform(low=-30, high=-15.0, size=developement_size // 2)
            eps_1 = rs.normal(loc=0.0, scale=0.5, size=developement_size // 2)
            y_1 = f(x_1) + eps_1

            x_2 = rs.uniform(low=15.0, high=30.0, size=developement_size // 2)
            eps_2 = rs.normal(loc=0.0, scale=1.0, size=developement_size // 2)
            y_2 = f(x_2) + eps_2

            x = np.concatenate([x_1, x_2], axis=0)
            y = np.concatenate([y_1, y_2], axis=0)

            test_X = np.linspace(x_min, x_max, test_size)
            test_y = f(test_X)

            x = x.reshape(-1, 1)
            y = y.reshape(-1, 1)

            train_X, valid_X, train_y, valid_y = train_test_split(
                x, y, test_size=0.33, random_state=random_state
            )

            test_X = test_X.reshape(-1, 1)
            test_y = test_y.reshape(-1, 1)

            return (train_X, train_y), (valid_X, valid_y), (test_X, test_y)


        (train_X, train_y), (valid_X, valid_y), (test_X, test_y) = load_data()

        y_mu, y_std = np.mean(train_y), np.std(train_y)

        x_lim, y_lim = 50, 7
        _ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))
        _ = plt.scatter(train_X, train_y, s=5, label="Training")
        _ = plt.scatter(valid_X, valid_y, s=5, label="Validation")
        _ = plt.plot(test_X, test_y, linestyle="--", color="gray", label="Test")
        _ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.25)
        _ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.25)
        _ = plt.xlim(-x_lim, x_lim)
        _ = plt.ylim(-y_lim, y_lim)
        _ = plt.legend()
        _ = plt.xlabel(r"$x$")
        _ = plt.ylabel(r"$f(x)$")
        _ = plt.grid(which="both", linestyle=":")




.. image-sg:: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_001.png
   :alt: plot nas deep ensemble uq regression pytorch
   :srcset: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 123-138

Configurable neural network with uncertainty
--------------------------------------------

We define a configurable Pytorch module to be able to explore:

- the number of layers
- the number of units per layer
- the activation function per layer
- the dropout rate
- the output layer

The output of this module will be a Gaussian distribution :math:`\mathcal{N}(\mu_\theta(x), \sigma_\theta(x))`, where :math:`\theta` represent the concatenation of the weights and the hyperparameters of our model.

The uncertainty :math:`\sigma_\theta(x)` estimated by the network is an estimator of :math:`V_Y[Y|X=x]` therefore corresponding
to aleatoric uncertainty (a.k.a., intrinsic noise).

.. GENERATED FROM PYTHON SOURCE LINES 138-219

.. code-block:: Python


    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, TensorDataset


    class DeepNormalRegressor(nn.Module):
        def __init__(
            self,
            n_inputs,
            layers,
            n_units_mean=64,
            n_units_std=64,
            std_offset=1e-3,
            softplus_factor=0.05,
            loc=0,
            scale=1.0,
        ):
            super().__init__()

            layers_ = []
            prev_n_units = n_inputs
            for n_units, activation, dropout_rate in layers:
                linear_layer = nn.Linear(prev_n_units, n_units)
                if activation == "relu":
                    activation_layer = nn.ReLU()
                elif activation == "sigmoid":
                    activation_layer = nn.Sigmoid()
                elif activation == "tanh":
                    activation_layer = nn.Tanh()
                elif activation == "swish":
                    activation_layer = nn.SiLU()
                elif activation == "mish":
                    activation_layer = nn.Mish()
                elif activation == "gelu":
                    activation_layer = nn.GELU()
                elif activation == "silu":
                    activation_layer = nn.SiLU()
                dropout_layer = nn.Dropout(dropout_rate)

                layers_.extend([linear_layer, activation_layer, dropout_layer])

                prev_n_units = n_units

            # Shared parameters
            self.shared_layer = nn.Sequential(
                *layers_,
            )

            # Mean parameters
            self.mean_layer = nn.Sequential(
                nn.Linear(prev_n_units, n_units_mean),
                nn.ReLU(),
                nn.Linear(n_units_mean, 1),
            )

            # Standard deviation parameters
            self.std_layer = nn.Sequential(
                nn.Linear(prev_n_units, n_units_std),
                nn.ReLU(),
                nn.Linear(n_units_std, 1),
                nn.Softplus(beta=1.0, threshold=20.0),  # enforces positivity
            )

            self.std_offset = std_offset
            self.softplus_factor = softplus_factor
            self.loc = loc
            self.scale = scale

        def forward(self, x):
            # Shared embedding
            shared = self.shared_layer(x)

            # Parametrization of the mean
            mu = self.mean_layer(shared) + self.loc

            # Parametrization of the standard deviation
            sigma = self.std_offset + self.std_layer(self.softplus_factor * shared) * self.scale

            return torch.distributions.Normal(mu, sigma)








.. GENERATED FROM PYTHON SOURCE LINES 220-231

Hyperparameter search space
---------------------------

We define the hyperparameter space that includes both **neural architecture** and **training hyperparameters**.

Without having a good heuristic on training hyperparameters given the neural architecture hyperparameter search space 
it is important to define them jointly with the neural architecture hyperparameters as they can have strong interactions. 

In the definition of the hyperparameter space, we add constraints using :class:`ConfigSpace.GreaterThanCondition` to
represent when an hyperparameter is active. In this example, "active" means it actually influence the code execution of
the trained model.

.. GENERATED FROM PYTHON SOURCE LINES 231-285

.. code-block:: Python


    from ConfigSpace import GreaterThanCondition
    from deephyper.hpo import HpProblem


    def create_hpo_problem(min_num_layers=3, max_num_layers=8, max_num_units=512):
        problem = HpProblem()

        # Neural Architecture Hyperparameters
        num_layers = problem.add_hyperparameter((min_num_layers, max_num_layers), "num_layers", default_value=5)

        conditions = []
        for i in range(max_num_layers):

            # Adding the hyperparameters that impact each layer of the model
            layer_i_units = problem.add_hyperparameter((16, max_num_units), f"layer_{i}_units", default_value=max_num_units)
            layer_i_activation = problem.add_hyperparameter(
                ["relu", "sigmoid", "tanh", "swish", "mish", "gelu", "silu"],
                f"layer_{i}_activation",
                default_value="relu",
            )
            layer_i_dropout_rate = problem.add_hyperparameter(
                (0.0, 0.25), f"layer_{i}_dropout_rate", default_value=0.0
            )

            # Adding the constraints to define when these hyperparameters are active
            if i + 1 > min_num_layers:
                conditions.extend(
                    [
                        GreaterThanCondition(layer_i_units, num_layers, i),
                        GreaterThanCondition(layer_i_activation, num_layers, i),
                        GreaterThanCondition(layer_i_dropout_rate, num_layers, i),
                    ]
                )

        problem.add_conditions(conditions)

        # Hyperparameters of the output layers
        problem.add_hyperparameter((16, max_num_units), "n_units_mean", default_value=max_num_units)
        problem.add_hyperparameter((16, max_num_units), "n_units_std", default_value=max_num_units)
        problem.add_hyperparameter((1e-8, 1e-2, "log-uniform"), "std_offset", default_value=1e-3)
        problem.add_hyperparameter((0.01, 1.0), "softplus_factor", default_value=0.05)

        # Training Hyperparameters
        problem.add_hyperparameter((1e-5, 1e-1, "log-uniform"), "learning_rate", default_value=2e-3)
        problem.add_hyperparameter((8, 256, "log-uniform"), "batch_size", default_value=32)
        problem.add_hyperparameter((0.01, 0.99), "lr_scheduler_factor", default_value=0.1)
        problem.add_hyperparameter((10, 100), "lr_scheduler_patience", default_value=20)

        return problem

    problem = create_hpo_problem()
    problem





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Configuration space object:
      Hyperparameters:
        batch_size, Type: UniformInteger, Range: [8, 256], Default: 32, on log-scale
        layer_0_activation, Type: Categorical, Choices: {relu, sigmoid, tanh, swish, mish, gelu, silu}, Default: relu
        layer_0_dropout_rate, Type: UniformFloat, Range: [0.0, 0.25], Default: 0.0
        layer_0_units, Type: UniformInteger, Range: [16, 512], Default: 512
        layer_1_activation, Type: Categorical, Choices: {relu, sigmoid, tanh, swish, mish, gelu, silu}, Default: relu
        layer_1_dropout_rate, Type: UniformFloat, Range: [0.0, 0.25], Default: 0.0
        layer_1_units, Type: UniformInteger, Range: [16, 512], Default: 512
        layer_2_activation, Type: Categorical, Choices: {relu, sigmoid, tanh, swish, mish, gelu, silu}, Default: relu
        layer_2_dropout_rate, Type: UniformFloat, Range: [0.0, 0.25], Default: 0.0
        layer_2_units, Type: UniformInteger, Range: [16, 512], Default: 512
        layer_3_activation, Type: Categorical, Choices: {relu, sigmoid, tanh, swish, mish, gelu, silu}, Default: relu
        layer_3_dropout_rate, Type: UniformFloat, Range: [0.0, 0.25], Default: 0.0
        layer_3_units, Type: UniformInteger, Range: [16, 512], Default: 512
        layer_4_activation, Type: Categorical, Choices: {relu, sigmoid, tanh, swish, mish, gelu, silu}, Default: relu
        layer_4_dropout_rate, Type: UniformFloat, Range: [0.0, 0.25], Default: 0.0
        layer_4_units, Type: UniformInteger, Range: [16, 512], Default: 512
        layer_5_activation, Type: Categorical, Choices: {relu, sigmoid, tanh, swish, mish, gelu, silu}, Default: relu
        layer_5_dropout_rate, Type: UniformFloat, Range: [0.0, 0.25], Default: 0.0
        layer_5_units, Type: UniformInteger, Range: [16, 512], Default: 512
        layer_6_activation, Type: Categorical, Choices: {relu, sigmoid, tanh, swish, mish, gelu, silu}, Default: relu
        layer_6_dropout_rate, Type: UniformFloat, Range: [0.0, 0.25], Default: 0.0
        layer_6_units, Type: UniformInteger, Range: [16, 512], Default: 512
        layer_7_activation, Type: Categorical, Choices: {relu, sigmoid, tanh, swish, mish, gelu, silu}, Default: relu
        layer_7_dropout_rate, Type: UniformFloat, Range: [0.0, 0.25], Default: 0.0
        layer_7_units, Type: UniformInteger, Range: [16, 512], Default: 512
        learning_rate, Type: UniformFloat, Range: [1e-05, 0.1], Default: 0.002, on log-scale
        lr_scheduler_factor, Type: UniformFloat, Range: [0.01, 0.99], Default: 0.1
        lr_scheduler_patience, Type: UniformInteger, Range: [10, 100], Default: 20
        n_units_mean, Type: UniformInteger, Range: [16, 512], Default: 512
        n_units_std, Type: UniformInteger, Range: [16, 512], Default: 512
        num_layers, Type: UniformInteger, Range: [3, 8], Default: 5
        softplus_factor, Type: UniformFloat, Range: [0.01, 1.0], Default: 0.05
        std_offset, Type: UniformFloat, Range: [1e-08, 0.01], Default: 0.001, on log-scale
      Conditions:
        layer_3_activation | num_layers > 3
        layer_3_dropout_rate | num_layers > 3
        layer_3_units | num_layers > 3
        layer_4_activation | num_layers > 4
        layer_4_dropout_rate | num_layers > 4
        layer_4_units | num_layers > 4
        layer_5_activation | num_layers > 5
        layer_5_dropout_rate | num_layers > 5
        layer_5_units | num_layers > 5
        layer_6_activation | num_layers > 6
        layer_6_dropout_rate | num_layers > 6
        layer_6_units | num_layers > 6
        layer_7_activation | num_layers > 7
        layer_7_dropout_rate | num_layers > 7
        layer_7_units | num_layers > 7




.. GENERATED FROM PYTHON SOURCE LINES 286-302

Loss and Metric
---------------

For the loss we will use the Gaussian negative log-likelihood to evalute the quality of the 
predicted distribution :math:`\mathcal{N}(\mu_\theta(x), \sigma_\theta(x))` using with formula:

.. math::

    L_\text{NLL}(x, y;\theta) = \frac{1}{2}\left(\log\left(\sigma_\theta^{2}(x)\right) + \frac{\left(y-\mu_{\theta}(x)\right)^{2}}{\sigma_{\theta}^{2}(x)}\right) + \text{cst}

As complementary metric, we use the squared error to evaluate the quality of the mean predictions :math:`\mu_\theta(x)`:

.. math::

    L_\text{SE}(x, y;\theta) = (\mu_\theta(x)-y)^2


.. GENERATED FROM PYTHON SOURCE LINES 302-322

.. code-block:: Python

    def nll(y, rv_y):
        """Negative log likelihood for Pytorch distribution.

        Args:
            y: true data.
            rv_y: learned (predicted) probability distribution.
        """
        return -rv_y.log_prob(y)


    def squared_error(y_true, rv_y):
        """Squared error for Pytorch distribution.

        Args:
            y: true data.
            rv_y: learned (predicted) probability distribution.
        """
        y_pred = rv_y.mean
        return (y_true - y_pred) ** 2








.. GENERATED FROM PYTHON SOURCE LINES 323-331

Training loop
-------------

In our training loop, we make sure to collect training and validation learning curves for better analysis.

We also add a mechanism to checkpoint weights of the model based on the best observed validation loss.

Finally, we add an early stopping mechanism to save computing resources.

.. GENERATED FROM PYTHON SOURCE LINES 331-423

.. dropdown:: Code (Training loop)

    .. code-block:: Python


        def train_one_step(model, optimizer, x_batch, y_batch):
            model.train()
            optimizer.zero_grad()
            y_dist = model(x_batch)

            loss = torch.mean(nll(y_batch, y_dist))
            mse = torch.mean(squared_error(y_batch, y_dist))

            loss.backward()
            optimizer.step()

            return loss, mse


        def train(
            job,
            model,
            optimizer,
            x_train,
            x_val,
            y_train,
            y_val,
            n_epochs,
            batch_size,
            scheduler=None,
            patience=200,
            progressbar=True,
        ):
            data_train = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)

            checkpointed_state_dict = model.state_dict()
            checkpointed_val_loss = np.inf

            train_loss, val_loss = [], []
            train_mse, val_mse = [], []

            tqdm_bar = tqdm(total=n_epochs, disable=not progressbar)

            for epoch in range(n_epochs):
                batch_losses_t, batch_losses_v, batch_mse_t, batch_mse_v = [], [], [], []

                for batch_x, batch_y in data_train:
                    b_train_loss, b_train_mse = train_one_step(model, optimizer, batch_x, batch_y)

                    model.eval()
                    y_dist = model(x_val)
                    b_val_loss = torch.mean(nll(y_val, y_dist))
                    b_val_mse = torch.mean(squared_error(y_val, y_dist))

                    batch_losses_t.append(to_numpy(b_train_loss))
                    batch_mse_t.append(to_numpy(b_train_mse))
                    batch_losses_v.append(to_numpy(b_val_loss))
                    batch_mse_v.append(to_numpy(b_val_mse))

                train_loss.append(np.mean(batch_losses_t))
                val_loss.append(np.mean(batch_losses_v))
                train_mse.append(np.mean(batch_mse_t))
                val_mse.append(np.mean(batch_mse_v))

                if scheduler is not None:
                    scheduler.step(val_loss[-1])

                tqdm_bar.update(1)
                tqdm_bar.set_postfix(
                    {
                        "train_loss": f"{train_loss[-1]:.3f}",
                        "val_loss": f"{val_loss[-1]:.3f}",
                        "train_mse": f"{train_mse[-1]:.3f}",
                        "val_mse": f"{val_mse[-1]:.3f}",
                    }
                )

                # Checkpoint weights if they improve
                if val_loss[-1] < checkpointed_val_loss:
                    checkpointed_val_loss = val_loss[-1]
                    checkpointed_state_dict = model.state_dict()

                # Early discarding
                job.record(budget=epoch+1, objective=-val_loss[-1])
                if job.stopped():
                    break

                if len(val_loss) > (patience + 1) and val_loss[-patience - 1] < min(val_loss[-patience:]):
                    break

            # Reload the best weights
            model.load_state_dict(checkpointed_state_dict)

            return train_loss, val_loss, train_mse, val_mse








.. GENERATED FROM PYTHON SOURCE LINES 424-426

Run time
--------

.. GENERATED FROM PYTHON SOURCE LINES 426-438

.. code-block:: Python

    import multiprocessing

    dtype = torch.float32
    if torch.cuda.is_available():
        device = "cuda"
        device_count = 1
    else:
        device = "cpu"
        device_count = multiprocessing.cpu_count()

    print(f"Runtime with {device=}, {device_count=}, {dtype=}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Runtime with device='cpu', device_count=10, dtype=torch.float32




.. GENERATED FROM PYTHON SOURCE LINES 439-448

.. dropdown:: Code (Conversion utility functions)

    .. code-block:: Python



        def to_torch(array):
            return torch.from_numpy(array).to(device=device, dtype=dtype)

        def to_numpy(tensor):
            return tensor.detach().cpu().numpy()








.. GENERATED FROM PYTHON SOURCE LINES 449-453

Evaluation function
-------------------

We start by defining a function that will create the Torch module from a dictionnary of hyperparameters.

.. GENERATED FROM PYTHON SOURCE LINES 453-476

.. code-block:: Python



    def create_model(parameters: dict, y_mu=0, y_std=1):
        num_layers = parameters["num_layers"]
        torch_module = DeepNormalRegressor(
            n_inputs=1,
            layers=[
                (
                    parameters[f"layer_{i}_units"],
                    parameters[f"layer_{i}_activation"],
                    parameters[f"layer_{i}_dropout_rate"],
                )
                for i in range(num_layers)
            ],
            n_units_mean=parameters["n_units_mean"],
            n_units_std=parameters["n_units_std"],
            std_offset=parameters["std_offset"],
            softplus_factor=parameters["softplus_factor"],
            loc=y_mu,
            scale=y_std,
        ).to(device=device, dtype=dtype)
        return torch_module








.. GENERATED FROM PYTHON SOURCE LINES 477-480

The evaluation function (often called ``run``-function in DeepHyper) is the function that 
receives suggested parameters as inputs ``job.parameters`` and returns an ``"objective"`` 
that we want to maximize.

.. GENERATED FROM PYTHON SOURCE LINES 481-542

.. code-block:: Python


    max_n_epochs = 1_000


    def run(job, model_checkpoint_dir=".", verbose=False):
        (x, y), (vx, vy), (tx, ty) = load_data()

        # Create the model based on neural architecture hyperparameters
        model = create_model(job.parameters, y_mu, y_std)

        if verbose:
            print(model)

        # Initialize training loop based on training hyperparameters
        optimizer = torch.optim.Adam(model.parameters(), lr=job.parameters["learning_rate"])
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            factor=job.parameters["lr_scheduler_factor"],
            patience=job.parameters["lr_scheduler_patience"],
        )

        x, vx, tx = to_torch(x), to_torch(vx), to_torch(tx)
        y, vy, ty = to_torch(y), to_torch(vy), to_torch(ty)

        try:
            train_losses, val_losses, train_mse, val_mse = train(
                job,
                model,
                optimizer,
                x,
                vx,
                y,
                vy,
                n_epochs=max_n_epochs,
                batch_size=job.parameters["batch_size"],
                scheduler=scheduler,
                progressbar=verbose,
            )
        except Exception:
            return "F_fit"

        ty_pred = model(tx)
        test_loss = to_numpy(torch.mean(nll(ty, ty_pred)))
        test_mse = to_numpy(torch.mean(squared_error(ty, ty_pred)))

        # Saving the model's state (i.e., weights)
        torch.save(model.state_dict(), os.path.join(model_checkpoint_dir, f"model_{job.id}.pt"))

        return {
            "objective": -val_losses[-1],
            "metadata": {
                "train_loss": train_losses,
                "val_loss": val_losses,
                "train_mse": train_mse,
                "val_mse": val_mse,
                "test_loss": test_loss,
                "test_mse": test_mse,
                "budget": len(val_losses),
            },
        }








.. GENERATED FROM PYTHON SOURCE LINES 543-548

Evaluation of the baseline
--------------------------

We evaluate the default configuration of hyperparameters that we call "baseline" using the same evaluation function.
This allows to test the evaluation function.

.. GENERATED FROM PYTHON SOURCE LINES 548-569

.. code-block:: Python


    from deephyper.evaluator import RunningJob

    baseline_dir = "nas_baseline_regression"

    def evaluate_baseline(problem):
        model_checkpoint_dir = os.path.join(baseline_dir, "models")
        pathlib.Path(model_checkpoint_dir).mkdir(parents=True, exist_ok=True)

        default_parameters = problem.default_configuration
        print(f"{default_parameters=}\n")

        result = run(
            RunningJob(parameters=default_parameters),
            model_checkpoint_dir=model_checkpoint_dir,
            verbose=True,
        )
        return result

    baseline_results = evaluate_baseline(problem)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    default_parameters={'batch_size': 32, 'layer_0_activation': 'relu', 'layer_0_dropout_rate': 0.0, 'layer_0_units': 512, 'layer_1_activation': 'relu', 'layer_1_dropout_rate': 0.0, 'layer_1_units': 512, 'layer_2_activation': 'relu', 'layer_2_dropout_rate': 0.0, 'layer_2_units': 512, 'learning_rate': 0.002, 'lr_scheduler_factor': 0.1, 'lr_scheduler_patience': 20, 'n_units_mean': 512, 'n_units_std': 512, 'num_layers': 5, 'softplus_factor': 0.05, 'std_offset': 0.001, 'layer_3_activation': 'relu', 'layer_3_dropout_rate': 0.0, 'layer_3_units': 512, 'layer_4_activation': 'relu', 'layer_4_dropout_rate': 0.0, 'layer_4_units': 512, 'layer_5_activation': 'relu', 'layer_5_dropout_rate': 0.0, 'layer_5_units': 16, 'layer_6_activation': 'relu', 'layer_6_dropout_rate': 0.0, 'layer_6_units': 16, 'layer_7_activation': 'relu', 'layer_7_dropout_rate': 0.0, 'layer_7_units': 16}

    DeepNormalRegressor(
      (shared_layer): Sequential(
        (0): Linear(in_features=1, out_features=512, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): ReLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): Linear(in_features=512, out_features=512, bias=True)
        (7): ReLU()
        (8): Dropout(p=0.0, inplace=False)
        (9): Linear(in_features=512, out_features=512, bias=True)
        (10): ReLU()
        (11): Dropout(p=0.0, inplace=False)
        (12): Linear(in_features=512, out_features=512, bias=True)
        (13): ReLU()
        (14): Dropout(p=0.0, inplace=False)
      )
      (mean_layer): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=1, bias=True)
      )
      (std_layer): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=1, bias=True)
        (3): Softplus(beta=1.0, threshold=20.0)
      )
    )
<<<<<<< HEAD
      0%|          | 0/1000 [00:00<?, ?it/s]      0%|          | 1/1000 [00:00<01:22, 12.05it/s, train_loss=3.408, val_loss=3.440, train_mse=8.728, val_mse=8.889]      0%|          | 2/1000 [00:00<01:09, 14.42it/s, train_loss=3.408, val_loss=3.440, train_mse=8.728, val_mse=8.889]      0%|          | 2/1000 [00:00<01:09, 14.42it/s, train_loss=1.790, val_loss=1.811, train_mse=1.921, val_mse=2.098]      0%|          | 3/1000 [00:00<01:09, 14.42it/s, train_loss=1.687, val_loss=1.790, train_mse=1.675, val_mse=2.094]      0%|          | 4/1000 [00:00<01:00, 16.33it/s, train_loss=1.687, val_loss=1.790, train_mse=1.675, val_mse=2.094]      0%|          | 4/1000 [00:00<01:00, 16.33it/s, train_loss=1.698, val_loss=1.814, train_mse=1.766, val_mse=2.152]      0%|          | 5/1000 [00:00<01:00, 16.33it/s, train_loss=1.684, val_loss=1.761, train_mse=1.691, val_mse=1.960]      1%|          | 6/1000 [00:00<01:26, 11.43it/s, train_loss=1.684, val_loss=1.761, train_mse=1.691, val_mse=1.960]      1%|          | 6/1000 [00:00<01:26, 11.43it/s, train_loss=1.642, val_loss=1.764, train_mse=1.566, val_mse=1.975]      1%|          | 7/1000 [00:00<01:26, 11.43it/s, train_loss=1.649, val_loss=1.741, train_mse=1.588, val_mse=1.892]      1%|          | 8/1000 [00:00<01:15, 13.15it/s, train_loss=1.649, val_loss=1.741, train_mse=1.588, val_mse=1.892]      1%|          | 8/1000 [00:00<01:15, 13.15it/s, train_loss=1.659, val_loss=1.742, train_mse=1.624, val_mse=1.897]      1%|          | 9/1000 [00:00<01:15, 13.15it/s, train_loss=1.628, val_loss=1.737, train_mse=1.528, val_mse=1.878]      1%|          | 10/1000 [00:00<01:06, 14.88it/s, train_loss=1.628, val_loss=1.737, train_mse=1.528, val_mse=1.878]      1%|          | 10/1000 [00:00<01:06, 14.88it/s, train_loss=1.655, val_loss=1.764, train_mse=1.617, val_mse=1.979]      1%|          | 11/1000 [00:00<01:06, 14.88it/s, train_loss=1.641, val_loss=1.761, train_mse=1.558, val_mse=1.948]      1%|          | 12/1000 [00:00<01:03, 15.49it/s, train_loss=1.641, val_loss=1.761, train_mse=1.558, val_mse=1.948]      1%|          | 12/1000 [00:00<01:03, 15.49it/s, train_loss=1.657, val_loss=1.740, train_mse=1.626, val_mse=1.888]      1%|▏         | 13/1000 [00:00<01:03, 15.49it/s, train_loss=1.664, val_loss=1.742, train_mse=1.631, val_mse=1.891]      1%|▏         | 14/1000 [00:00<01:07, 14.67it/s, train_loss=1.664, val_loss=1.742, train_mse=1.631, val_mse=1.891]      1%|▏         | 14/1000 [00:00<01:07, 14.67it/s, train_loss=1.655, val_loss=1.764, train_mse=1.600, val_mse=1.966]      2%|▏         | 15/1000 [00:01<01:07, 14.67it/s, train_loss=1.616, val_loss=1.704, train_mse=1.474, val_mse=1.765]      2%|▏         | 16/1000 [00:01<01:04, 15.35it/s, train_loss=1.616, val_loss=1.704, train_mse=1.474, val_mse=1.765]      2%|▏         | 16/1000 [00:01<01:04, 15.35it/s, train_loss=1.593, val_loss=1.691, train_mse=1.405, val_mse=1.718]      2%|▏         | 17/1000 [00:01<01:04, 15.35it/s, train_loss=1.609, val_loss=1.703, train_mse=1.454, val_mse=1.747]      2%|▏         | 18/1000 [00:01<01:02, 15.65it/s, train_loss=1.609, val_loss=1.703, train_mse=1.454, val_mse=1.747]      2%|▏         | 18/1000 [00:01<01:02, 15.65it/s, train_loss=1.649, val_loss=1.699, train_mse=1.575, val_mse=1.732]      2%|▏         | 19/1000 [00:01<01:02, 15.65it/s, train_loss=1.596, val_loss=1.697, train_mse=1.411, val_mse=1.736]      2%|▏         | 20/1000 [00:01<01:02, 15.71it/s, train_loss=1.596, val_loss=1.697, train_mse=1.411, val_mse=1.736]      2%|▏         | 20/1000 [00:01<01:02, 15.71it/s, train_loss=1.547, val_loss=1.649, train_mse=1.274, val_mse=1.580]      2%|▏         | 21/1000 [00:01<01:02, 15.71it/s, train_loss=1.554, val_loss=1.663, train_mse=1.303, val_mse=1.602]      2%|▏         | 22/1000 [00:01<01:04, 15.08it/s, train_loss=1.554, val_loss=1.663, train_mse=1.303, val_mse=1.602]      2%|▏         | 22/1000 [00:01<01:04, 15.08it/s, train_loss=1.553, val_loss=1.661, train_mse=1.303, val_mse=1.596]      2%|▏         | 23/1000 [00:01<01:04, 15.08it/s, train_loss=1.565, val_loss=1.659, train_mse=1.335, val_mse=1.594]      2%|▏         | 24/1000 [00:01<01:06, 14.65it/s, train_loss=1.565, val_loss=1.659, train_mse=1.335, val_mse=1.594]      2%|▏         | 24/1000 [00:01<01:06, 14.65it/s, train_loss=1.551, val_loss=1.646, train_mse=1.297, val_mse=1.563]      2%|▎         | 25/1000 [00:01<01:06, 14.65it/s, train_loss=1.547, val_loss=1.641, train_mse=1.288, val_mse=1.547]      3%|▎         | 26/1000 [00:01<01:05, 14.97it/s, train_loss=1.547, val_loss=1.641, train_mse=1.288, val_mse=1.547]      3%|▎         | 26/1000 [00:01<01:05, 14.97it/s, train_loss=1.532, val_loss=1.648, train_mse=1.250, val_mse=1.566]      3%|▎         | 27/1000 [00:01<01:04, 14.97it/s, train_loss=1.536, val_loss=1.648, train_mse=1.261, val_mse=1.564]      3%|▎         | 28/1000 [00:01<01:04, 15.13it/s, train_loss=1.536, val_loss=1.648, train_mse=1.261, val_mse=1.564]      3%|▎         | 28/1000 [00:01<01:04, 15.13it/s, train_loss=1.539, val_loss=1.669, train_mse=1.270, val_mse=1.610]      3%|▎         | 29/1000 [00:01<01:04, 15.13it/s, train_loss=1.555, val_loss=1.644, train_mse=1.314, val_mse=1.546]      3%|▎         | 30/1000 [00:02<01:01, 15.71it/s, train_loss=1.555, val_loss=1.644, train_mse=1.314, val_mse=1.546]      3%|▎         | 30/1000 [00:02<01:01, 15.71it/s, train_loss=1.535, val_loss=1.654, train_mse=1.256, val_mse=1.577]      3%|▎         | 31/1000 [00:02<01:01, 15.71it/s, train_loss=1.578, val_loss=1.672, train_mse=1.377, val_mse=1.625]      3%|▎         | 32/1000 [00:02<00:58, 16.60it/s, train_loss=1.578, val_loss=1.672, train_mse=1.377, val_mse=1.625]      3%|▎         | 32/1000 [00:02<00:58, 16.60it/s, train_loss=1.540, val_loss=1.637, train_mse=1.272, val_mse=1.535]      3%|▎         | 33/1000 [00:02<00:58, 16.60it/s, train_loss=1.544, val_loss=1.641, train_mse=1.281, val_mse=1.547]      3%|▎         | 34/1000 [00:02<00:57, 16.94it/s, train_loss=1.544, val_loss=1.641, train_mse=1.281, val_mse=1.547]      3%|▎         | 34/1000 [00:02<00:57, 16.94it/s, train_loss=1.549, val_loss=1.643, train_mse=1.290, val_mse=1.548]      4%|▎         | 35/1000 [00:02<00:56, 16.94it/s, train_loss=1.541, val_loss=1.629, train_mse=1.275, val_mse=1.509]      4%|▎         | 36/1000 [00:02<00:55, 17.44it/s, train_loss=1.541, val_loss=1.629, train_mse=1.275, val_mse=1.509]      4%|▎         | 36/1000 [00:02<00:55, 17.44it/s, train_loss=1.525, val_loss=1.631, train_mse=1.234, val_mse=1.510]      4%|▎         | 37/1000 [00:02<00:55, 17.44it/s, train_loss=1.527, val_loss=1.646, train_mse=1.239, val_mse=1.543]      4%|▍         | 38/1000 [00:02<00:55, 17.27it/s, train_loss=1.527, val_loss=1.646, train_mse=1.239, val_mse=1.543]      4%|▍         | 38/1000 [00:02<00:55, 17.27it/s, train_loss=1.532, val_loss=1.648, train_mse=1.250, val_mse=1.546]      4%|▍         | 39/1000 [00:02<00:55, 17.27it/s, train_loss=1.546, val_loss=1.660, train_mse=1.287, val_mse=1.577]      4%|▍         | 40/1000 [00:02<00:55, 17.22it/s, train_loss=1.546, val_loss=1.660, train_mse=1.287, val_mse=1.577]      4%|▍         | 40/1000 [00:02<00:55, 17.22it/s, train_loss=1.542, val_loss=1.641, train_mse=1.277, val_mse=1.532]      4%|▍         | 41/1000 [00:02<00:55, 17.22it/s, train_loss=1.528, val_loss=1.639, train_mse=1.243, val_mse=1.527]      4%|▍         | 42/1000 [00:02<00:55, 17.14it/s, train_loss=1.528, val_loss=1.639, train_mse=1.243, val_mse=1.527]      4%|▍         | 42/1000 [00:02<00:55, 17.14it/s, train_loss=1.533, val_loss=1.632, train_mse=1.254, val_mse=1.508]      4%|▍         | 43/1000 [00:02<00:55, 17.14it/s, train_loss=1.528, val_loss=1.648, train_mse=1.240, val_mse=1.554]      4%|▍         | 44/1000 [00:02<00:55, 17.29it/s, train_loss=1.528, val_loss=1.648, train_mse=1.240, val_mse=1.554]      4%|▍         | 44/1000 [00:02<00:55, 17.29it/s, train_loss=1.542, val_loss=1.627, train_mse=1.276, val_mse=1.495]      4%|▍         | 45/1000 [00:02<00:55, 17.29it/s, train_loss=1.520, val_loss=1.634, train_mse=1.222, val_mse=1.512]      5%|▍         | 46/1000 [00:02<00:53, 17.77it/s, train_loss=1.520, val_loss=1.634, train_mse=1.222, val_mse=1.512]      5%|▍         | 46/1000 [00:02<00:53, 17.77it/s, train_loss=1.529, val_loss=1.628, train_mse=1.245, val_mse=1.494]      5%|▍         | 47/1000 [00:02<00:53, 17.77it/s, train_loss=1.513, val_loss=1.636, train_mse=1.205, val_mse=1.516]      5%|▍         | 48/1000 [00:03<00:52, 18.10it/s, train_loss=1.513, val_loss=1.636, train_mse=1.205, val_mse=1.516]      5%|▍         | 48/1000 [00:03<00:52, 18.10it/s, train_loss=1.534, val_loss=1.630, train_mse=1.255, val_mse=1.494]      5%|▍         | 49/1000 [00:03<00:52, 18.10it/s, train_loss=1.537, val_loss=1.637, train_mse=1.267, val_mse=1.512]      5%|▌         | 50/1000 [00:03<00:53, 17.68it/s, train_loss=1.537, val_loss=1.637, train_mse=1.267, val_mse=1.512]      5%|▌         | 50/1000 [00:03<00:53, 17.68it/s, train_loss=1.520, val_loss=1.630, train_mse=1.222, val_mse=1.500]      5%|▌         | 51/1000 [00:03<00:53, 17.68it/s, train_loss=1.528, val_loss=1.628, train_mse=1.244, val_mse=1.496]      5%|▌         | 52/1000 [00:03<00:53, 17.88it/s, train_loss=1.528, val_loss=1.628, train_mse=1.244, val_mse=1.496]      5%|▌         | 52/1000 [00:03<00:53, 17.88it/s, train_loss=1.526, val_loss=1.631, train_mse=1.237, val_mse=1.502]      5%|▌         | 53/1000 [00:03<00:52, 17.88it/s, train_loss=1.528, val_loss=1.630, train_mse=1.242, val_mse=1.496]      5%|▌         | 54/1000 [00:03<00:52, 18.05it/s, train_loss=1.528, val_loss=1.630, train_mse=1.242, val_mse=1.496]      5%|▌         | 54/1000 [00:03<00:52, 18.05it/s, train_loss=1.539, val_loss=1.629, train_mse=1.271, val_mse=1.492]      6%|▌         | 55/1000 [00:03<00:52, 18.05it/s, train_loss=1.531, val_loss=1.626, train_mse=1.251, val_mse=1.489]      6%|▌         | 56/1000 [00:03<00:52, 18.10it/s, train_loss=1.531, val_loss=1.626, train_mse=1.251, val_mse=1.489]      6%|▌         | 56/1000 [00:03<00:52, 18.10it/s, train_loss=1.530, val_loss=1.631, train_mse=1.249, val_mse=1.505]      6%|▌         | 57/1000 [00:03<00:52, 18.10it/s, train_loss=1.520, val_loss=1.631, train_mse=1.221, val_mse=1.508]      6%|▌         | 58/1000 [00:03<00:55, 17.03it/s, train_loss=1.520, val_loss=1.631, train_mse=1.221, val_mse=1.508]      6%|▌         | 58/1000 [00:03<00:55, 17.03it/s, train_loss=1.533, val_loss=1.632, train_mse=1.252, val_mse=1.499]      6%|▌         | 59/1000 [00:03<00:55, 17.03it/s, train_loss=1.522, val_loss=1.642, train_mse=1.228, val_mse=1.529]      6%|▌         | 60/1000 [00:03<01:08, 13.82it/s, train_loss=1.522, val_loss=1.642, train_mse=1.228, val_mse=1.529]      6%|▌         | 60/1000 [00:03<01:08, 13.82it/s, train_loss=1.521, val_loss=1.629, train_mse=1.226, val_mse=1.494]      6%|▌         | 61/1000 [00:03<01:07, 13.82it/s, train_loss=1.523, val_loss=1.637, train_mse=1.232, val_mse=1.511]      6%|▌         | 62/1000 [00:03<01:09, 13.59it/s, train_loss=1.523, val_loss=1.637, train_mse=1.232, val_mse=1.511]      6%|▌         | 62/1000 [00:03<01:09, 13.59it/s, train_loss=1.526, val_loss=1.634, train_mse=1.237, val_mse=1.504]      6%|▋         | 63/1000 [00:04<01:08, 13.59it/s, train_loss=1.512, val_loss=1.634, train_mse=1.203, val_mse=1.503]      6%|▋         | 64/1000 [00:04<01:06, 14.01it/s, train_loss=1.512, val_loss=1.634, train_mse=1.203, val_mse=1.503]      6%|▋         | 64/1000 [00:04<01:06, 14.01it/s, train_loss=1.513, val_loss=1.629, train_mse=1.206, val_mse=1.495]      6%|▋         | 65/1000 [00:04<01:06, 14.01it/s, train_loss=1.516, val_loss=1.637, train_mse=1.215, val_mse=1.513]      7%|▋         | 66/1000 [00:04<01:02, 15.01it/s, train_loss=1.516, val_loss=1.637, train_mse=1.215, val_mse=1.513]      7%|▋         | 66/1000 [00:04<01:02, 15.01it/s, train_loss=1.533, val_loss=1.631, train_mse=1.255, val_mse=1.493]      7%|▋         | 67/1000 [00:04<01:02, 15.01it/s, train_loss=1.518, val_loss=1.637, train_mse=1.220, val_mse=1.509]      7%|▋         | 68/1000 [00:04<00:58, 16.04it/s, train_loss=1.518, val_loss=1.637, train_mse=1.220, val_mse=1.509]      7%|▋         | 68/1000 [00:04<00:58, 16.04it/s, train_loss=1.517, val_loss=1.635, train_mse=1.214, val_mse=1.510]      7%|▋         | 69/1000 [00:04<00:58, 16.04it/s, train_loss=1.517, val_loss=1.629, train_mse=1.216, val_mse=1.489]      7%|▋         | 70/1000 [00:04<00:54, 17.04it/s, train_loss=1.517, val_loss=1.629, train_mse=1.216, val_mse=1.489]      7%|▋         | 70/1000 [00:04<00:54, 17.04it/s, train_loss=1.533, val_loss=1.645, train_mse=1.256, val_mse=1.527]      7%|▋         | 71/1000 [00:04<00:54, 17.04it/s, train_loss=1.770, val_loss=1.801, train_mse=1.855, val_mse=1.929]      7%|▋         | 72/1000 [00:04<00:52, 17.65it/s, train_loss=1.770, val_loss=1.801, train_mse=1.855, val_mse=1.929]      7%|▋         | 72/1000 [00:04<00:52, 17.65it/s, train_loss=1.620, val_loss=1.724, train_mse=1.496, val_mse=1.801]      7%|▋         | 73/1000 [00:04<00:52, 17.65it/s, train_loss=1.563, val_loss=1.650, train_mse=1.299, val_mse=1.566]      7%|▋         | 74/1000 [00:04<01:00, 15.34it/s, train_loss=1.563, val_loss=1.650, train_mse=1.299, val_mse=1.566]      7%|▋         | 74/1000 [00:04<01:00, 15.34it/s, train_loss=1.539, val_loss=1.646, train_mse=1.240, val_mse=1.556]      8%|▊         | 75/1000 [00:04<01:00, 15.34it/s, train_loss=1.557, val_loss=1.670, train_mse=1.311, val_mse=1.614]      8%|▊         | 76/1000 [00:04<00:59, 15.49it/s, train_loss=1.557, val_loss=1.670, train_mse=1.311, val_mse=1.614]      8%|▊         | 76/1000 [00:04<00:59, 15.49it/s, train_loss=1.534, val_loss=1.658, train_mse=1.258, val_mse=1.579]      8%|▊         | 77/1000 [00:04<00:59, 15.49it/s, train_loss=1.555, val_loss=1.640, train_mse=1.312, val_mse=1.528]      8%|▊         | 78/1000 [00:04<01:04, 14.24it/s, train_loss=1.555, val_loss=1.640, train_mse=1.312, val_mse=1.528]      8%|▊         | 78/1000 [00:04<01:04, 14.24it/s, train_loss=1.526, val_loss=1.640, train_mse=1.240, val_mse=1.530]      8%|▊         | 79/1000 [00:05<01:04, 14.24it/s, train_loss=1.533, val_loss=1.638, train_mse=1.254, val_mse=1.523]      8%|▊         | 80/1000 [00:05<00:59, 15.53it/s, train_loss=1.533, val_loss=1.638, train_mse=1.254, val_mse=1.523]      8%|▊         | 80/1000 [00:05<00:59, 15.53it/s, train_loss=1.528, val_loss=1.639, train_mse=1.244, val_mse=1.525]      8%|▊         | 81/1000 [00:05<00:59, 15.53it/s, train_loss=1.532, val_loss=1.639, train_mse=1.253, val_mse=1.526]      8%|▊         | 82/1000 [00:05<00:55, 16.46it/s, train_loss=1.532, val_loss=1.639, train_mse=1.253, val_mse=1.526]      8%|▊         | 82/1000 [00:05<00:55, 16.46it/s, train_loss=1.526, val_loss=1.638, train_mse=1.238, val_mse=1.523]      8%|▊         | 83/1000 [00:05<00:55, 16.46it/s, train_loss=1.517, val_loss=1.637, train_mse=1.216, val_mse=1.521]      8%|▊         | 84/1000 [00:05<00:53, 17.16it/s, train_loss=1.517, val_loss=1.637, train_mse=1.216, val_mse=1.521]      8%|▊         | 84/1000 [00:05<00:53, 17.16it/s, train_loss=1.523, val_loss=1.638, train_mse=1.229, val_mse=1.522]      8%|▊         | 85/1000 [00:05<00:53, 17.16it/s, train_loss=1.548, val_loss=1.638, train_mse=1.296, val_mse=1.522]      9%|▊         | 86/1000 [00:05<00:52, 17.49it/s, train_loss=1.548, val_loss=1.638, train_mse=1.296, val_mse=1.522]      9%|▊         | 86/1000 [00:05<00:52, 17.49it/s, train_loss=1.540, val_loss=1.637, train_mse=1.276, val_mse=1.520]      9%|▊         | 87/1000 [00:05<00:52, 17.49it/s, train_loss=1.536, val_loss=1.640, train_mse=1.262, val_mse=1.528]      9%|▉         | 88/1000 [00:05<00:52, 17.33it/s, train_loss=1.536, val_loss=1.640, train_mse=1.262, val_mse=1.528]      9%|▉         | 88/1000 [00:05<00:52, 17.33it/s, train_loss=1.525, val_loss=1.642, train_mse=1.237, val_mse=1.534]      9%|▉         | 89/1000 [00:05<00:52, 17.33it/s, train_loss=1.538, val_loss=1.639, train_mse=1.268, val_mse=1.524]      9%|▉         | 90/1000 [00:05<00:52, 17.31it/s, train_loss=1.538, val_loss=1.639, train_mse=1.268, val_mse=1.524]      9%|▉         | 90/1000 [00:05<00:52, 17.31it/s, train_loss=1.533, val_loss=1.637, train_mse=1.254, val_mse=1.521]      9%|▉         | 91/1000 [00:05<00:52, 17.31it/s, train_loss=1.518, val_loss=1.637, train_mse=1.217, val_mse=1.522]      9%|▉         | 92/1000 [00:05<00:50, 17.89it/s, train_loss=1.518, val_loss=1.637, train_mse=1.217, val_mse=1.522]      9%|▉         | 92/1000 [00:05<00:50, 17.89it/s, train_loss=1.526, val_loss=1.636, train_mse=1.237, val_mse=1.518]      9%|▉         | 93/1000 [00:05<00:50, 17.89it/s, train_loss=1.521, val_loss=1.636, train_mse=1.226, val_mse=1.518]      9%|▉         | 94/1000 [00:05<00:50, 17.81it/s, train_loss=1.521, val_loss=1.636, train_mse=1.226, val_mse=1.518]      9%|▉         | 94/1000 [00:05<00:50, 17.81it/s, train_loss=1.514, val_loss=1.637, train_mse=1.207, val_mse=1.519]     10%|▉         | 95/1000 [00:05<00:50, 17.81it/s, train_loss=1.521, val_loss=1.636, train_mse=1.226, val_mse=1.516]     10%|▉         | 96/1000 [00:05<00:50, 17.83it/s, train_loss=1.521, val_loss=1.636, train_mse=1.226, val_mse=1.516]     10%|▉         | 96/1000 [00:05<00:50, 17.83it/s, train_loss=1.518, val_loss=1.636, train_mse=1.218, val_mse=1.516]     10%|▉         | 97/1000 [00:06<00:50, 17.83it/s, train_loss=1.521, val_loss=1.637, train_mse=1.227, val_mse=1.518]     10%|▉         | 98/1000 [00:06<00:49, 18.04it/s, train_loss=1.521, val_loss=1.637, train_mse=1.227, val_mse=1.518]     10%|▉         | 98/1000 [00:06<00:49, 18.04it/s, train_loss=1.550, val_loss=1.637, train_mse=1.299, val_mse=1.517]     10%|▉         | 99/1000 [00:06<00:49, 18.04it/s, train_loss=1.533, val_loss=1.637, train_mse=1.256, val_mse=1.518]     10%|█         | 100/1000 [00:06<00:49, 18.36it/s, train_loss=1.533, val_loss=1.637, train_mse=1.256, val_mse=1.518]     10%|█         | 100/1000 [00:06<00:49, 18.36it/s, train_loss=1.525, val_loss=1.637, train_mse=1.236, val_mse=1.518]     10%|█         | 101/1000 [00:06<00:48, 18.36it/s, train_loss=1.537, val_loss=1.637, train_mse=1.267, val_mse=1.518]     10%|█         | 102/1000 [00:06<00:48, 18.52it/s, train_loss=1.537, val_loss=1.637, train_mse=1.267, val_mse=1.518]     10%|█         | 102/1000 [00:06<00:48, 18.52it/s, train_loss=1.510, val_loss=1.637, train_mse=1.197, val_mse=1.518]     10%|█         | 103/1000 [00:06<00:48, 18.52it/s, train_loss=1.522, val_loss=1.637, train_mse=1.228, val_mse=1.517]     10%|█         | 104/1000 [00:06<00:51, 17.52it/s, train_loss=1.522, val_loss=1.637, train_mse=1.228, val_mse=1.517]     10%|█         | 104/1000 [00:06<00:51, 17.52it/s, train_loss=1.525, val_loss=1.637, train_mse=1.236, val_mse=1.517]     10%|█         | 105/1000 [00:06<00:51, 17.52it/s, train_loss=1.507, val_loss=1.637, train_mse=1.192, val_mse=1.517]     11%|█         | 106/1000 [00:06<00:49, 17.96it/s, train_loss=1.507, val_loss=1.637, train_mse=1.192, val_mse=1.517]     11%|█         | 106/1000 [00:06<00:49, 17.96it/s, train_loss=1.541, val_loss=1.637, train_mse=1.276, val_mse=1.517]     11%|█         | 107/1000 [00:06<00:49, 17.96it/s, train_loss=1.530, val_loss=1.637, train_mse=1.249, val_mse=1.517]     11%|█         | 108/1000 [00:06<00:48, 18.31it/s, train_loss=1.530, val_loss=1.637, train_mse=1.249, val_mse=1.517]     11%|█         | 108/1000 [00:06<00:48, 18.31it/s, train_loss=1.520, val_loss=1.637, train_mse=1.223, val_mse=1.517]     11%|█         | 109/1000 [00:06<00:48, 18.31it/s, train_loss=1.515, val_loss=1.636, train_mse=1.210, val_mse=1.517]     11%|█         | 110/1000 [00:06<00:48, 18.33it/s, train_loss=1.515, val_loss=1.636, train_mse=1.210, val_mse=1.517]     11%|█         | 110/1000 [00:06<00:48, 18.33it/s, train_loss=1.521, val_loss=1.636, train_mse=1.225, val_mse=1.517]     11%|█         | 111/1000 [00:06<00:48, 18.33it/s, train_loss=1.518, val_loss=1.636, train_mse=1.218, val_mse=1.517]     11%|█         | 112/1000 [00:06<00:48, 18.13it/s, train_loss=1.518, val_loss=1.636, train_mse=1.218, val_mse=1.517]     11%|█         | 112/1000 [00:06<00:48, 18.13it/s, train_loss=1.525, val_loss=1.637, train_mse=1.237, val_mse=1.517]     11%|█▏        | 113/1000 [00:06<00:48, 18.13it/s, train_loss=1.523, val_loss=1.637, train_mse=1.232, val_mse=1.517]     11%|█▏        | 114/1000 [00:06<00:47, 18.61it/s, train_loss=1.523, val_loss=1.637, train_mse=1.232, val_mse=1.517]     11%|█▏        | 114/1000 [00:06<00:47, 18.61it/s, train_loss=1.555, val_loss=1.636, train_mse=1.312, val_mse=1.517]     12%|█▏        | 115/1000 [00:06<00:47, 18.61it/s, train_loss=1.536, val_loss=1.636, train_mse=1.265, val_mse=1.517]     12%|█▏        | 116/1000 [00:07<00:47, 18.80it/s, train_loss=1.536, val_loss=1.636, train_mse=1.265, val_mse=1.517]     12%|█▏        | 116/1000 [00:07<00:47, 18.80it/s, train_loss=1.523, val_loss=1.636, train_mse=1.231, val_mse=1.517]     12%|█▏        | 117/1000 [00:07<00:46, 18.80it/s, train_loss=1.523, val_loss=1.636, train_mse=1.231, val_mse=1.517]     12%|█▏        | 118/1000 [00:07<00:46, 18.89it/s, train_loss=1.523, val_loss=1.636, train_mse=1.231, val_mse=1.517]     12%|█▏        | 118/1000 [00:07<00:46, 18.89it/s, train_loss=1.519, val_loss=1.636, train_mse=1.222, val_mse=1.517]     12%|█▏        | 119/1000 [00:07<00:46, 18.89it/s, train_loss=1.510, val_loss=1.636, train_mse=1.199, val_mse=1.517]     12%|█▏        | 120/1000 [00:07<00:47, 18.67it/s, train_loss=1.510, val_loss=1.636, train_mse=1.199, val_mse=1.517]     12%|█▏        | 120/1000 [00:07<00:47, 18.67it/s, train_loss=1.521, val_loss=1.636, train_mse=1.224, val_mse=1.517]     12%|█▏        | 121/1000 [00:07<00:47, 18.67it/s, train_loss=1.531, val_loss=1.636, train_mse=1.251, val_mse=1.517]     12%|█▏        | 122/1000 [00:07<00:47, 18.38it/s, train_loss=1.531, val_loss=1.636, train_mse=1.251, val_mse=1.517]     12%|█▏        | 122/1000 [00:07<00:47, 18.38it/s, train_loss=1.521, val_loss=1.636, train_mse=1.226, val_mse=1.517]     12%|█▏        | 123/1000 [00:07<00:47, 18.38it/s, train_loss=1.538, val_loss=1.636, train_mse=1.269, val_mse=1.517]     12%|█▏        | 124/1000 [00:07<00:47, 18.40it/s, train_loss=1.538, val_loss=1.636, train_mse=1.269, val_mse=1.517]     12%|█▏        | 124/1000 [00:07<00:47, 18.40it/s, train_loss=1.519, val_loss=1.636, train_mse=1.222, val_mse=1.517]     12%|█▎        | 125/1000 [00:07<00:47, 18.40it/s, train_loss=1.524, val_loss=1.636, train_mse=1.233, val_mse=1.517]     13%|█▎        | 126/1000 [00:07<00:47, 18.56it/s, train_loss=1.524, val_loss=1.636, train_mse=1.233, val_mse=1.517]     13%|█▎        | 126/1000 [00:07<00:47, 18.56it/s, train_loss=1.518, val_loss=1.636, train_mse=1.219, val_mse=1.517]     13%|█▎        | 127/1000 [00:07<00:47, 18.56it/s, train_loss=1.520, val_loss=1.636, train_mse=1.222, val_mse=1.517]     13%|█▎        | 128/1000 [00:07<00:46, 18.56it/s, train_loss=1.544, val_loss=1.636, train_mse=1.284, val_mse=1.517]     13%|█▎        | 129/1000 [00:07<00:45, 19.02it/s, train_loss=1.544, val_loss=1.636, train_mse=1.284, val_mse=1.517]     13%|█▎        | 129/1000 [00:07<00:45, 19.02it/s, train_loss=1.530, val_loss=1.636, train_mse=1.249, val_mse=1.517]     13%|█▎        | 130/1000 [00:07<00:45, 19.02it/s, train_loss=1.527, val_loss=1.636, train_mse=1.241, val_mse=1.517]     13%|█▎        | 131/1000 [00:07<00:45, 19.04it/s, train_loss=1.527, val_loss=1.636, train_mse=1.241, val_mse=1.517]     13%|█▎        | 131/1000 [00:07<00:45, 19.04it/s, train_loss=1.522, val_loss=1.636, train_mse=1.229, val_mse=1.517]     13%|█▎        | 132/1000 [00:07<00:45, 19.04it/s, train_loss=1.512, val_loss=1.636, train_mse=1.203, val_mse=1.517]     13%|█▎        | 133/1000 [00:07<00:45, 19.13it/s, train_loss=1.512, val_loss=1.636, train_mse=1.203, val_mse=1.517]     13%|█▎        | 133/1000 [00:07<00:45, 19.13it/s, train_loss=1.519, val_loss=1.636, train_mse=1.220, val_mse=1.517]     13%|█▎        | 134/1000 [00:07<00:45, 19.13it/s, train_loss=1.516, val_loss=1.636, train_mse=1.212, val_mse=1.517]     14%|█▎        | 135/1000 [00:08<00:46, 18.71it/s, train_loss=1.516, val_loss=1.636, train_mse=1.212, val_mse=1.517]     14%|█▎        | 135/1000 [00:08<00:46, 18.71it/s, train_loss=1.532, val_loss=1.636, train_mse=1.254, val_mse=1.517]     14%|█▎        | 136/1000 [00:08<00:46, 18.71it/s, train_loss=1.520, val_loss=1.636, train_mse=1.223, val_mse=1.517]     14%|█▎        | 137/1000 [00:08<00:47, 18.19it/s, train_loss=1.520, val_loss=1.636, train_mse=1.223, val_mse=1.517]     14%|█▎        | 137/1000 [00:08<00:47, 18.19it/s, train_loss=1.538, val_loss=1.636, train_mse=1.269, val_mse=1.517]     14%|█▍        | 138/1000 [00:08<00:47, 18.19it/s, train_loss=1.536, val_loss=1.636, train_mse=1.265, val_mse=1.517]     14%|█▍        | 139/1000 [00:08<00:48, 17.67it/s, train_loss=1.536, val_loss=1.636, train_mse=1.265, val_mse=1.517]     14%|█▍        | 139/1000 [00:08<00:48, 17.67it/s, train_loss=1.519, val_loss=1.636, train_mse=1.220, val_mse=1.517]     14%|█▍        | 140/1000 [00:08<00:48, 17.67it/s, train_loss=1.525, val_loss=1.636, train_mse=1.238, val_mse=1.517]     14%|█▍        | 141/1000 [00:08<00:47, 17.94it/s, train_loss=1.525, val_loss=1.636, train_mse=1.238, val_mse=1.517]     14%|█▍        | 141/1000 [00:08<00:47, 17.94it/s, train_loss=1.521, val_loss=1.636, train_mse=1.226, val_mse=1.517]     14%|█▍        | 142/1000 [00:08<00:47, 17.94it/s, train_loss=1.509, val_loss=1.636, train_mse=1.197, val_mse=1.517]     14%|█▍        | 143/1000 [00:08<00:47, 18.20it/s, train_loss=1.509, val_loss=1.636, train_mse=1.197, val_mse=1.517]     14%|█▍        | 143/1000 [00:08<00:47, 18.20it/s, train_loss=1.518, val_loss=1.636, train_mse=1.218, val_mse=1.517]     14%|█▍        | 144/1000 [00:08<00:47, 18.20it/s, train_loss=1.533, val_loss=1.636, train_mse=1.255, val_mse=1.517]     14%|█▍        | 145/1000 [00:08<00:46, 18.34it/s, train_loss=1.533, val_loss=1.636, train_mse=1.255, val_mse=1.517]     14%|█▍        | 145/1000 [00:08<00:46, 18.34it/s, train_loss=1.523, val_loss=1.636, train_mse=1.230, val_mse=1.517]     15%|█▍        | 146/1000 [00:08<00:46, 18.34it/s, train_loss=1.522, val_loss=1.636, train_mse=1.228, val_mse=1.517]     15%|█▍        | 147/1000 [00:08<00:47, 18.13it/s, train_loss=1.522, val_loss=1.636, train_mse=1.228, val_mse=1.517]     15%|█▍        | 147/1000 [00:08<00:47, 18.13it/s, train_loss=1.527, val_loss=1.636, train_mse=1.240, val_mse=1.517]     15%|█▍        | 148/1000 [00:08<00:47, 18.13it/s, train_loss=1.521, val_loss=1.636, train_mse=1.224, val_mse=1.517]     15%|█▍        | 149/1000 [00:08<00:48, 17.64it/s, train_loss=1.521, val_loss=1.636, train_mse=1.224, val_mse=1.517]     15%|█▍        | 149/1000 [00:08<00:48, 17.64it/s, train_loss=1.526, val_loss=1.636, train_mse=1.239, val_mse=1.517]     15%|█▌        | 150/1000 [00:08<00:48, 17.64it/s, train_loss=1.514, val_loss=1.636, train_mse=1.207, val_mse=1.517]     15%|█▌        | 151/1000 [00:09<00:56, 14.98it/s, train_loss=1.514, val_loss=1.636, train_mse=1.207, val_mse=1.517]     15%|█▌        | 151/1000 [00:09<00:56, 14.98it/s, train_loss=1.539, val_loss=1.636, train_mse=1.272, val_mse=1.517]     15%|█▌        | 152/1000 [00:09<00:56, 14.98it/s, train_loss=1.518, val_loss=1.636, train_mse=1.218, val_mse=1.517]     15%|█▌        | 153/1000 [00:09<00:56, 15.08it/s, train_loss=1.518, val_loss=1.636, train_mse=1.218, val_mse=1.517]     15%|█▌        | 153/1000 [00:09<00:56, 15.08it/s, train_loss=1.524, val_loss=1.636, train_mse=1.233, val_mse=1.517]     15%|█▌        | 154/1000 [00:09<00:56, 15.08it/s, train_loss=1.528, val_loss=1.636, train_mse=1.244, val_mse=1.517]     16%|█▌        | 155/1000 [00:09<00:52, 16.11it/s, train_loss=1.528, val_loss=1.636, train_mse=1.244, val_mse=1.517]     16%|█▌        | 155/1000 [00:09<00:52, 16.11it/s, train_loss=1.518, val_loss=1.636, train_mse=1.219, val_mse=1.517]     16%|█▌        | 156/1000 [00:09<00:52, 16.11it/s, train_loss=1.521, val_loss=1.636, train_mse=1.226, val_mse=1.517]     16%|█▌        | 157/1000 [00:09<00:50, 16.74it/s, train_loss=1.521, val_loss=1.636, train_mse=1.226, val_mse=1.517]     16%|█▌        | 157/1000 [00:09<00:50, 16.74it/s, train_loss=1.512, val_loss=1.636, train_mse=1.203, val_mse=1.517]     16%|█▌        | 158/1000 [00:09<00:50, 16.74it/s, train_loss=1.520, val_loss=1.636, train_mse=1.223, val_mse=1.517]     16%|█▌        | 159/1000 [00:09<00:47, 17.55it/s, train_loss=1.520, val_loss=1.636, train_mse=1.223, val_mse=1.517]     16%|█▌        | 159/1000 [00:09<00:47, 17.55it/s, train_loss=1.529, val_loss=1.636, train_mse=1.246, val_mse=1.517]     16%|█▌        | 160/1000 [00:09<00:47, 17.55it/s, train_loss=1.515, val_loss=1.636, train_mse=1.210, val_mse=1.517]     16%|█▌        | 161/1000 [00:09<00:46, 18.17it/s, train_loss=1.515, val_loss=1.636, train_mse=1.210, val_mse=1.517]     16%|█▌        | 161/1000 [00:09<00:46, 18.17it/s, train_loss=1.543, val_loss=1.636, train_mse=1.283, val_mse=1.517]     16%|█▌        | 162/1000 [00:09<00:46, 18.17it/s, train_loss=1.531, val_loss=1.636, train_mse=1.250, val_mse=1.517]     16%|█▋        | 163/1000 [00:09<00:44, 18.68it/s, train_loss=1.531, val_loss=1.636, train_mse=1.250, val_mse=1.517]     16%|█▋        | 163/1000 [00:09<00:44, 18.68it/s, train_loss=1.522, val_loss=1.636, train_mse=1.227, val_mse=1.517]     16%|█▋        | 164/1000 [00:09<00:44, 18.68it/s, train_loss=1.531, val_loss=1.636, train_mse=1.252, val_mse=1.517]     16%|█▋        | 165/1000 [00:09<00:43, 19.02it/s, train_loss=1.531, val_loss=1.636, train_mse=1.252, val_mse=1.517]     16%|█▋        | 165/1000 [00:09<00:43, 19.02it/s, train_loss=1.517, val_loss=1.636, train_mse=1.215, val_mse=1.517]     17%|█▋        | 166/1000 [00:09<00:43, 19.02it/s, train_loss=1.531, val_loss=1.636, train_mse=1.250, val_mse=1.517]     17%|█▋        | 167/1000 [00:09<00:43, 19.26it/s, train_loss=1.531, val_loss=1.636, train_mse=1.250, val_mse=1.517]     17%|█▋        | 167/1000 [00:09<00:43, 19.26it/s, train_loss=1.517, val_loss=1.636, train_mse=1.215, val_mse=1.517]     17%|█▋        | 168/1000 [00:09<00:43, 19.26it/s, train_loss=1.521, val_loss=1.636, train_mse=1.227, val_mse=1.517]     17%|█▋        | 169/1000 [00:09<00:43, 19.32it/s, train_loss=1.521, val_loss=1.636, train_mse=1.227, val_mse=1.517]     17%|█▋        | 169/1000 [00:09<00:43, 19.32it/s, train_loss=1.515, val_loss=1.636, train_mse=1.211, val_mse=1.517]     17%|█▋        | 170/1000 [00:10<00:42, 19.32it/s, train_loss=1.533, val_loss=1.636, train_mse=1.257, val_mse=1.517]     17%|█▋        | 171/1000 [00:10<00:42, 19.42it/s, train_loss=1.533, val_loss=1.636, train_mse=1.257, val_mse=1.517]     17%|█▋        | 171/1000 [00:10<00:42, 19.42it/s, train_loss=1.522, val_loss=1.636, train_mse=1.228, val_mse=1.517]     17%|█▋        | 172/1000 [00:10<00:42, 19.42it/s, train_loss=1.530, val_loss=1.636, train_mse=1.248, val_mse=1.517]     17%|█▋        | 173/1000 [00:10<00:42, 19.36it/s, train_loss=1.530, val_loss=1.636, train_mse=1.248, val_mse=1.517]     17%|█▋        | 173/1000 [00:10<00:42, 19.36it/s, train_loss=1.528, val_loss=1.636, train_mse=1.244, val_mse=1.517]     17%|█▋        | 174/1000 [00:10<00:42, 19.36it/s, train_loss=1.515, val_loss=1.636, train_mse=1.211, val_mse=1.517]     18%|█▊        | 175/1000 [00:10<00:42, 19.33it/s, train_loss=1.515, val_loss=1.636, train_mse=1.211, val_mse=1.517]     18%|█▊        | 175/1000 [00:10<00:42, 19.33it/s, train_loss=1.520, val_loss=1.636, train_mse=1.223, val_mse=1.517]     18%|█▊        | 176/1000 [00:10<00:42, 19.33it/s, train_loss=1.511, val_loss=1.636, train_mse=1.201, val_mse=1.517]     18%|█▊        | 177/1000 [00:10<00:42, 19.17it/s, train_loss=1.511, val_loss=1.636, train_mse=1.201, val_mse=1.517]     18%|█▊        | 177/1000 [00:10<00:42, 19.17it/s, train_loss=1.528, val_loss=1.636, train_mse=1.244, val_mse=1.517]     18%|█▊        | 178/1000 [00:10<00:42, 19.17it/s, train_loss=1.511, val_loss=1.636, train_mse=1.202, val_mse=1.517]     18%|█▊        | 179/1000 [00:10<00:43, 18.80it/s, train_loss=1.511, val_loss=1.636, train_mse=1.202, val_mse=1.517]     18%|█▊        | 179/1000 [00:10<00:43, 18.80it/s, train_loss=1.516, val_loss=1.636, train_mse=1.214, val_mse=1.517]     18%|█▊        | 180/1000 [00:10<00:43, 18.80it/s, train_loss=1.519, val_loss=1.636, train_mse=1.220, val_mse=1.517]     18%|█▊        | 181/1000 [00:10<00:43, 18.96it/s, train_loss=1.519, val_loss=1.636, train_mse=1.220, val_mse=1.517]     18%|█▊        | 181/1000 [00:10<00:43, 18.96it/s, train_loss=1.516, val_loss=1.636, train_mse=1.213, val_mse=1.517]     18%|█▊        | 182/1000 [00:10<00:43, 18.96it/s, train_loss=1.528, val_loss=1.636, train_mse=1.244, val_mse=1.517]     18%|█▊        | 183/1000 [00:10<00:42, 19.06it/s, train_loss=1.528, val_loss=1.636, train_mse=1.244, val_mse=1.517]     18%|█▊        | 183/1000 [00:10<00:42, 19.06it/s, train_loss=1.536, val_loss=1.636, train_mse=1.265, val_mse=1.517]     18%|█▊        | 184/1000 [00:10<00:42, 19.06it/s, train_loss=1.513, val_loss=1.636, train_mse=1.205, val_mse=1.517]     18%|█▊        | 185/1000 [00:10<00:43, 18.64it/s, train_loss=1.513, val_loss=1.636, train_mse=1.205, val_mse=1.517]     18%|█▊        | 185/1000 [00:10<00:43, 18.64it/s, train_loss=1.535, val_loss=1.636, train_mse=1.260, val_mse=1.517]     19%|█▊        | 186/1000 [00:10<00:43, 18.64it/s, train_loss=1.521, val_loss=1.636, train_mse=1.226, val_mse=1.517]     19%|█▊        | 187/1000 [00:10<00:43, 18.84it/s, train_loss=1.521, val_loss=1.636, train_mse=1.226, val_mse=1.517]     19%|█▊        | 187/1000 [00:10<00:43, 18.84it/s, train_loss=1.522, val_loss=1.636, train_mse=1.229, val_mse=1.517]     19%|█▉        | 188/1000 [00:10<00:43, 18.84it/s, train_loss=1.516, val_loss=1.636, train_mse=1.212, val_mse=1.517]     19%|█▉        | 189/1000 [00:11<00:43, 18.84it/s, train_loss=1.514, val_loss=1.636, train_mse=1.208, val_mse=1.517]     19%|█▉        | 190/1000 [00:11<00:42, 19.23it/s, train_loss=1.514, val_loss=1.636, train_mse=1.208, val_mse=1.517]     19%|█▉        | 190/1000 [00:11<00:42, 19.23it/s, train_loss=1.515, val_loss=1.636, train_mse=1.211, val_mse=1.517]     19%|█▉        | 191/1000 [00:11<00:42, 19.23it/s, train_loss=1.528, val_loss=1.636, train_mse=1.244, val_mse=1.517]     19%|█▉        | 192/1000 [00:11<00:45, 17.86it/s, train_loss=1.528, val_loss=1.636, train_mse=1.244, val_mse=1.517]     19%|█▉        | 192/1000 [00:11<00:45, 17.86it/s, train_loss=1.507, val_loss=1.636, train_mse=1.190, val_mse=1.517]     19%|█▉        | 193/1000 [00:11<00:45, 17.86it/s, train_loss=1.541, val_loss=1.636, train_mse=1.277, val_mse=1.517]     19%|█▉        | 194/1000 [00:11<00:45, 17.63it/s, train_loss=1.541, val_loss=1.636, train_mse=1.277, val_mse=1.517]     19%|█▉        | 194/1000 [00:11<00:45, 17.63it/s, train_loss=1.540, val_loss=1.636, train_mse=1.275, val_mse=1.517]     20%|█▉        | 195/1000 [00:11<00:45, 17.63it/s, train_loss=1.514, val_loss=1.636, train_mse=1.208, val_mse=1.517]     20%|█▉        | 196/1000 [00:11<00:44, 17.95it/s, train_loss=1.514, val_loss=1.636, train_mse=1.208, val_mse=1.517]     20%|█▉        | 196/1000 [00:11<00:44, 17.95it/s, train_loss=1.519, val_loss=1.636, train_mse=1.220, val_mse=1.517]     20%|█▉        | 197/1000 [00:11<00:44, 17.95it/s, train_loss=1.521, val_loss=1.636, train_mse=1.225, val_mse=1.517]     20%|█▉        | 198/1000 [00:11<00:43, 18.40it/s, train_loss=1.521, val_loss=1.636, train_mse=1.225, val_mse=1.517]     20%|█▉        | 198/1000 [00:11<00:43, 18.40it/s, train_loss=1.515, val_loss=1.636, train_mse=1.211, val_mse=1.517]     20%|█▉        | 199/1000 [00:11<00:43, 18.40it/s, train_loss=1.532, val_loss=1.636, train_mse=1.252, val_mse=1.517]     20%|██        | 200/1000 [00:11<00:44, 17.91it/s, train_loss=1.532, val_loss=1.636, train_mse=1.252, val_mse=1.517]     20%|██        | 200/1000 [00:11<00:44, 17.91it/s, train_loss=1.524, val_loss=1.636, train_mse=1.233, val_mse=1.517]     20%|██        | 201/1000 [00:11<00:44, 17.91it/s, train_loss=1.514, val_loss=1.636, train_mse=1.208, val_mse=1.517]     20%|██        | 202/1000 [00:11<00:45, 17.55it/s, train_loss=1.514, val_loss=1.636, train_mse=1.208, val_mse=1.517]     20%|██        | 202/1000 [00:11<00:45, 17.55it/s, train_loss=1.517, val_loss=1.636, train_mse=1.217, val_mse=1.517]     20%|██        | 203/1000 [00:11<00:45, 17.55it/s, train_loss=1.530, val_loss=1.636, train_mse=1.249, val_mse=1.517]     20%|██        | 204/1000 [00:11<00:48, 16.42it/s, train_loss=1.530, val_loss=1.636, train_mse=1.249, val_mse=1.517]     20%|██        | 204/1000 [00:11<00:48, 16.42it/s, train_loss=1.522, val_loss=1.636, train_mse=1.227, val_mse=1.517]     20%|██        | 205/1000 [00:11<00:48, 16.42it/s, train_loss=1.517, val_loss=1.636, train_mse=1.216, val_mse=1.517]     21%|██        | 206/1000 [00:12<00:48, 16.33it/s, train_loss=1.517, val_loss=1.636, train_mse=1.216, val_mse=1.517]     21%|██        | 206/1000 [00:12<00:48, 16.33it/s, train_loss=1.532, val_loss=1.636, train_mse=1.255, val_mse=1.517]     21%|██        | 207/1000 [00:12<00:48, 16.33it/s, train_loss=1.526, val_loss=1.636, train_mse=1.237, val_mse=1.517]     21%|██        | 208/1000 [00:12<00:48, 16.18it/s, train_loss=1.526, val_loss=1.636, train_mse=1.237, val_mse=1.517]     21%|██        | 208/1000 [00:12<00:48, 16.18it/s, train_loss=1.518, val_loss=1.636, train_mse=1.219, val_mse=1.517]     21%|██        | 209/1000 [00:12<00:48, 16.18it/s, train_loss=1.522, val_loss=1.636, train_mse=1.228, val_mse=1.517]     21%|██        | 210/1000 [00:12<00:47, 16.66it/s, train_loss=1.522, val_loss=1.636, train_mse=1.228, val_mse=1.517]     21%|██        | 210/1000 [00:12<00:47, 16.66it/s, train_loss=1.523, val_loss=1.636, train_mse=1.231, val_mse=1.517]     21%|██        | 211/1000 [00:12<00:47, 16.66it/s, train_loss=1.522, val_loss=1.636, train_mse=1.228, val_mse=1.517]     21%|██        | 212/1000 [00:12<00:46, 16.81it/s, train_loss=1.522, val_loss=1.636, train_mse=1.228, val_mse=1.517]     21%|██        | 212/1000 [00:12<00:46, 16.81it/s, train_loss=1.515, val_loss=1.636, train_mse=1.210, val_mse=1.517]     21%|██▏       | 213/1000 [00:12<00:46, 16.81it/s, train_loss=1.520, val_loss=1.636, train_mse=1.223, val_mse=1.517]     21%|██▏       | 214/1000 [00:12<00:45, 17.15it/s, train_loss=1.520, val_loss=1.636, train_mse=1.223, val_mse=1.517]     21%|██▏       | 214/1000 [00:12<00:45, 17.15it/s, train_loss=1.515, val_loss=1.636, train_mse=1.212, val_mse=1.517]     22%|██▏       | 215/1000 [00:12<00:45, 17.15it/s, train_loss=1.513, val_loss=1.636, train_mse=1.205, val_mse=1.517]     22%|██▏       | 216/1000 [00:12<00:45, 17.22it/s, train_loss=1.513, val_loss=1.636, train_mse=1.205, val_mse=1.517]     22%|██▏       | 216/1000 [00:12<00:45, 17.22it/s, train_loss=1.515, val_loss=1.636, train_mse=1.210, val_mse=1.517]     22%|██▏       | 217/1000 [00:12<00:45, 17.22it/s, train_loss=1.512, val_loss=1.636, train_mse=1.202, val_mse=1.517]     22%|██▏       | 218/1000 [00:12<00:44, 17.57it/s, train_loss=1.512, val_loss=1.636, train_mse=1.202, val_mse=1.517]     22%|██▏       | 218/1000 [00:12<00:44, 17.57it/s, train_loss=1.515, val_loss=1.636, train_mse=1.211, val_mse=1.517]     22%|██▏       | 219/1000 [00:12<00:44, 17.57it/s, train_loss=1.527, val_loss=1.636, train_mse=1.241, val_mse=1.517]     22%|██▏       | 220/1000 [00:12<00:43, 17.77it/s, train_loss=1.527, val_loss=1.636, train_mse=1.241, val_mse=1.517]     22%|██▏       | 220/1000 [00:12<00:43, 17.77it/s, train_loss=1.512, val_loss=1.636, train_mse=1.203, val_mse=1.517]     22%|██▏       | 221/1000 [00:12<00:43, 17.77it/s, train_loss=1.534, val_loss=1.636, train_mse=1.258, val_mse=1.517]     22%|██▏       | 222/1000 [00:12<00:43, 17.73it/s, train_loss=1.534, val_loss=1.636, train_mse=1.258, val_mse=1.517]     22%|██▏       | 222/1000 [00:12<00:43, 17.73it/s, train_loss=1.530, val_loss=1.636, train_mse=1.247, val_mse=1.517]     22%|██▏       | 223/1000 [00:12<00:43, 17.73it/s, train_loss=1.518, val_loss=1.636, train_mse=1.217, val_mse=1.517]     22%|██▏       | 224/1000 [00:13<00:43, 17.92it/s, train_loss=1.518, val_loss=1.636, train_mse=1.217, val_mse=1.517]     22%|██▏       | 224/1000 [00:13<00:43, 17.92it/s, train_loss=1.520, val_loss=1.636, train_mse=1.224, val_mse=1.517]     22%|██▎       | 225/1000 [00:13<00:43, 17.92it/s, train_loss=1.519, val_loss=1.636, train_mse=1.221, val_mse=1.517]     23%|██▎       | 226/1000 [00:13<00:42, 18.15it/s, train_loss=1.519, val_loss=1.636, train_mse=1.221, val_mse=1.517]     23%|██▎       | 226/1000 [00:13<00:42, 18.15it/s, train_loss=1.515, val_loss=1.636, train_mse=1.211, val_mse=1.517]     23%|██▎       | 227/1000 [00:13<00:42, 18.15it/s, train_loss=1.524, val_loss=1.636, train_mse=1.234, val_mse=1.517]     23%|██▎       | 228/1000 [00:13<00:42, 18.13it/s, train_loss=1.524, val_loss=1.636, train_mse=1.234, val_mse=1.517]     23%|██▎       | 228/1000 [00:13<00:42, 18.13it/s, train_loss=1.528, val_loss=1.636, train_mse=1.244, val_mse=1.517]     23%|██▎       | 229/1000 [00:13<00:42, 18.13it/s, train_loss=1.529, val_loss=1.636, train_mse=1.245, val_mse=1.517]     23%|██▎       | 230/1000 [00:13<00:41, 18.41it/s, train_loss=1.529, val_loss=1.636, train_mse=1.245, val_mse=1.517]     23%|██▎       | 230/1000 [00:13<00:41, 18.41it/s, train_loss=1.536, val_loss=1.636, train_mse=1.261, val_mse=1.517]     23%|██▎       | 231/1000 [00:13<00:41, 18.41it/s, train_loss=1.534, val_loss=1.636, train_mse=1.259, val_mse=1.517]     23%|██▎       | 232/1000 [00:13<00:41, 18.44it/s, train_loss=1.534, val_loss=1.636, train_mse=1.259, val_mse=1.517]     23%|██▎       | 232/1000 [00:13<00:41, 18.44it/s, train_loss=1.524, val_loss=1.636, train_mse=1.232, val_mse=1.517]     23%|██▎       | 233/1000 [00:13<00:41, 18.44it/s, train_loss=1.539, val_loss=1.636, train_mse=1.272, val_mse=1.517]     23%|██▎       | 234/1000 [00:13<00:42, 17.83it/s, train_loss=1.539, val_loss=1.636, train_mse=1.272, val_mse=1.517]     23%|██▎       | 234/1000 [00:13<00:42, 17.83it/s, train_loss=1.521, val_loss=1.636, train_mse=1.226, val_mse=1.517]     24%|██▎       | 235/1000 [00:13<00:42, 17.83it/s, train_loss=1.518, val_loss=1.636, train_mse=1.219, val_mse=1.517]     24%|██▎       | 236/1000 [00:13<00:42, 17.98it/s, train_loss=1.518, val_loss=1.636, train_mse=1.219, val_mse=1.517]     24%|██▎       | 236/1000 [00:13<00:42, 17.98it/s, train_loss=1.519, val_loss=1.636, train_mse=1.221, val_mse=1.517]     24%|██▎       | 237/1000 [00:13<00:42, 17.98it/s, train_loss=1.523, val_loss=1.636, train_mse=1.230, val_mse=1.517]     24%|██▍       | 238/1000 [00:13<00:41, 18.42it/s, train_loss=1.523, val_loss=1.636, train_mse=1.230, val_mse=1.517]     24%|██▍       | 238/1000 [00:13<00:41, 18.42it/s, train_loss=1.523, val_loss=1.636, train_mse=1.231, val_mse=1.517]     24%|██▍       | 239/1000 [00:13<00:41, 18.42it/s, train_loss=1.516, val_loss=1.636, train_mse=1.212, val_mse=1.517]     24%|██▍       | 240/1000 [00:13<00:41, 18.51it/s, train_loss=1.516, val_loss=1.636, train_mse=1.212, val_mse=1.517]     24%|██▍       | 240/1000 [00:13<00:41, 18.51it/s, train_loss=1.516, val_loss=1.636, train_mse=1.213, val_mse=1.517]     24%|██▍       | 241/1000 [00:13<00:41, 18.51it/s, train_loss=1.520, val_loss=1.636, train_mse=1.223, val_mse=1.517]     24%|██▍       | 242/1000 [00:14<00:41, 18.27it/s, train_loss=1.520, val_loss=1.636, train_mse=1.223, val_mse=1.517]     24%|██▍       | 242/1000 [00:14<00:41, 18.27it/s, train_loss=1.529, val_loss=1.636, train_mse=1.246, val_mse=1.517]     24%|██▍       | 243/1000 [00:14<00:41, 18.27it/s, train_loss=1.532, val_loss=1.636, train_mse=1.253, val_mse=1.517]     24%|██▍       | 244/1000 [00:14<00:40, 18.51it/s, train_loss=1.532, val_loss=1.636, train_mse=1.253, val_mse=1.517]     24%|██▍       | 244/1000 [00:14<00:40, 18.51it/s, train_loss=1.523, val_loss=1.636, train_mse=1.232, val_mse=1.517]     24%|██▍       | 245/1000 [00:14<00:40, 18.51it/s, train_loss=1.518, val_loss=1.636, train_mse=1.218, val_mse=1.517]     25%|██▍       | 246/1000 [00:14<00:40, 18.58it/s, train_loss=1.518, val_loss=1.636, train_mse=1.218, val_mse=1.517]     25%|██▍       | 246/1000 [00:14<00:40, 18.58it/s, train_loss=1.519, val_loss=1.636, train_mse=1.222, val_mse=1.517]     25%|██▍       | 247/1000 [00:14<00:40, 18.58it/s, train_loss=1.519, val_loss=1.636, train_mse=1.221, val_mse=1.517]     25%|██▍       | 248/1000 [00:14<00:40, 18.59it/s, train_loss=1.519, val_loss=1.636, train_mse=1.221, val_mse=1.517]     25%|██▍       | 248/1000 [00:14<00:40, 18.59it/s, train_loss=1.529, val_loss=1.636, train_mse=1.247, val_mse=1.517]     25%|██▍       | 249/1000 [00:14<00:40, 18.59it/s, train_loss=1.534, val_loss=1.636, train_mse=1.259, val_mse=1.517]     25%|██▌       | 250/1000 [00:14<00:40, 18.62it/s, train_loss=1.534, val_loss=1.636, train_mse=1.259, val_mse=1.517]     25%|██▌       | 250/1000 [00:14<00:40, 18.62it/s, train_loss=1.526, val_loss=1.636, train_mse=1.238, val_mse=1.517]     25%|██▌       | 251/1000 [00:14<00:40, 18.62it/s, train_loss=1.514, val_loss=1.636, train_mse=1.207, val_mse=1.517]     25%|██▌       | 252/1000 [00:14<00:40, 18.66it/s, train_loss=1.514, val_loss=1.636, train_mse=1.207, val_mse=1.517]     25%|██▌       | 252/1000 [00:14<00:40, 18.66it/s, train_loss=1.518, val_loss=1.636, train_mse=1.217, val_mse=1.517]     25%|██▌       | 253/1000 [00:14<00:40, 18.66it/s, train_loss=1.533, val_loss=1.636, train_mse=1.258, val_mse=1.517]     25%|██▌       | 254/1000 [00:14<00:40, 18.58it/s, train_loss=1.533, val_loss=1.636, train_mse=1.258, val_mse=1.517]     25%|██▌       | 254/1000 [00:14<00:40, 18.58it/s, train_loss=1.514, val_loss=1.636, train_mse=1.207, val_mse=1.517]     26%|██▌       | 255/1000 [00:14<00:40, 18.58it/s, train_loss=1.510, val_loss=1.636, train_mse=1.198, val_mse=1.517]     26%|██▌       | 255/1000 [00:14<00:43, 17.32it/s, train_loss=1.510, val_loss=1.636, train_mse=1.198, val_mse=1.517]
=======
      0%|          | 0/1000 [00:00<?, ?it/s]      0%|          | 1/1000 [00:00<01:30, 11.09it/s, train_loss=2.598, val_loss=2.692, train_mse=5.996, val_mse=6.562]      0%|          | 2/1000 [00:00<01:17, 12.80it/s, train_loss=2.598, val_loss=2.692, train_mse=5.996, val_mse=6.562]      0%|          | 2/1000 [00:00<01:17, 12.80it/s, train_loss=1.790, val_loss=1.827, train_mse=2.045, val_mse=2.248]      0%|          | 3/1000 [00:00<01:17, 12.80it/s, train_loss=1.693, val_loss=1.763, train_mse=1.706, val_mse=1.991]      0%|          | 4/1000 [00:00<01:12, 13.69it/s, train_loss=1.693, val_loss=1.763, train_mse=1.706, val_mse=1.991]      0%|          | 4/1000 [00:00<01:12, 13.69it/s, train_loss=1.678, val_loss=1.771, train_mse=1.690, val_mse=2.001]      0%|          | 5/1000 [00:00<01:12, 13.69it/s, train_loss=1.664, val_loss=1.774, train_mse=1.632, val_mse=2.013]      1%|          | 6/1000 [00:00<01:08, 14.44it/s, train_loss=1.664, val_loss=1.774, train_mse=1.632, val_mse=2.013]      1%|          | 6/1000 [00:00<01:08, 14.44it/s, train_loss=1.687, val_loss=1.783, train_mse=1.724, val_mse=2.059]      1%|          | 7/1000 [00:00<01:08, 14.44it/s, train_loss=1.667, val_loss=1.749, train_mse=1.642, val_mse=1.927]      1%|          | 8/1000 [00:00<01:08, 14.47it/s, train_loss=1.667, val_loss=1.749, train_mse=1.642, val_mse=1.927]      1%|          | 8/1000 [00:00<01:08, 14.47it/s, train_loss=1.656, val_loss=1.756, train_mse=1.628, val_mse=1.954]      1%|          | 9/1000 [00:00<01:08, 14.47it/s, train_loss=1.670, val_loss=1.776, train_mse=1.677, val_mse=2.035]      1%|          | 10/1000 [00:00<01:10, 14.11it/s, train_loss=1.670, val_loss=1.776, train_mse=1.677, val_mse=2.035]      1%|          | 10/1000 [00:00<01:10, 14.11it/s, train_loss=1.658, val_loss=1.780, train_mse=1.601, val_mse=2.032]      1%|          | 11/1000 [00:00<01:10, 14.11it/s, train_loss=1.640, val_loss=1.732, train_mse=1.557, val_mse=1.856]      1%|          | 12/1000 [00:00<01:04, 15.33it/s, train_loss=1.640, val_loss=1.732, train_mse=1.557, val_mse=1.856]      1%|          | 12/1000 [00:00<01:04, 15.33it/s, train_loss=1.611, val_loss=1.708, train_mse=1.464, val_mse=1.772]      1%|▏         | 13/1000 [00:00<01:04, 15.33it/s, train_loss=1.620, val_loss=1.708, train_mse=1.479, val_mse=1.761]      1%|▏         | 14/1000 [00:00<01:01, 16.04it/s, train_loss=1.620, val_loss=1.708, train_mse=1.479, val_mse=1.761]      1%|▏         | 14/1000 [00:00<01:01, 16.04it/s, train_loss=1.617, val_loss=1.688, train_mse=1.471, val_mse=1.693]      2%|▏         | 15/1000 [00:00<01:01, 16.04it/s, train_loss=1.554, val_loss=1.656, train_mse=1.286, val_mse=1.595]      2%|▏         | 16/1000 [00:01<00:57, 17.02it/s, train_loss=1.554, val_loss=1.656, train_mse=1.286, val_mse=1.595]      2%|▏         | 16/1000 [00:01<00:57, 17.02it/s, train_loss=1.545, val_loss=1.647, train_mse=1.271, val_mse=1.568]      2%|▏         | 17/1000 [00:01<00:57, 17.02it/s, train_loss=1.555, val_loss=1.672, train_mse=1.309, val_mse=1.630]      2%|▏         | 18/1000 [00:01<00:56, 17.30it/s, train_loss=1.555, val_loss=1.672, train_mse=1.309, val_mse=1.630]      2%|▏         | 18/1000 [00:01<00:56, 17.30it/s, train_loss=1.538, val_loss=1.668, train_mse=1.265, val_mse=1.606]      2%|▏         | 19/1000 [00:01<00:56, 17.30it/s, train_loss=1.564, val_loss=1.666, train_mse=1.335, val_mse=1.604]      2%|▏         | 20/1000 [00:01<01:01, 15.97it/s, train_loss=1.564, val_loss=1.666, train_mse=1.335, val_mse=1.604]      2%|▏         | 20/1000 [00:01<01:01, 15.97it/s, train_loss=1.576, val_loss=1.678, train_mse=1.371, val_mse=1.653]      2%|▏         | 21/1000 [00:01<01:01, 15.97it/s, train_loss=1.542, val_loss=1.652, train_mse=1.279, val_mse=1.584]      2%|▏         | 22/1000 [00:01<01:02, 15.61it/s, train_loss=1.542, val_loss=1.652, train_mse=1.279, val_mse=1.584]      2%|▏         | 22/1000 [00:01<01:02, 15.61it/s, train_loss=1.551, val_loss=1.655, train_mse=1.303, val_mse=1.585]      2%|▏         | 23/1000 [00:01<01:02, 15.61it/s, train_loss=1.564, val_loss=1.653, train_mse=1.334, val_mse=1.574]      2%|▏         | 24/1000 [00:01<01:00, 16.03it/s, train_loss=1.564, val_loss=1.653, train_mse=1.334, val_mse=1.574]      2%|▏         | 24/1000 [00:01<01:00, 16.03it/s, train_loss=1.542, val_loss=1.653, train_mse=1.277, val_mse=1.577]      2%|▎         | 25/1000 [00:01<01:00, 16.03it/s, train_loss=1.542, val_loss=1.645, train_mse=1.272, val_mse=1.551]      3%|▎         | 26/1000 [00:01<00:59, 16.29it/s, train_loss=1.542, val_loss=1.645, train_mse=1.272, val_mse=1.551]      3%|▎         | 26/1000 [00:01<00:59, 16.29it/s, train_loss=1.527, val_loss=1.645, train_mse=1.238, val_mse=1.552]      3%|▎         | 27/1000 [00:01<00:59, 16.29it/s, train_loss=1.528, val_loss=1.642, train_mse=1.242, val_mse=1.544]      3%|▎         | 28/1000 [00:01<00:57, 16.81it/s, train_loss=1.528, val_loss=1.642, train_mse=1.242, val_mse=1.544]      3%|▎         | 28/1000 [00:01<00:57, 16.81it/s, train_loss=1.537, val_loss=1.637, train_mse=1.265, val_mse=1.522]      3%|▎         | 29/1000 [00:01<00:57, 16.81it/s, train_loss=1.534, val_loss=1.643, train_mse=1.258, val_mse=1.536]      3%|▎         | 30/1000 [00:01<01:00, 16.04it/s, train_loss=1.534, val_loss=1.643, train_mse=1.258, val_mse=1.536]      3%|▎         | 30/1000 [00:01<01:00, 16.04it/s, train_loss=1.546, val_loss=1.658, train_mse=1.289, val_mse=1.575]      3%|▎         | 31/1000 [00:01<01:00, 16.04it/s, train_loss=1.522, val_loss=1.633, train_mse=1.228, val_mse=1.512]      3%|▎         | 32/1000 [00:02<00:58, 16.55it/s, train_loss=1.522, val_loss=1.633, train_mse=1.228, val_mse=1.512]      3%|▎         | 32/1000 [00:02<00:58, 16.55it/s, train_loss=1.529, val_loss=1.641, train_mse=1.246, val_mse=1.531]      3%|▎         | 33/1000 [00:02<00:58, 16.55it/s, train_loss=1.519, val_loss=1.632, train_mse=1.220, val_mse=1.508]      3%|▎         | 34/1000 [00:02<00:58, 16.63it/s, train_loss=1.519, val_loss=1.632, train_mse=1.220, val_mse=1.508]      3%|▎         | 34/1000 [00:02<00:58, 16.63it/s, train_loss=1.518, val_loss=1.636, train_mse=1.218, val_mse=1.520]      4%|▎         | 35/1000 [00:02<00:58, 16.63it/s, train_loss=1.526, val_loss=1.635, train_mse=1.237, val_mse=1.511]      4%|▎         | 36/1000 [00:02<00:55, 17.47it/s, train_loss=1.526, val_loss=1.635, train_mse=1.237, val_mse=1.511]      4%|▎         | 36/1000 [00:02<00:55, 17.47it/s, train_loss=1.519, val_loss=1.633, train_mse=1.219, val_mse=1.500]      4%|▎         | 37/1000 [00:02<00:55, 17.47it/s, train_loss=1.537, val_loss=1.650, train_mse=1.264, val_mse=1.547]      4%|▍         | 38/1000 [00:02<00:53, 17.87it/s, train_loss=1.537, val_loss=1.650, train_mse=1.264, val_mse=1.547]      4%|▍         | 38/1000 [00:02<00:53, 17.87it/s, train_loss=1.526, val_loss=1.629, train_mse=1.238, val_mse=1.499]      4%|▍         | 39/1000 [00:02<00:53, 17.87it/s, train_loss=1.514, val_loss=1.630, train_mse=1.208, val_mse=1.501]      4%|▍         | 40/1000 [00:02<00:52, 18.40it/s, train_loss=1.514, val_loss=1.630, train_mse=1.208, val_mse=1.501]      4%|▍         | 40/1000 [00:02<00:52, 18.40it/s, train_loss=1.531, val_loss=1.641, train_mse=1.250, val_mse=1.523]      4%|▍         | 41/1000 [00:02<00:52, 18.40it/s, train_loss=1.522, val_loss=1.636, train_mse=1.226, val_mse=1.508]      4%|▍         | 42/1000 [00:02<00:51, 18.75it/s, train_loss=1.522, val_loss=1.636, train_mse=1.226, val_mse=1.508]      4%|▍         | 42/1000 [00:02<00:51, 18.75it/s, train_loss=1.525, val_loss=1.641, train_mse=1.236, val_mse=1.525]      4%|▍         | 43/1000 [00:02<00:51, 18.75it/s, train_loss=1.527, val_loss=1.642, train_mse=1.241, val_mse=1.527]      4%|▍         | 44/1000 [00:02<00:50, 19.07it/s, train_loss=1.527, val_loss=1.642, train_mse=1.241, val_mse=1.527]      4%|▍         | 44/1000 [00:02<00:50, 19.07it/s, train_loss=1.526, val_loss=1.642, train_mse=1.238, val_mse=1.529]      4%|▍         | 45/1000 [00:02<00:50, 19.07it/s, train_loss=1.528, val_loss=1.634, train_mse=1.242, val_mse=1.504]      5%|▍         | 46/1000 [00:02<00:49, 19.27it/s, train_loss=1.528, val_loss=1.634, train_mse=1.242, val_mse=1.504]      5%|▍         | 46/1000 [00:02<00:49, 19.27it/s, train_loss=1.527, val_loss=1.643, train_mse=1.241, val_mse=1.528]      5%|▍         | 47/1000 [00:02<00:49, 19.27it/s, train_loss=1.528, val_loss=1.638, train_mse=1.243, val_mse=1.517]      5%|▍         | 48/1000 [00:02<00:49, 19.28it/s, train_loss=1.528, val_loss=1.638, train_mse=1.243, val_mse=1.517]      5%|▍         | 48/1000 [00:02<00:49, 19.28it/s, train_loss=1.523, val_loss=1.633, train_mse=1.230, val_mse=1.506]      5%|▍         | 49/1000 [00:02<00:49, 19.28it/s, train_loss=1.521, val_loss=1.630, train_mse=1.227, val_mse=1.499]      5%|▌         | 50/1000 [00:02<00:49, 19.27it/s, train_loss=1.521, val_loss=1.630, train_mse=1.227, val_mse=1.499]      5%|▌         | 50/1000 [00:02<00:49, 19.27it/s, train_loss=1.518, val_loss=1.633, train_mse=1.219, val_mse=1.501]      5%|▌         | 51/1000 [00:03<00:49, 19.27it/s, train_loss=1.520, val_loss=1.633, train_mse=1.223, val_mse=1.499]      5%|▌         | 52/1000 [00:03<00:54, 17.34it/s, train_loss=1.520, val_loss=1.633, train_mse=1.223, val_mse=1.499]      5%|▌         | 52/1000 [00:03<00:54, 17.34it/s, train_loss=1.515, val_loss=1.634, train_mse=1.211, val_mse=1.507]      5%|▌         | 53/1000 [00:03<00:54, 17.34it/s, train_loss=1.524, val_loss=1.632, train_mse=1.232, val_mse=1.495]      5%|▌         | 54/1000 [00:03<00:52, 18.01it/s, train_loss=1.524, val_loss=1.632, train_mse=1.232, val_mse=1.495]      5%|▌         | 54/1000 [00:03<00:52, 18.01it/s, train_loss=1.525, val_loss=1.632, train_mse=1.235, val_mse=1.492]      6%|▌         | 55/1000 [00:03<00:52, 18.01it/s, train_loss=1.545, val_loss=1.649, train_mse=1.283, val_mse=1.545]      6%|▌         | 56/1000 [00:03<00:51, 18.20it/s, train_loss=1.545, val_loss=1.649, train_mse=1.283, val_mse=1.545]      6%|▌         | 56/1000 [00:03<00:51, 18.20it/s, train_loss=1.531, val_loss=1.628, train_mse=1.250, val_mse=1.497]      6%|▌         | 57/1000 [00:03<00:51, 18.20it/s, train_loss=1.520, val_loss=1.631, train_mse=1.220, val_mse=1.502]      6%|▌         | 58/1000 [00:03<00:51, 18.37it/s, train_loss=1.520, val_loss=1.631, train_mse=1.220, val_mse=1.502]      6%|▌         | 58/1000 [00:03<00:51, 18.37it/s, train_loss=1.540, val_loss=1.638, train_mse=1.272, val_mse=1.522]      6%|▌         | 59/1000 [00:03<00:51, 18.37it/s, train_loss=1.548, val_loss=1.633, train_mse=1.291, val_mse=1.506]      6%|▌         | 60/1000 [00:03<00:50, 18.58it/s, train_loss=1.548, val_loss=1.633, train_mse=1.291, val_mse=1.506]      6%|▌         | 60/1000 [00:03<00:50, 18.58it/s, train_loss=1.528, val_loss=1.630, train_mse=1.242, val_mse=1.501]      6%|▌         | 61/1000 [00:03<00:50, 18.58it/s, train_loss=1.518, val_loss=1.628, train_mse=1.217, val_mse=1.501]      6%|▌         | 62/1000 [00:03<00:49, 18.92it/s, train_loss=1.518, val_loss=1.628, train_mse=1.217, val_mse=1.501]      6%|▌         | 62/1000 [00:03<00:49, 18.92it/s, train_loss=1.532, val_loss=1.633, train_mse=1.253, val_mse=1.505]      6%|▋         | 63/1000 [00:03<00:49, 18.92it/s, train_loss=1.532, val_loss=1.642, train_mse=1.253, val_mse=1.524]      6%|▋         | 64/1000 [00:03<00:49, 18.92it/s, train_loss=1.529, val_loss=1.630, train_mse=1.244, val_mse=1.499]      6%|▋         | 65/1000 [00:03<00:48, 19.36it/s, train_loss=1.529, val_loss=1.630, train_mse=1.244, val_mse=1.499]      6%|▋         | 65/1000 [00:03<00:48, 19.36it/s, train_loss=1.531, val_loss=1.638, train_mse=1.250, val_mse=1.522]      7%|▋         | 66/1000 [00:03<00:48, 19.36it/s, train_loss=1.519, val_loss=1.627, train_mse=1.220, val_mse=1.494]      7%|▋         | 67/1000 [00:03<00:48, 19.25it/s, train_loss=1.519, val_loss=1.627, train_mse=1.220, val_mse=1.494]      7%|▋         | 67/1000 [00:03<00:48, 19.25it/s, train_loss=1.522, val_loss=1.635, train_mse=1.227, val_mse=1.508]      7%|▋         | 68/1000 [00:03<00:48, 19.25it/s, train_loss=1.518, val_loss=1.631, train_mse=1.219, val_mse=1.492]      7%|▋         | 69/1000 [00:03<00:49, 18.64it/s, train_loss=1.518, val_loss=1.631, train_mse=1.219, val_mse=1.492]      7%|▋         | 69/1000 [00:03<00:49, 18.64it/s, train_loss=1.514, val_loss=1.630, train_mse=1.210, val_mse=1.492]      7%|▋         | 70/1000 [00:04<00:49, 18.64it/s, train_loss=1.519, val_loss=1.638, train_mse=1.220, val_mse=1.512]      7%|▋         | 71/1000 [00:04<00:52, 17.59it/s, train_loss=1.519, val_loss=1.638, train_mse=1.220, val_mse=1.512]      7%|▋         | 71/1000 [00:04<00:52, 17.59it/s, train_loss=1.535, val_loss=1.630, train_mse=1.261, val_mse=1.493]      7%|▋         | 72/1000 [00:04<00:52, 17.59it/s, train_loss=1.699, val_loss=1.936, train_mse=1.678, val_mse=2.312]      7%|▋         | 73/1000 [00:04<00:52, 17.59it/s, train_loss=1.662, val_loss=1.738, train_mse=1.658, val_mse=1.901]      7%|▋         | 74/1000 [00:04<00:50, 18.33it/s, train_loss=1.662, val_loss=1.738, train_mse=1.658, val_mse=1.901]      7%|▋         | 74/1000 [00:04<00:50, 18.33it/s, train_loss=1.624, val_loss=1.741, train_mse=1.476, val_mse=1.852]      8%|▊         | 75/1000 [00:04<00:50, 18.33it/s, train_loss=1.578, val_loss=1.654, train_mse=1.372, val_mse=1.608]      8%|▊         | 76/1000 [00:04<00:49, 18.71it/s, train_loss=1.578, val_loss=1.654, train_mse=1.372, val_mse=1.608]      8%|▊         | 76/1000 [00:04<00:49, 18.71it/s, train_loss=1.548, val_loss=1.649, train_mse=1.286, val_mse=1.587]      8%|▊         | 77/1000 [00:04<00:49, 18.71it/s, train_loss=1.567, val_loss=1.693, train_mse=1.345, val_mse=1.690]      8%|▊         | 78/1000 [00:04<00:48, 18.91it/s, train_loss=1.567, val_loss=1.693, train_mse=1.345, val_mse=1.690]      8%|▊         | 78/1000 [00:04<00:48, 18.91it/s, train_loss=1.556, val_loss=1.648, train_mse=1.311, val_mse=1.564]      8%|▊         | 79/1000 [00:04<00:48, 18.91it/s, train_loss=1.532, val_loss=1.637, train_mse=1.251, val_mse=1.528]      8%|▊         | 80/1000 [00:04<00:47, 19.18it/s, train_loss=1.532, val_loss=1.637, train_mse=1.251, val_mse=1.528]      8%|▊         | 80/1000 [00:04<00:47, 19.18it/s, train_loss=1.532, val_loss=1.643, train_mse=1.254, val_mse=1.535]      8%|▊         | 81/1000 [00:04<00:47, 19.18it/s, train_loss=1.528, val_loss=1.645, train_mse=1.244, val_mse=1.540]      8%|▊         | 82/1000 [00:04<00:48, 19.08it/s, train_loss=1.528, val_loss=1.645, train_mse=1.244, val_mse=1.540]      8%|▊         | 82/1000 [00:04<00:48, 19.08it/s, train_loss=1.536, val_loss=1.646, train_mse=1.259, val_mse=1.552]      8%|▊         | 83/1000 [00:04<00:48, 19.08it/s, train_loss=1.554, val_loss=1.652, train_mse=1.308, val_mse=1.596]      8%|▊         | 84/1000 [00:04<00:47, 19.28it/s, train_loss=1.554, val_loss=1.652, train_mse=1.308, val_mse=1.596]      8%|▊         | 84/1000 [00:04<00:47, 19.28it/s, train_loss=1.553, val_loss=1.667, train_mse=1.310, val_mse=1.627]      8%|▊         | 85/1000 [00:04<00:47, 19.28it/s, train_loss=1.552, val_loss=1.679, train_mse=1.305, val_mse=1.638]      9%|▊         | 86/1000 [00:04<00:47, 19.30it/s, train_loss=1.552, val_loss=1.679, train_mse=1.305, val_mse=1.638]      9%|▊         | 86/1000 [00:04<00:47, 19.30it/s, train_loss=1.556, val_loss=1.665, train_mse=1.316, val_mse=1.607]      9%|▊         | 87/1000 [00:04<00:47, 19.30it/s, train_loss=1.557, val_loss=1.649, train_mse=1.322, val_mse=1.568]      9%|▉         | 88/1000 [00:04<00:47, 19.15it/s, train_loss=1.557, val_loss=1.649, train_mse=1.322, val_mse=1.568]      9%|▉         | 88/1000 [00:04<00:47, 19.15it/s, train_loss=1.529, val_loss=1.640, train_mse=1.247, val_mse=1.543]      9%|▉         | 89/1000 [00:05<00:47, 19.15it/s, train_loss=1.525, val_loss=1.643, train_mse=1.235, val_mse=1.547]      9%|▉         | 90/1000 [00:05<00:48, 18.88it/s, train_loss=1.525, val_loss=1.643, train_mse=1.235, val_mse=1.547]      9%|▉         | 90/1000 [00:05<00:48, 18.88it/s, train_loss=1.522, val_loss=1.642, train_mse=1.230, val_mse=1.545]      9%|▉         | 91/1000 [00:05<00:48, 18.88it/s, train_loss=1.533, val_loss=1.642, train_mse=1.258, val_mse=1.541]      9%|▉         | 92/1000 [00:05<00:50, 17.86it/s, train_loss=1.533, val_loss=1.642, train_mse=1.258, val_mse=1.541]      9%|▉         | 92/1000 [00:05<00:50, 17.86it/s, train_loss=1.527, val_loss=1.642, train_mse=1.243, val_mse=1.540]      9%|▉         | 93/1000 [00:05<00:50, 17.86it/s, train_loss=1.534, val_loss=1.644, train_mse=1.262, val_mse=1.545]      9%|▉         | 94/1000 [00:05<00:52, 17.29it/s, train_loss=1.534, val_loss=1.644, train_mse=1.262, val_mse=1.545]      9%|▉         | 94/1000 [00:05<00:52, 17.29it/s, train_loss=1.531, val_loss=1.642, train_mse=1.250, val_mse=1.540]     10%|▉         | 95/1000 [00:05<00:52, 17.29it/s, train_loss=1.534, val_loss=1.644, train_mse=1.260, val_mse=1.544]     10%|▉         | 96/1000 [00:05<00:54, 16.53it/s, train_loss=1.534, val_loss=1.644, train_mse=1.260, val_mse=1.544]     10%|▉         | 96/1000 [00:05<00:54, 16.53it/s, train_loss=1.528, val_loss=1.645, train_mse=1.245, val_mse=1.548]     10%|▉         | 97/1000 [00:05<00:54, 16.53it/s, train_loss=1.527, val_loss=1.643, train_mse=1.242, val_mse=1.542]     10%|▉         | 98/1000 [00:05<00:54, 16.52it/s, train_loss=1.527, val_loss=1.643, train_mse=1.242, val_mse=1.542]     10%|▉         | 98/1000 [00:05<00:54, 16.52it/s, train_loss=1.523, val_loss=1.642, train_mse=1.231, val_mse=1.538]     10%|▉         | 99/1000 [00:05<00:54, 16.52it/s, train_loss=1.528, val_loss=1.641, train_mse=1.245, val_mse=1.536]     10%|█         | 100/1000 [00:05<00:53, 16.88it/s, train_loss=1.528, val_loss=1.641, train_mse=1.245, val_mse=1.536]     10%|█         | 100/1000 [00:05<00:53, 16.88it/s, train_loss=1.527, val_loss=1.642, train_mse=1.243, val_mse=1.538]     10%|█         | 101/1000 [00:05<00:53, 16.88it/s, train_loss=1.530, val_loss=1.644, train_mse=1.250, val_mse=1.541]     10%|█         | 102/1000 [00:05<00:51, 17.42it/s, train_loss=1.530, val_loss=1.644, train_mse=1.250, val_mse=1.541]     10%|█         | 102/1000 [00:05<00:51, 17.42it/s, train_loss=1.522, val_loss=1.644, train_mse=1.230, val_mse=1.542]     10%|█         | 103/1000 [00:05<00:51, 17.42it/s, train_loss=1.520, val_loss=1.644, train_mse=1.224, val_mse=1.541]     10%|█         | 104/1000 [00:05<00:50, 17.76it/s, train_loss=1.520, val_loss=1.644, train_mse=1.224, val_mse=1.541]     10%|█         | 104/1000 [00:05<00:50, 17.76it/s, train_loss=1.534, val_loss=1.644, train_mse=1.261, val_mse=1.540]     10%|█         | 105/1000 [00:05<00:50, 17.76it/s, train_loss=1.531, val_loss=1.644, train_mse=1.253, val_mse=1.540]     11%|█         | 106/1000 [00:06<00:49, 18.12it/s, train_loss=1.531, val_loss=1.644, train_mse=1.253, val_mse=1.540]     11%|█         | 106/1000 [00:06<00:49, 18.12it/s, train_loss=1.543, val_loss=1.642, train_mse=1.282, val_mse=1.536]     11%|█         | 107/1000 [00:06<00:49, 18.12it/s, train_loss=1.524, val_loss=1.642, train_mse=1.235, val_mse=1.536]     11%|█         | 108/1000 [00:06<00:48, 18.30it/s, train_loss=1.524, val_loss=1.642, train_mse=1.235, val_mse=1.536]     11%|█         | 108/1000 [00:06<00:48, 18.30it/s, train_loss=1.530, val_loss=1.642, train_mse=1.251, val_mse=1.536]     11%|█         | 109/1000 [00:06<00:48, 18.30it/s, train_loss=1.527, val_loss=1.642, train_mse=1.244, val_mse=1.536]     11%|█         | 110/1000 [00:06<00:48, 18.38it/s, train_loss=1.527, val_loss=1.642, train_mse=1.244, val_mse=1.536]     11%|█         | 110/1000 [00:06<00:48, 18.38it/s, train_loss=1.527, val_loss=1.642, train_mse=1.241, val_mse=1.537]     11%|█         | 111/1000 [00:06<00:48, 18.38it/s, train_loss=1.527, val_loss=1.642, train_mse=1.243, val_mse=1.537]     11%|█         | 112/1000 [00:06<00:47, 18.73it/s, train_loss=1.527, val_loss=1.642, train_mse=1.243, val_mse=1.537]     11%|█         | 112/1000 [00:06<00:47, 18.73it/s, train_loss=1.530, val_loss=1.642, train_mse=1.250, val_mse=1.537]     11%|█▏        | 113/1000 [00:06<00:47, 18.73it/s, train_loss=1.533, val_loss=1.642, train_mse=1.256, val_mse=1.536]     11%|█▏        | 114/1000 [00:06<00:46, 19.01it/s, train_loss=1.533, val_loss=1.642, train_mse=1.256, val_mse=1.536]     11%|█▏        | 114/1000 [00:06<00:46, 19.01it/s, train_loss=1.528, val_loss=1.642, train_mse=1.245, val_mse=1.536]     12%|█▏        | 115/1000 [00:06<00:46, 19.01it/s, train_loss=1.520, val_loss=1.642, train_mse=1.225, val_mse=1.536]     12%|█▏        | 116/1000 [00:06<00:46, 19.09it/s, train_loss=1.520, val_loss=1.642, train_mse=1.225, val_mse=1.536]     12%|█▏        | 116/1000 [00:06<00:46, 19.09it/s, train_loss=1.525, val_loss=1.642, train_mse=1.237, val_mse=1.536]     12%|█▏        | 117/1000 [00:06<00:46, 19.09it/s, train_loss=1.531, val_loss=1.642, train_mse=1.254, val_mse=1.536]     12%|█▏        | 118/1000 [00:06<00:46, 18.78it/s, train_loss=1.531, val_loss=1.642, train_mse=1.254, val_mse=1.536]     12%|█▏        | 118/1000 [00:06<00:46, 18.78it/s, train_loss=1.531, val_loss=1.642, train_mse=1.253, val_mse=1.537]     12%|█▏        | 119/1000 [00:06<00:46, 18.78it/s, train_loss=1.529, val_loss=1.642, train_mse=1.249, val_mse=1.537]     12%|█▏        | 120/1000 [00:06<00:46, 18.94it/s, train_loss=1.529, val_loss=1.642, train_mse=1.249, val_mse=1.537]     12%|█▏        | 120/1000 [00:06<00:46, 18.94it/s, train_loss=1.538, val_loss=1.642, train_mse=1.270, val_mse=1.537]     12%|█▏        | 121/1000 [00:06<00:46, 18.94it/s, train_loss=1.535, val_loss=1.642, train_mse=1.261, val_mse=1.537]     12%|█▏        | 122/1000 [00:06<00:46, 18.95it/s, train_loss=1.535, val_loss=1.642, train_mse=1.261, val_mse=1.537]     12%|█▏        | 122/1000 [00:06<00:46, 18.95it/s, train_loss=1.530, val_loss=1.642, train_mse=1.252, val_mse=1.537]     12%|█▏        | 123/1000 [00:06<00:46, 18.95it/s, train_loss=1.523, val_loss=1.642, train_mse=1.233, val_mse=1.536]     12%|█▏        | 124/1000 [00:06<00:46, 19.04it/s, train_loss=1.523, val_loss=1.642, train_mse=1.233, val_mse=1.536]     12%|█▏        | 124/1000 [00:06<00:46, 19.04it/s, train_loss=1.516, val_loss=1.642, train_mse=1.214, val_mse=1.536]     12%|█▎        | 125/1000 [00:07<00:45, 19.04it/s, train_loss=1.520, val_loss=1.642, train_mse=1.225, val_mse=1.537]     13%|█▎        | 126/1000 [00:07<00:46, 18.86it/s, train_loss=1.520, val_loss=1.642, train_mse=1.225, val_mse=1.537]     13%|█▎        | 126/1000 [00:07<00:46, 18.86it/s, train_loss=1.525, val_loss=1.642, train_mse=1.239, val_mse=1.537]     13%|█▎        | 127/1000 [00:07<00:46, 18.86it/s, train_loss=1.518, val_loss=1.642, train_mse=1.219, val_mse=1.536]     13%|█▎        | 128/1000 [00:07<00:45, 19.03it/s, train_loss=1.518, val_loss=1.642, train_mse=1.219, val_mse=1.536]     13%|█▎        | 128/1000 [00:07<00:45, 19.03it/s, train_loss=1.526, val_loss=1.642, train_mse=1.240, val_mse=1.536]     13%|█▎        | 129/1000 [00:07<00:45, 19.03it/s, train_loss=1.518, val_loss=1.642, train_mse=1.221, val_mse=1.536]     13%|█▎        | 130/1000 [00:07<00:46, 18.88it/s, train_loss=1.518, val_loss=1.642, train_mse=1.221, val_mse=1.536]     13%|█▎        | 130/1000 [00:07<00:46, 18.88it/s, train_loss=1.531, val_loss=1.642, train_mse=1.253, val_mse=1.536]     13%|█▎        | 131/1000 [00:07<00:46, 18.88it/s, train_loss=1.527, val_loss=1.642, train_mse=1.244, val_mse=1.536]     13%|█▎        | 132/1000 [00:07<00:45, 18.88it/s, train_loss=1.526, val_loss=1.642, train_mse=1.241, val_mse=1.536]     13%|█▎        | 133/1000 [00:07<00:44, 19.33it/s, train_loss=1.526, val_loss=1.642, train_mse=1.241, val_mse=1.536]     13%|█▎        | 133/1000 [00:07<00:44, 19.33it/s, train_loss=1.525, val_loss=1.642, train_mse=1.237, val_mse=1.536]     13%|█▎        | 134/1000 [00:07<00:44, 19.33it/s, train_loss=1.533, val_loss=1.642, train_mse=1.259, val_mse=1.536]     14%|█▎        | 135/1000 [00:07<00:44, 19.30it/s, train_loss=1.533, val_loss=1.642, train_mse=1.259, val_mse=1.536]     14%|█▎        | 135/1000 [00:07<00:44, 19.30it/s, train_loss=1.532, val_loss=1.642, train_mse=1.256, val_mse=1.536]     14%|█▎        | 136/1000 [00:07<00:44, 19.30it/s, train_loss=1.528, val_loss=1.642, train_mse=1.244, val_mse=1.536]     14%|█▎        | 137/1000 [00:07<00:44, 19.35it/s, train_loss=1.528, val_loss=1.642, train_mse=1.244, val_mse=1.536]     14%|█▎        | 137/1000 [00:07<00:44, 19.35it/s, train_loss=1.543, val_loss=1.642, train_mse=1.286, val_mse=1.536]     14%|█▍        | 138/1000 [00:07<00:44, 19.35it/s, train_loss=1.522, val_loss=1.642, train_mse=1.230, val_mse=1.536]     14%|█▍        | 139/1000 [00:07<00:46, 18.63it/s, train_loss=1.522, val_loss=1.642, train_mse=1.230, val_mse=1.536]     14%|█▍        | 139/1000 [00:07<00:46, 18.63it/s, train_loss=1.528, val_loss=1.642, train_mse=1.246, val_mse=1.536]     14%|█▍        | 140/1000 [00:07<00:46, 18.63it/s, train_loss=1.525, val_loss=1.642, train_mse=1.237, val_mse=1.536]     14%|█▍        | 141/1000 [00:07<00:45, 18.77it/s, train_loss=1.525, val_loss=1.642, train_mse=1.237, val_mse=1.536]     14%|█▍        | 141/1000 [00:07<00:45, 18.77it/s, train_loss=1.526, val_loss=1.642, train_mse=1.239, val_mse=1.536]     14%|█▍        | 142/1000 [00:07<00:45, 18.77it/s, train_loss=1.517, val_loss=1.642, train_mse=1.218, val_mse=1.536]     14%|█▍        | 143/1000 [00:07<00:45, 18.95it/s, train_loss=1.517, val_loss=1.642, train_mse=1.218, val_mse=1.536]     14%|█▍        | 143/1000 [00:07<00:45, 18.95it/s, train_loss=1.522, val_loss=1.642, train_mse=1.230, val_mse=1.536]     14%|█▍        | 144/1000 [00:08<00:45, 18.95it/s, train_loss=1.529, val_loss=1.642, train_mse=1.249, val_mse=1.536]     14%|█▍        | 145/1000 [00:08<00:45, 18.86it/s, train_loss=1.529, val_loss=1.642, train_mse=1.249, val_mse=1.536]     14%|█▍        | 145/1000 [00:08<00:45, 18.86it/s, train_loss=1.519, val_loss=1.642, train_mse=1.222, val_mse=1.536]     15%|█▍        | 146/1000 [00:08<00:45, 18.86it/s, train_loss=1.519, val_loss=1.642, train_mse=1.223, val_mse=1.536]     15%|█▍        | 147/1000 [00:08<00:44, 19.04it/s, train_loss=1.519, val_loss=1.642, train_mse=1.223, val_mse=1.536]     15%|█▍        | 147/1000 [00:08<00:44, 19.04it/s, train_loss=1.516, val_loss=1.642, train_mse=1.214, val_mse=1.536]     15%|█▍        | 148/1000 [00:08<00:44, 19.04it/s, train_loss=1.523, val_loss=1.642, train_mse=1.232, val_mse=1.536]     15%|█▍        | 149/1000 [00:08<00:44, 19.04it/s, train_loss=1.523, val_loss=1.642, train_mse=1.232, val_mse=1.536]     15%|█▍        | 149/1000 [00:08<00:44, 19.04it/s, train_loss=1.521, val_loss=1.642, train_mse=1.228, val_mse=1.536]     15%|█▌        | 150/1000 [00:08<00:44, 19.04it/s, train_loss=1.520, val_loss=1.642, train_mse=1.225, val_mse=1.536]     15%|█▌        | 151/1000 [00:08<00:44, 19.19it/s, train_loss=1.520, val_loss=1.642, train_mse=1.225, val_mse=1.536]     15%|█▌        | 151/1000 [00:08<00:44, 19.19it/s, train_loss=1.533, val_loss=1.642, train_mse=1.258, val_mse=1.536]     15%|█▌        | 152/1000 [00:08<00:44, 19.19it/s, train_loss=1.524, val_loss=1.642, train_mse=1.234, val_mse=1.536]     15%|█▌        | 153/1000 [00:08<00:44, 18.83it/s, train_loss=1.524, val_loss=1.642, train_mse=1.234, val_mse=1.536]     15%|█▌        | 153/1000 [00:08<00:44, 18.83it/s, train_loss=1.514, val_loss=1.642, train_mse=1.210, val_mse=1.536]     15%|█▌        | 154/1000 [00:08<00:44, 18.83it/s, train_loss=1.530, val_loss=1.642, train_mse=1.250, val_mse=1.536]     16%|█▌        | 155/1000 [00:08<00:45, 18.66it/s, train_loss=1.530, val_loss=1.642, train_mse=1.250, val_mse=1.536]     16%|█▌        | 155/1000 [00:08<00:45, 18.66it/s, train_loss=1.518, val_loss=1.642, train_mse=1.219, val_mse=1.536]     16%|█▌        | 156/1000 [00:08<00:45, 18.66it/s, train_loss=1.523, val_loss=1.642, train_mse=1.233, val_mse=1.536]     16%|█▌        | 157/1000 [00:08<00:44, 18.87it/s, train_loss=1.523, val_loss=1.642, train_mse=1.233, val_mse=1.536]     16%|█▌        | 157/1000 [00:08<00:44, 18.87it/s, train_loss=1.524, val_loss=1.642, train_mse=1.236, val_mse=1.536]     16%|█▌        | 158/1000 [00:08<00:44, 18.87it/s, train_loss=1.533, val_loss=1.642, train_mse=1.257, val_mse=1.536]     16%|█▌        | 159/1000 [00:08<00:44, 18.86it/s, train_loss=1.533, val_loss=1.642, train_mse=1.257, val_mse=1.536]     16%|█▌        | 159/1000 [00:08<00:44, 18.86it/s, train_loss=1.524, val_loss=1.642, train_mse=1.236, val_mse=1.536]     16%|█▌        | 160/1000 [00:08<00:44, 18.86it/s, train_loss=1.510, val_loss=1.642, train_mse=1.199, val_mse=1.536]     16%|█▌        | 161/1000 [00:08<00:44, 18.94it/s, train_loss=1.510, val_loss=1.642, train_mse=1.199, val_mse=1.536]     16%|█▌        | 161/1000 [00:08<00:44, 18.94it/s, train_loss=1.544, val_loss=1.642, train_mse=1.286, val_mse=1.536]     16%|█▌        | 162/1000 [00:08<00:44, 18.94it/s, train_loss=1.526, val_loss=1.642, train_mse=1.242, val_mse=1.536]     16%|█▋        | 163/1000 [00:09<00:44, 18.72it/s, train_loss=1.526, val_loss=1.642, train_mse=1.242, val_mse=1.536]     16%|█▋        | 163/1000 [00:09<00:44, 18.72it/s, train_loss=1.536, val_loss=1.642, train_mse=1.268, val_mse=1.536]     16%|█▋        | 164/1000 [00:09<00:44, 18.72it/s, train_loss=1.537, val_loss=1.642, train_mse=1.269, val_mse=1.536]     16%|█▋        | 165/1000 [00:09<00:44, 18.79it/s, train_loss=1.537, val_loss=1.642, train_mse=1.269, val_mse=1.536]     16%|█▋        | 165/1000 [00:09<00:44, 18.79it/s, train_loss=1.529, val_loss=1.642, train_mse=1.248, val_mse=1.536]     17%|█▋        | 166/1000 [00:09<00:44, 18.79it/s, train_loss=1.536, val_loss=1.642, train_mse=1.267, val_mse=1.536]     17%|█▋        | 167/1000 [00:09<00:44, 18.79it/s, train_loss=1.544, val_loss=1.642, train_mse=1.286, val_mse=1.536]     17%|█▋        | 168/1000 [00:09<00:43, 19.27it/s, train_loss=1.544, val_loss=1.642, train_mse=1.286, val_mse=1.536]     17%|█▋        | 168/1000 [00:09<00:43, 19.27it/s, train_loss=1.531, val_loss=1.642, train_mse=1.254, val_mse=1.536]     17%|█▋        | 169/1000 [00:09<00:43, 19.27it/s, train_loss=1.544, val_loss=1.642, train_mse=1.289, val_mse=1.536]     17%|█▋        | 170/1000 [00:09<00:43, 19.12it/s, train_loss=1.544, val_loss=1.642, train_mse=1.289, val_mse=1.536]     17%|█▋        | 170/1000 [00:09<00:43, 19.12it/s, train_loss=1.522, val_loss=1.642, train_mse=1.230, val_mse=1.536]     17%|█▋        | 171/1000 [00:09<00:43, 19.12it/s, train_loss=1.539, val_loss=1.642, train_mse=1.271, val_mse=1.536]     17%|█▋        | 172/1000 [00:09<00:43, 19.12it/s, train_loss=1.530, val_loss=1.642, train_mse=1.252, val_mse=1.536]     17%|█▋        | 173/1000 [00:09<00:43, 19.15it/s, train_loss=1.530, val_loss=1.642, train_mse=1.252, val_mse=1.536]     17%|█▋        | 173/1000 [00:09<00:43, 19.15it/s, train_loss=1.531, val_loss=1.642, train_mse=1.251, val_mse=1.536]     17%|█▋        | 174/1000 [00:09<00:43, 19.15it/s, train_loss=1.536, val_loss=1.642, train_mse=1.267, val_mse=1.536]     18%|█▊        | 175/1000 [00:09<00:43, 19.10it/s, train_loss=1.536, val_loss=1.642, train_mse=1.267, val_mse=1.536]     18%|█▊        | 175/1000 [00:09<00:43, 19.10it/s, train_loss=1.530, val_loss=1.642, train_mse=1.252, val_mse=1.536]     18%|█▊        | 176/1000 [00:09<00:43, 19.10it/s, train_loss=1.513, val_loss=1.642, train_mse=1.208, val_mse=1.536]     18%|█▊        | 177/1000 [00:09<00:43, 18.97it/s, train_loss=1.513, val_loss=1.642, train_mse=1.208, val_mse=1.536]     18%|█▊        | 177/1000 [00:09<00:43, 18.97it/s, train_loss=1.525, val_loss=1.642, train_mse=1.239, val_mse=1.536]     18%|█▊        | 178/1000 [00:09<00:43, 18.97it/s, train_loss=1.521, val_loss=1.642, train_mse=1.228, val_mse=1.536]     18%|█▊        | 179/1000 [00:09<00:42, 19.15it/s, train_loss=1.521, val_loss=1.642, train_mse=1.228, val_mse=1.536]     18%|█▊        | 179/1000 [00:09<00:42, 19.15it/s, train_loss=1.529, val_loss=1.642, train_mse=1.247, val_mse=1.536]     18%|█▊        | 180/1000 [00:09<00:42, 19.15it/s, train_loss=1.516, val_loss=1.642, train_mse=1.216, val_mse=1.536]     18%|█▊        | 181/1000 [00:09<00:42, 19.21it/s, train_loss=1.516, val_loss=1.642, train_mse=1.216, val_mse=1.536]     18%|█▊        | 181/1000 [00:09<00:42, 19.21it/s, train_loss=1.530, val_loss=1.642, train_mse=1.251, val_mse=1.536]     18%|█▊        | 182/1000 [00:10<00:42, 19.21it/s, train_loss=1.537, val_loss=1.642, train_mse=1.270, val_mse=1.536]     18%|█▊        | 183/1000 [00:10<00:42, 19.30it/s, train_loss=1.537, val_loss=1.642, train_mse=1.270, val_mse=1.536]     18%|█▊        | 183/1000 [00:10<00:42, 19.30it/s, train_loss=1.529, val_loss=1.642, train_mse=1.246, val_mse=1.536]     18%|█▊        | 184/1000 [00:10<00:42, 19.30it/s, train_loss=1.523, val_loss=1.642, train_mse=1.233, val_mse=1.536]     18%|█▊        | 185/1000 [00:10<00:42, 19.27it/s, train_loss=1.523, val_loss=1.642, train_mse=1.233, val_mse=1.536]     18%|█▊        | 185/1000 [00:10<00:42, 19.27it/s, train_loss=1.518, val_loss=1.642, train_mse=1.221, val_mse=1.536]     19%|█▊        | 186/1000 [00:10<00:42, 19.27it/s, train_loss=1.528, val_loss=1.642, train_mse=1.243, val_mse=1.536]     19%|█▊        | 187/1000 [00:10<00:41, 19.48it/s, train_loss=1.528, val_loss=1.642, train_mse=1.243, val_mse=1.536]     19%|█▊        | 187/1000 [00:10<00:41, 19.48it/s, train_loss=1.531, val_loss=1.642, train_mse=1.252, val_mse=1.536]     19%|█▉        | 188/1000 [00:10<00:41, 19.48it/s, train_loss=1.519, val_loss=1.642, train_mse=1.223, val_mse=1.536]     19%|█▉        | 189/1000 [00:10<00:41, 19.62it/s, train_loss=1.519, val_loss=1.642, train_mse=1.223, val_mse=1.536]     19%|█▉        | 189/1000 [00:10<00:41, 19.62it/s, train_loss=1.538, val_loss=1.642, train_mse=1.274, val_mse=1.536]     19%|█▉        | 190/1000 [00:10<00:41, 19.62it/s, train_loss=1.520, val_loss=1.642, train_mse=1.225, val_mse=1.536]     19%|█▉        | 191/1000 [00:10<00:41, 19.57it/s, train_loss=1.520, val_loss=1.642, train_mse=1.225, val_mse=1.536]     19%|█▉        | 191/1000 [00:10<00:41, 19.57it/s, train_loss=1.522, val_loss=1.642, train_mse=1.231, val_mse=1.536]     19%|█▉        | 192/1000 [00:10<00:41, 19.57it/s, train_loss=1.522, val_loss=1.642, train_mse=1.230, val_mse=1.536]     19%|█▉        | 193/1000 [00:10<00:41, 19.33it/s, train_loss=1.522, val_loss=1.642, train_mse=1.230, val_mse=1.536]     19%|█▉        | 193/1000 [00:10<00:41, 19.33it/s, train_loss=1.527, val_loss=1.642, train_mse=1.243, val_mse=1.536]     19%|█▉        | 194/1000 [00:10<00:41, 19.33it/s, train_loss=1.521, val_loss=1.642, train_mse=1.227, val_mse=1.536]     20%|█▉        | 195/1000 [00:10<00:41, 19.36it/s, train_loss=1.521, val_loss=1.642, train_mse=1.227, val_mse=1.536]     20%|█▉        | 195/1000 [00:10<00:41, 19.36it/s, train_loss=1.538, val_loss=1.642, train_mse=1.271, val_mse=1.536]     20%|█▉        | 196/1000 [00:10<00:41, 19.36it/s, train_loss=1.529, val_loss=1.642, train_mse=1.246, val_mse=1.536]     20%|█▉        | 197/1000 [00:10<00:42, 19.00it/s, train_loss=1.529, val_loss=1.642, train_mse=1.246, val_mse=1.536]     20%|█▉        | 197/1000 [00:10<00:42, 19.00it/s, train_loss=1.524, val_loss=1.642, train_mse=1.235, val_mse=1.536]     20%|█▉        | 198/1000 [00:10<00:42, 19.00it/s, train_loss=1.513, val_loss=1.642, train_mse=1.206, val_mse=1.536]     20%|█▉        | 199/1000 [00:10<00:42, 18.97it/s, train_loss=1.513, val_loss=1.642, train_mse=1.206, val_mse=1.536]     20%|█▉        | 199/1000 [00:10<00:42, 18.97it/s, train_loss=1.512, val_loss=1.642, train_mse=1.205, val_mse=1.536]     20%|██        | 200/1000 [00:10<00:42, 18.97it/s, train_loss=1.542, val_loss=1.642, train_mse=1.281, val_mse=1.536]     20%|██        | 201/1000 [00:11<00:41, 19.22it/s, train_loss=1.542, val_loss=1.642, train_mse=1.281, val_mse=1.536]     20%|██        | 201/1000 [00:11<00:41, 19.22it/s, train_loss=1.520, val_loss=1.642, train_mse=1.225, val_mse=1.536]     20%|██        | 202/1000 [00:11<00:41, 19.22it/s, train_loss=1.529, val_loss=1.642, train_mse=1.247, val_mse=1.536]     20%|██        | 203/1000 [00:11<00:41, 19.38it/s, train_loss=1.529, val_loss=1.642, train_mse=1.247, val_mse=1.536]     20%|██        | 203/1000 [00:11<00:41, 19.38it/s, train_loss=1.524, val_loss=1.642, train_mse=1.236, val_mse=1.536]     20%|██        | 204/1000 [00:11<00:41, 19.38it/s, train_loss=1.527, val_loss=1.642, train_mse=1.243, val_mse=1.536]     20%|██        | 205/1000 [00:11<00:41, 19.04it/s, train_loss=1.527, val_loss=1.642, train_mse=1.243, val_mse=1.536]     20%|██        | 205/1000 [00:11<00:41, 19.04it/s, train_loss=1.533, val_loss=1.642, train_mse=1.259, val_mse=1.536]     21%|██        | 206/1000 [00:11<00:41, 19.04it/s, train_loss=1.535, val_loss=1.642, train_mse=1.262, val_mse=1.536]     21%|██        | 207/1000 [00:11<00:41, 19.03it/s, train_loss=1.535, val_loss=1.642, train_mse=1.262, val_mse=1.536]     21%|██        | 207/1000 [00:11<00:41, 19.03it/s, train_loss=1.524, val_loss=1.642, train_mse=1.235, val_mse=1.536]     21%|██        | 208/1000 [00:11<00:41, 19.03it/s, train_loss=1.525, val_loss=1.642, train_mse=1.239, val_mse=1.536]     21%|██        | 209/1000 [00:11<00:41, 19.03it/s, train_loss=1.533, val_loss=1.642, train_mse=1.257, val_mse=1.536]     21%|██        | 210/1000 [00:11<00:41, 19.17it/s, train_loss=1.533, val_loss=1.642, train_mse=1.257, val_mse=1.536]     21%|██        | 210/1000 [00:11<00:41, 19.17it/s, train_loss=1.543, val_loss=1.642, train_mse=1.284, val_mse=1.536]     21%|██        | 211/1000 [00:11<00:41, 19.17it/s, train_loss=1.525, val_loss=1.642, train_mse=1.237, val_mse=1.536]     21%|██        | 212/1000 [00:11<00:41, 19.14it/s, train_loss=1.525, val_loss=1.642, train_mse=1.237, val_mse=1.536]     21%|██        | 212/1000 [00:11<00:41, 19.14it/s, train_loss=1.525, val_loss=1.642, train_mse=1.236, val_mse=1.536]     21%|██▏       | 213/1000 [00:11<00:41, 19.14it/s, train_loss=1.527, val_loss=1.642, train_mse=1.242, val_mse=1.536]     21%|██▏       | 214/1000 [00:11<00:41, 18.75it/s, train_loss=1.527, val_loss=1.642, train_mse=1.242, val_mse=1.536]     21%|██▏       | 214/1000 [00:11<00:41, 18.75it/s, train_loss=1.530, val_loss=1.642, train_mse=1.250, val_mse=1.536]     22%|██▏       | 215/1000 [00:11<00:41, 18.75it/s, train_loss=1.517, val_loss=1.642, train_mse=1.217, val_mse=1.536]     22%|██▏       | 216/1000 [00:11<00:41, 18.97it/s, train_loss=1.517, val_loss=1.642, train_mse=1.217, val_mse=1.536]     22%|██▏       | 216/1000 [00:11<00:41, 18.97it/s, train_loss=1.521, val_loss=1.642, train_mse=1.229, val_mse=1.536]     22%|██▏       | 217/1000 [00:11<00:41, 18.97it/s, train_loss=1.531, val_loss=1.642, train_mse=1.252, val_mse=1.536]     22%|██▏       | 218/1000 [00:11<00:41, 18.99it/s, train_loss=1.531, val_loss=1.642, train_mse=1.252, val_mse=1.536]     22%|██▏       | 218/1000 [00:11<00:41, 18.99it/s, train_loss=1.532, val_loss=1.642, train_mse=1.255, val_mse=1.536]     22%|██▏       | 219/1000 [00:11<00:41, 18.99it/s, train_loss=1.528, val_loss=1.642, train_mse=1.246, val_mse=1.536]     22%|██▏       | 220/1000 [00:12<00:40, 19.09it/s, train_loss=1.528, val_loss=1.642, train_mse=1.246, val_mse=1.536]     22%|██▏       | 220/1000 [00:12<00:40, 19.09it/s, train_loss=1.520, val_loss=1.642, train_mse=1.227, val_mse=1.536]     22%|██▏       | 221/1000 [00:12<00:40, 19.09it/s, train_loss=1.526, val_loss=1.642, train_mse=1.240, val_mse=1.536]     22%|██▏       | 222/1000 [00:12<00:40, 19.00it/s, train_loss=1.526, val_loss=1.642, train_mse=1.240, val_mse=1.536]     22%|██▏       | 222/1000 [00:12<00:40, 19.00it/s, train_loss=1.526, val_loss=1.642, train_mse=1.238, val_mse=1.536]     22%|██▏       | 223/1000 [00:12<00:40, 19.00it/s, train_loss=1.523, val_loss=1.642, train_mse=1.234, val_mse=1.536]     22%|██▏       | 224/1000 [00:12<00:40, 19.12it/s, train_loss=1.523, val_loss=1.642, train_mse=1.234, val_mse=1.536]     22%|██▏       | 224/1000 [00:12<00:40, 19.12it/s, train_loss=1.523, val_loss=1.642, train_mse=1.232, val_mse=1.536]     22%|██▎       | 225/1000 [00:12<00:40, 19.12it/s, train_loss=1.527, val_loss=1.642, train_mse=1.243, val_mse=1.536]     23%|██▎       | 226/1000 [00:12<00:41, 18.84it/s, train_loss=1.527, val_loss=1.642, train_mse=1.243, val_mse=1.536]     23%|██▎       | 226/1000 [00:12<00:41, 18.84it/s, train_loss=1.525, val_loss=1.642, train_mse=1.239, val_mse=1.536]     23%|██▎       | 227/1000 [00:12<00:41, 18.84it/s, train_loss=1.525, val_loss=1.642, train_mse=1.237, val_mse=1.536]     23%|██▎       | 228/1000 [00:12<00:40, 19.09it/s, train_loss=1.525, val_loss=1.642, train_mse=1.237, val_mse=1.536]     23%|██▎       | 228/1000 [00:12<00:40, 19.09it/s, train_loss=1.518, val_loss=1.642, train_mse=1.219, val_mse=1.536]     23%|██▎       | 229/1000 [00:12<00:40, 19.09it/s, train_loss=1.535, val_loss=1.642, train_mse=1.263, val_mse=1.536]     23%|██▎       | 230/1000 [00:12<00:41, 18.70it/s, train_loss=1.535, val_loss=1.642, train_mse=1.263, val_mse=1.536]     23%|██▎       | 230/1000 [00:12<00:41, 18.70it/s, train_loss=1.525, val_loss=1.642, train_mse=1.239, val_mse=1.536]     23%|██▎       | 231/1000 [00:12<00:41, 18.70it/s, train_loss=1.523, val_loss=1.642, train_mse=1.232, val_mse=1.536]     23%|██▎       | 232/1000 [00:12<00:40, 18.98it/s, train_loss=1.523, val_loss=1.642, train_mse=1.232, val_mse=1.536]     23%|██▎       | 232/1000 [00:12<00:40, 18.98it/s, train_loss=1.517, val_loss=1.642, train_mse=1.219, val_mse=1.536]     23%|██▎       | 233/1000 [00:12<00:40, 18.98it/s, train_loss=1.532, val_loss=1.642, train_mse=1.258, val_mse=1.536]     23%|██▎       | 234/1000 [00:12<00:41, 18.25it/s, train_loss=1.532, val_loss=1.642, train_mse=1.258, val_mse=1.536]     23%|██▎       | 234/1000 [00:12<00:41, 18.25it/s, train_loss=1.524, val_loss=1.642, train_mse=1.235, val_mse=1.536]     24%|██▎       | 235/1000 [00:12<00:41, 18.25it/s, train_loss=1.521, val_loss=1.642, train_mse=1.227, val_mse=1.536]     24%|██▎       | 236/1000 [00:12<00:48, 15.82it/s, train_loss=1.521, val_loss=1.642, train_mse=1.227, val_mse=1.536]     24%|██▎       | 236/1000 [00:12<00:48, 15.82it/s, train_loss=1.526, val_loss=1.642, train_mse=1.241, val_mse=1.536]     24%|██▎       | 237/1000 [00:13<00:48, 15.82it/s, train_loss=1.530, val_loss=1.642, train_mse=1.250, val_mse=1.536]     24%|██▍       | 238/1000 [00:13<00:53, 14.29it/s, train_loss=1.530, val_loss=1.642, train_mse=1.250, val_mse=1.536]     24%|██▍       | 238/1000 [00:13<00:53, 14.29it/s, train_loss=1.533, val_loss=1.642, train_mse=1.257, val_mse=1.536]     24%|██▍       | 239/1000 [00:13<00:53, 14.29it/s, train_loss=1.530, val_loss=1.642, train_mse=1.250, val_mse=1.536]     24%|██▍       | 240/1000 [00:13<00:50, 15.06it/s, train_loss=1.530, val_loss=1.642, train_mse=1.250, val_mse=1.536]     24%|██▍       | 240/1000 [00:13<00:50, 15.06it/s, train_loss=1.532, val_loss=1.642, train_mse=1.256, val_mse=1.536]     24%|██▍       | 241/1000 [00:13<00:50, 15.06it/s, train_loss=1.530, val_loss=1.642, train_mse=1.249, val_mse=1.536]     24%|██▍       | 242/1000 [00:13<00:47, 15.91it/s, train_loss=1.530, val_loss=1.642, train_mse=1.249, val_mse=1.536]     24%|██▍       | 242/1000 [00:13<00:47, 15.91it/s, train_loss=1.528, val_loss=1.642, train_mse=1.245, val_mse=1.536]     24%|██▍       | 243/1000 [00:13<00:47, 15.91it/s, train_loss=1.538, val_loss=1.642, train_mse=1.274, val_mse=1.536]     24%|██▍       | 244/1000 [00:13<00:45, 16.59it/s, train_loss=1.538, val_loss=1.642, train_mse=1.274, val_mse=1.536]     24%|██▍       | 244/1000 [00:13<00:45, 16.59it/s, train_loss=1.518, val_loss=1.642, train_mse=1.221, val_mse=1.536]     24%|██▍       | 245/1000 [00:13<00:45, 16.59it/s, train_loss=1.523, val_loss=1.642, train_mse=1.234, val_mse=1.536]     25%|██▍       | 246/1000 [00:13<00:44, 17.04it/s, train_loss=1.523, val_loss=1.642, train_mse=1.234, val_mse=1.536]     25%|██▍       | 246/1000 [00:13<00:44, 17.04it/s, train_loss=1.519, val_loss=1.642, train_mse=1.221, val_mse=1.536]     25%|██▍       | 247/1000 [00:13<00:44, 17.04it/s, train_loss=1.530, val_loss=1.642, train_mse=1.251, val_mse=1.536]     25%|██▍       | 248/1000 [00:13<00:43, 17.38it/s, train_loss=1.530, val_loss=1.642, train_mse=1.251, val_mse=1.536]     25%|██▍       | 248/1000 [00:13<00:43, 17.38it/s, train_loss=1.536, val_loss=1.642, train_mse=1.264, val_mse=1.536]     25%|██▍       | 249/1000 [00:13<00:43, 17.38it/s, train_loss=1.531, val_loss=1.642, train_mse=1.253, val_mse=1.536]     25%|██▌       | 250/1000 [00:13<00:42, 17.84it/s, train_loss=1.531, val_loss=1.642, train_mse=1.253, val_mse=1.536]     25%|██▌       | 250/1000 [00:13<00:42, 17.84it/s, train_loss=1.555, val_loss=1.642, train_mse=1.314, val_mse=1.536]     25%|██▌       | 251/1000 [00:13<00:41, 17.84it/s, train_loss=1.531, val_loss=1.642, train_mse=1.253, val_mse=1.536]     25%|██▌       | 252/1000 [00:13<00:41, 18.24it/s, train_loss=1.531, val_loss=1.642, train_mse=1.253, val_mse=1.536]     25%|██▌       | 252/1000 [00:13<00:41, 18.24it/s, train_loss=1.520, val_loss=1.642, train_mse=1.226, val_mse=1.536]     25%|██▌       | 253/1000 [00:13<00:40, 18.24it/s, train_loss=1.530, val_loss=1.642, train_mse=1.251, val_mse=1.536]     25%|██▌       | 254/1000 [00:13<00:39, 18.71it/s, train_loss=1.530, val_loss=1.642, train_mse=1.251, val_mse=1.536]     25%|██▌       | 254/1000 [00:13<00:39, 18.71it/s, train_loss=1.531, val_loss=1.642, train_mse=1.254, val_mse=1.536]     26%|██▌       | 255/1000 [00:14<00:39, 18.71it/s, train_loss=1.537, val_loss=1.642, train_mse=1.270, val_mse=1.536]     26%|██▌       | 256/1000 [00:14<00:39, 18.72it/s, train_loss=1.537, val_loss=1.642, train_mse=1.270, val_mse=1.536]     26%|██▌       | 256/1000 [00:14<00:39, 18.72it/s, train_loss=1.524, val_loss=1.642, train_mse=1.234, val_mse=1.536]     26%|██▌       | 257/1000 [00:14<00:39, 18.72it/s, train_loss=1.526, val_loss=1.642, train_mse=1.240, val_mse=1.536]     26%|██▌       | 258/1000 [00:14<00:38, 19.04it/s, train_loss=1.526, val_loss=1.642, train_mse=1.240, val_mse=1.536]     26%|██▌       | 258/1000 [00:14<00:38, 19.04it/s, train_loss=1.525, val_loss=1.642, train_mse=1.238, val_mse=1.536]     26%|██▌       | 259/1000 [00:14<00:38, 19.04it/s, train_loss=1.530, val_loss=1.642, train_mse=1.252, val_mse=1.536]     26%|██▌       | 260/1000 [00:14<00:38, 19.04it/s, train_loss=1.541, val_loss=1.642, train_mse=1.280, val_mse=1.536]     26%|██▌       | 261/1000 [00:14<00:38, 19.35it/s, train_loss=1.541, val_loss=1.642, train_mse=1.280, val_mse=1.536]     26%|██▌       | 261/1000 [00:14<00:38, 19.35it/s, train_loss=1.541, val_loss=1.642, train_mse=1.277, val_mse=1.536]     26%|██▌       | 262/1000 [00:14<00:38, 19.35it/s, train_loss=1.523, val_loss=1.642, train_mse=1.232, val_mse=1.536]     26%|██▋       | 263/1000 [00:14<00:39, 18.84it/s, train_loss=1.523, val_loss=1.642, train_mse=1.232, val_mse=1.536]     26%|██▋       | 263/1000 [00:14<00:39, 18.84it/s, train_loss=1.539, val_loss=1.642, train_mse=1.273, val_mse=1.536]     26%|██▋       | 264/1000 [00:14<00:39, 18.84it/s, train_loss=1.542, val_loss=1.642, train_mse=1.280, val_mse=1.536]     26%|██▋       | 265/1000 [00:14<00:38, 19.02it/s, train_loss=1.542, val_loss=1.642, train_mse=1.280, val_mse=1.536]     26%|██▋       | 265/1000 [00:14<00:38, 19.02it/s, train_loss=1.534, val_loss=1.642, train_mse=1.260, val_mse=1.536]     27%|██▋       | 266/1000 [00:14<00:38, 19.02it/s, train_loss=1.518, val_loss=1.642, train_mse=1.220, val_mse=1.536]     27%|██▋       | 266/1000 [00:14<00:40, 18.22it/s, train_loss=1.518, val_loss=1.642, train_mse=1.220, val_mse=1.536]
>>>>>>> master




.. GENERATED FROM PYTHON SOURCE LINES 570-576

Then, we look at the learning curves of our baseline model returned by the evaluation function.

These curves display a good learning behaviour:

- the training and validation curves follow each other closely and are decreasing.
- a clear convergence plateau is reached at the end of the training.

.. GENERATED FROM PYTHON SOURCE LINES 576-599

.. dropdown:: Code (Make learning curves plot)

    .. code-block:: Python


        _ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))

        x_values = np.arange(1, len(baseline_results["metadata"]["train_loss"]) + 1)
        _ = plt.plot(
            x_values,
            baseline_results["metadata"]["train_loss"],
            label="Training",
        )
        _ = plt.plot(
            x_values,
            baseline_results["metadata"]["val_loss"],
            label="Validation",
        )

        _ = plt.xlim(x_values.min(), x_values.max())
        _ = plt.grid(which="both", linestyle=":")
        _ = plt.legend()
        _ = plt.xlabel("Epochs")
        _ = plt.ylabel("NLL")





.. image-sg:: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_002.png
   :alt: plot nas deep ensemble uq regression pytorch
   :srcset: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 600-603

In addition, we look at the predictions by reloading the checkpointed weights.

We first need to recreate the torch module and then we update its state using the checkpointed weights.

.. GENERATED FROM PYTHON SOURCE LINES 603-614

.. code-block:: Python


    weights_path = os.path.join(baseline_dir, "models",  "model_0.0.pt")
    parameters = problem.default_configuration
    torch_module = create_model(parameters, y_mu, y_std)
    torch_module.load_state_dict(torch.load(weights_path, weights_only=True))
    torch_module.eval()

    y_pred = torch_module.forward(to_torch(test_X))
    y_pred_mean = to_numpy(y_pred.loc)
    y_pred_std = to_numpy(y_pred.scale)








.. GENERATED FROM PYTHON SOURCE LINES 615-641

.. dropdown:: Code (Make prediction plot)

    .. code-block:: Python


        _ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))
        _ = plt.scatter(train_X, train_y, s=5, label="Training")
        _ = plt.scatter(valid_X, valid_y, s=5, label="Validation")
        _ = plt.plot(test_X, test_y, linestyle="--", color="gray", label="Test")

        _ = plt.plot(test_X, y_pred_mean, label=r"$\mu(x)$")
        kappa = 1.96
        _ = plt.fill_between(
            test_X.reshape(-1),
            (y_pred_mean - kappa * y_pred_std).reshape(-1),
            (y_pred_mean + kappa * y_pred_std).reshape(-1),
            alpha=0.25,
            label=r"$\sigma_\text{al}(x)$",
        )

        _ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.15)
        _ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.15)
        _ = plt.xlim(-x_lim, x_lim)
        _ = plt.ylim(-y_lim, y_lim)
        _ = plt.legend(ncols=2)
        _ = plt.xlabel(r"$x$")
        _ = plt.ylabel(r"$f(x)$")
        _ = plt.grid(which="both", linestyle=":")




.. image-sg:: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_003.png
   :alt: plot nas deep ensemble uq regression pytorch
   :srcset: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 642-729

Neural architecture search
--------------------------

We will now use Bayesian opimization to perform neural architecture search. 
The sequential Bayesian optimization algorithm can be described by the following pseudo-code:

Sequential Bayesian optimization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Algorithm**: Bayesian Optimization (a.k.a., Efficient Global Optimization (EGO))

  Inputs
    :math:`\texttt{thetaSpace}`: a hyperparameter space

    :math:`\texttt{nInitial}`: the number of initial hyperparameter configurations

    :math:`\texttt{f}`: a function that returns the objective of the learning workflow

  Outputs
    :math:`\texttt{thetaStar}` the recommended hyperparameter configuration

  :math:`\texttt{thetaArray}, \texttt{objArray} \gets` New empty arrays of hyperparameter configurations and objectives
  :math:`\texttt{model} \gets` New surrogate model

  Loop until stopping criteria is not valid

    If Length of :math:`\texttt{thetaArray} < \texttt{nInitial}` then

      :math:`\texttt{theta} \gets` Sample hyperparameter configuration from :math:`\texttt{thetaSpace}`

    Else

      Update :math:`\texttt{model}` with :math:`\texttt{thetaArray}, \texttt{objArray}`

      :math:`\texttt{theta} \gets` Returns :math:`\texttt{theta}` in :math:`\texttt{thetaSpace}` that maximizes 
      the acquisition function for the current :math:`\texttt{model}`

    :math:`\texttt{obj} \gets` Returns the objective of learning workflow :math:`\texttt{f}(\texttt{theta})`

    :math:`\texttt{thetaArray}  \gets` Concatenate :math:`\texttt{thetaArray}` with :math:`[\texttt{theta}]`

    :math:`\texttt{objArray}  \gets` Concatenate :math:`\texttt{objArray}` with :math:`[\texttt{obj}]`

    :math:`\texttt{thetaStar} \gets` Update recommendation

Parallel Bayesian optimization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In DeepHyper, instead of just performing sequential Bayesian optimization we provide asynchronous parallelisation for
Bayesian optimization (and other methods). This allows to execute multiple evaluation function in parallel to collect observations of objectives
faster.

In this example, we will focus on using centralized Bayesian optimization (CBO). In this setting, we have one main process that runs the
Bayesian optimization algorithm and we have multiple worker processes that run evaluation functions. The class we use for this is
:class:`deephyper.hpo.CBO`.

Let us start by explaining import configuration parameters of :class:`deephyper.hpo.CBO`:

- ``initial_points``: is a list of initial hyperparameter configurations to test, we add the baseline hyperparameters as we want to be at least better than this configuration.
- ``surrogate_model_*``: are parameters related to the surrogate model we use, here ``"ET"`` is an alias for the Extremely Randomized Trees regression model.
- ``multi_point_strategy``: is the strategy we use for parallel suggestion of hyperparameters, here we use the `qUCBd` that will sample for each new parallel configuration a different :math:`\kappa^j_i` value from an exponential with mean :math:`\kappa_i` where :math:`j` is the index in the current generated parallel batch and :math:`i` is the iteration of the Bayesian optimization loop. ``UCB`` corresponds to the Upper Confidence Bound acquisition function:

.. math::

    \alpha_\text{UCB}(\theta;\kappa) = \mu_\text{ET}(\theta) + \kappa \cdot \sigma_\text{ET}(\theta)

where :math:`\mu_\text{ET}(\theta)` and :math:`\sigma_\text{ET}^2(\theta)` are respectively estimators of :math:`E_C[C|\Theta=\theta]` and :math:`V_C[C|\Theta=\theta]` with :math:`C` the random variable describing the objective (or cost) and :math:`\Theta` the random variable describing the hyperparameters alone.

Finally the ``"d"`` postfix in ``"qUCBd"`` means that we will only consider the epistemic component of the uncertainty returned by the surrogate model.
Thanks to the law of total variance we have the following decomposition:

.. math::

    V_C[C|\Theta=\theta] = E_\text{tree}\left[V_C[C|\Theta=\theta;\text{tree}\right] + V_\text{tree}\left[E_C[C|\Theta=\theta;\text{tree}]\right]

Then, we define :math:`\sigma_{\text{ET},\text{ep}}(\theta)` as the empirical estimate of :math:`V_\text{tree}\left[E_C[C|\Theta=\theta;\text{tree}]\right]`.
Then, we define :math:`\alpha_\text{qUCBd}(\theta;\kappa^j_i)` as:

.. math::

    \alpha_\text{qUCBd}(\theta;\kappa^j_i) = \mu_\text{ET}(\theta) + \kappa^j_i \cdot \sigma_{\text{ET},\text{ep}}(\theta)

Interestingly the same trick will be used later to decompose the uncertainty of the deep ensemble.

- ``acq_optimizer_*```: are parameters related to optimization of the previously defined acquisition function.
- ``kappa`` and ``scheduler``: are the parameters that define the schedule of :math:`\kappa^j_i` previously mentionned.
- ``objective_scaler``: is a parameter that can be used to rescale the observed objectives (e.g., identity, min-max, log).

.. GENERATED FROM PYTHON SOURCE LINES 729-750

.. code-block:: Python

    search_kwargs = {
        "initial_points": [problem.default_configuration],
        "n_initial_points": 2 * len(problem) + 1,  # Number of initial random points
        "surrogate_model": "ET",  # Use Extra Trees as surrogate model
        "surrogate_model_kwargs": {
            "n_estimators": 50,  # Relatively small number of trees in the surrogate to make it "fast"
            "min_samples_split": 8,  # Larger number to avoid small leaf nodes (smoothing the objective response)
        },
        "multi_point_strategy": "qUCBd",  # Multi-point strategy for asynchronous batch generations (explained later)
        "acq_optimizer": "mixedga",  # Use continuous Genetic Algorithm for the acquisition function optimizer
        "acq_optimizer_freq": 1,  # Frequency of the acquisition function optimizer (1 = each new batch generation) increasing this value can help amortize the computational cost of acquisition function optimization
        "filter_duplicated": False,  # Deactivate filtration of duplicated new points
        "kappa": 10.0,  # Initial value of exploration-exploitation parameter for the acquisition function
        "scheduler": {  # Scheduler for the exploration-exploitation parameter "kappa"
            "type": "periodic-exp-decay",  # Periodic exponential decay
            "period": 50,  # Period over which the decay is applied. It is useful to escape local solutions.
            "kappa_final": 0.001,  # Value of kappa at the end of each "period"
        },
        "objective_scaler": "identity",
        "random_state": 42,  # Random seed
    }







.. GENERATED FROM PYTHON SOURCE LINES 751-763

Then, we create the search instance.

For this we pass the hyperparameter ``problem``, the ``evaluator`` and also a ``stopper`` (optional).

The ``problem`` is the instance of :class:`deephyper.hpo.HpProblem` that we defined in previous sections.

The ``evaluator`` is a subclass of :class:`deephyper.evaluator.Evaluator` that provides a ``.submit(...)`` method and a ``.gather(...)`` method to
submit and gather asynchronous evaluations.

The ``stopper`` is an optional parameter that allows to use an early-discarding (a.k.a., multi-fidelity) strategy to stop early low performing evaluations.
In our case we will use the median early-discarding strategy. 
This strategy consists in early stopping the training if the observed objective at the current budget is worse than the median objective for the same budget.

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 763-812
=======
.. GENERATED FROM PYTHON SOURCE LINES 763-816
>>>>>>> master

.. code-block:: Python

    from deephyper.evaluator import Evaluator
    from deephyper.evaluator.callback import TqdmCallback
    from deephyper.hpo import CBO
    from deephyper.stopper import MedianStopper


    hpo_dir = "nas_regression"


    def run_neural_architecture_search(problem, max_evals):
        model_checkpoint_dir = os.path.join(hpo_dir, "models")
        pathlib.Path(model_checkpoint_dir).mkdir(parents=True, exist_ok=True)

        method_kwargs = {
            "run_function_kwargs": {
                "model_checkpoint_dir": model_checkpoint_dir,
                "verbose": False,
            },
            "callbacks": [TqdmCallback()],
        }

        if device == "cuda":
            method_kwargs.update({
                "num_cpus": device_count,
                "num_gpus": device_count,
                "num_cpus_per_task": 1,
                "num_gpus_per_task": 1,
            })
        else:
            method_kwargs.update({
                "num_cpus": device_count,
                "num_cpus_per_task": 1,
            })
    

        evaluator = Evaluator.create(
            run,
            method="ray",  
            method_kwargs=method_kwargs,
        )

<<<<<<< HEAD
        stopper= MedianStopper(min_steps=50, max_steps=max_n_epochs, interval_steps=50)
=======
        stopper = None
    
        # Uncomment the following to speed-up the search
        # stopper = MedianStopper(min_steps=50, max_steps=max_n_epochs, interval_steps=50)

>>>>>>> master
        search = CBO(problem, evaluator, log_dir=hpo_dir, stopper=stopper, **search_kwargs)

        results = search.search(max_evals=max_evals)

        return results









<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 813-821

Preload cached results if you want to skip the slow neural architecture search step by running the following commands:
=======
.. GENERATED FROM PYTHON SOURCE LINES 817-830

You can download precomputed results if you want to skip the slow neural architecture search. We provide the following two set of precomputed results:

- Link to precomputed results without stopper: ``https://drive.google.com/uc?id=1VOV-UM0ws0lopHvoYT_9RAiRdT1y4Kus``
- Link to precomputed results with median stopper: ``https://drive.google.com/uc?id=1VOV-UM0ws0lopHvoYT_9RAiRdT1y4Kus``

Then run the following commands and adapt the url:
>>>>>>> master

.. code-block:: bash

    %%bash
    pip install gdown  # Install if necessary
    gdown "https://drive.google.com/uc?id=1VOV-UM0ws0lopHvoYT_9RAiRdT1y4Kus"
    tar -xvf nas_regression.tar.gz

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 823-829
=======
.. GENERATED FROM PYTHON SOURCE LINES 832-838
>>>>>>> master

If you want to remove previously computed results run the following command:

.. code-block:: bash

    %%bash
    rm -rf nas_regression/

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 831-833
=======
.. GENERATED FROM PYTHON SOURCE LINES 840-842
>>>>>>> master

As the search can take some time to finalize we provide a mechanism that checks if results were already computed and skip 
the neural architecture search if it is the case.

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 833-845
=======
.. GENERATED FROM PYTHON SOURCE LINES 842-854
>>>>>>> master

.. code-block:: Python

    max_evals = 250

    hpo_results = None
    hpo_results_path = os.path.join(hpo_dir, "results.csv")
    if os.path.exists(hpo_results_path):
        print("Reloading results...")
        hpo_results = pd.read_csv(hpo_results_path)

    if hpo_results is None or len(hpo_results) < max_evals:
        print("Running neural architecture search...")
        hpo_results = run_neural_architecture_search(problem, max_evals)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Reloading results...




<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 846-850
=======
.. GENERATED FROM PYTHON SOURCE LINES 855-859
>>>>>>> master

Analysis of the results
-----------------------

We will now look at the results of the search globally in term of evolution of the objective and worker's activity.

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 850-878
=======
.. GENERATED FROM PYTHON SOURCE LINES 859-887
>>>>>>> master

.. code-block:: Python


    from deephyper.analysis.hpo import plot_search_trajectory_single_objective_hpo
    from deephyper.analysis.hpo import plot_worker_utilization


    fig, axes = plt.subplots(
        nrows=2,
        ncols=1,
        sharex=True,
        figsize=(WIDTH_PLOTS, HEIGHT_PLOTS),
    )

    _ = plot_search_trajectory_single_objective_hpo(
        hpo_results,
        mode="min",
        x_units="seconds",
        ax=axes[0],
    )
    axes[0].set_yscale("log")

    _ = plot_worker_utilization(
        hpo_results,
        profile_type="submit/gather",
        ax=axes[1],
    )

    plt.tight_layout()




.. image-sg:: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_004.png
   :alt: plot nas deep ensemble uq regression pytorch
   :srcset: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_004.png
   :class: sphx-glr-single-img





<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 879-880

Then, we split results between successful and failed results if there are some.

.. GENERATED FROM PYTHON SOURCE LINES 880-888
=======
.. GENERATED FROM PYTHON SOURCE LINES 888-889

Then, we split results between successful and failed results if there are some.

.. GENERATED FROM PYTHON SOURCE LINES 889-897
>>>>>>> master

.. code-block:: Python

    from deephyper.analysis.hpo import filter_failed_objectives


    hpo_results, hpo_results_failed = filter_failed_objectives(hpo_results)

    hpo_results







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>p:batch_size</th>
          <th>p:layer_0_activation</th>
          <th>p:layer_0_dropout_rate</th>
          <th>p:layer_0_units</th>
          <th>p:layer_1_activation</th>
          <th>p:layer_1_dropout_rate</th>
          <th>p:layer_1_units</th>
          <th>p:layer_2_activation</th>
          <th>p:layer_2_dropout_rate</th>
          <th>p:layer_2_units</th>
          <th>p:learning_rate</th>
          <th>p:lr_scheduler_factor</th>
          <th>p:lr_scheduler_patience</th>
          <th>p:n_units_mean</th>
          <th>p:n_units_std</th>
          <th>p:num_layers</th>
          <th>p:softplus_factor</th>
          <th>p:std_offset</th>
          <th>p:layer_3_activation</th>
          <th>p:layer_3_dropout_rate</th>
          <th>p:layer_3_units</th>
          <th>p:layer_4_activation</th>
          <th>p:layer_4_dropout_rate</th>
          <th>p:layer_4_units</th>
          <th>p:layer_5_activation</th>
          <th>p:layer_5_dropout_rate</th>
          <th>p:layer_5_units</th>
          <th>p:layer_6_activation</th>
          <th>p:layer_6_dropout_rate</th>
          <th>p:layer_6_units</th>
          <th>p:layer_7_activation</th>
          <th>p:layer_7_dropout_rate</th>
          <th>p:layer_7_units</th>
          <th>objective</th>
          <th>job_id</th>
          <th>job_status</th>
          <th>m:timestamp_submit</th>
          <th>m:train_loss</th>
          <th>m:val_loss</th>
          <th>m:train_mse</th>
          <th>m:val_mse</th>
          <th>m:test_loss</th>
          <th>m:test_mse</th>
          <th>m:budget</th>
          <th>m:timestamp_gather</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
<<<<<<< HEAD
          <td>70</td>
          <td>sigmoid</td>
          <td>0.181926</td>
          <td>474</td>
          <td>relu</td>
          <td>0.206947</td>
          <td>102</td>
          <td>gelu</td>
          <td>0.077514</td>
          <td>102</td>
          <td>0.025985</td>
          <td>0.865468</td>
          <td>44</td>
          <td>34</td>
          <td>72</td>
          <td>6</td>
          <td>0.295450</td>
          <td>3.219338e-06</td>
          <td>tanh</td>
          <td>0.013806</td>
          <td>31</td>
          <td>sigmoid</td>
          <td>0.084893</td>
          <td>133</td>
          <td>swish</td>
          <td>0.041425</td>
          <td>342</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>-2.150262</td>
          <td>2</td>
          <td>DONE</td>
          <td>3.115092</td>
          <td>[3.3446648, 3.007759, 2.1224763, 2.1232972, 2....</td>
          <td>[3.658542, 2.7046585, 2.2429872, 2.2186995, 2....</td>
          <td>[13.944258, 5.2797594, 3.7825398, 3.7604504, 3...</td>
          <td>[14.685605, 5.4939284, 4.2901144, 4.2900653, 4...</td>
          <td>1.914078</td>
          <td>2.502486</td>
          <td>207</td>
          <td>11.251570</td>
        </tr>
        <tr>
          <th>1</th>
          <td>97</td>
          <td>mish</td>
          <td>0.092652</td>
          <td>353</td>
          <td>mish</td>
          <td>0.160574</td>
          <td>209</td>
          <td>gelu</td>
          <td>0.187758</td>
          <td>473</td>
          <td>0.011654</td>
          <td>0.285402</td>
          <td>67</td>
          <td>138</td>
          <td>352</td>
          <td>4</td>
          <td>0.587813</td>
          <td>4.738140e-08</td>
          <td>swish</td>
          <td>0.231107</td>
          <td>320</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>-11.422319</td>
          <td>8</td>
          <td>DONE</td>
          <td>3.124672</td>
          <td>[3.8383796e+17, 4.4299097, 4.9976664, 4.943719...</td>
          <td>[3.221744e+17, 4.5298862, 4.7057, 4.00931, 4.0...</td>
          <td>[1923.6177, 198.72899, 747.47736, 1066.5563, 6...</td>
          <td>[1655.038, 337.08624, 288.66772, 61.806, 178.6...</td>
          <td>11.205925</td>
          <td>596.508360</td>
          <td>204</td>
          <td>14.438305</td>
        </tr>
        <tr>
          <th>2</th>
          <td>12</td>
          <td>sigmoid</td>
          <td>0.098485</td>
          <td>167</td>
          <td>tanh</td>
          <td>0.081428</td>
          <td>162</td>
          <td>relu</td>
          <td>0.185991</td>
          <td>422</td>
          <td>0.000010</td>
          <td>0.400599</td>
          <td>83</td>
          <td>236</td>
          <td>292</td>
          <td>3</td>
          <td>0.685736</td>
          <td>2.607425e-06</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>-1.628614</td>
          <td>7</td>
          <td>DONE</td>
          <td>3.123110</td>
          <td>[2.236774, 2.1814837, 2.1265445, 2.0818307, 2....</td>
          <td>[2.3746197, 2.30508, 2.2423122, 2.1839232, 2.1...</td>
          <td>[3.71194, 3.5859272, 3.432112, 3.299646, 3.131...</td>
          <td>[4.225223, 4.0678625, 3.90591, 3.7386181, 3.56...</td>
          <td>2.380924</td>
          <td>3.308409</td>
          <td>100</td>
          <td>16.841407</td>
        </tr>
        <tr>
          <th>3</th>
=======
>>>>>>> master
          <td>68</td>
          <td>silu</td>
          <td>0.122704</td>
          <td>105</td>
          <td>mish</td>
          <td>0.171507</td>
          <td>130</td>
          <td>silu</td>
          <td>0.025040</td>
          <td>258</td>
          <td>0.000051</td>
          <td>0.726719</td>
          <td>31</td>
          <td>29</td>
          <td>320</td>
          <td>5</td>
          <td>0.071061</td>
          <td>9.339449e-03</td>
          <td>tanh</td>
          <td>0.039217</td>
          <td>149</td>
          <td>gelu</td>
          <td>0.192898</td>
          <td>228</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
<<<<<<< HEAD
          <td>0.0</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>-1.623544</td>
          <td>5</td>
          <td>DONE</td>
          <td>3.119682</td>
          <td>[2.210437, 2.1761377, 2.1406722, 2.1041787, 2....</td>
          <td>[2.3381114, 2.291759, 2.2486758, 2.206605, 2.1...</td>
          <td>[3.673555, 3.5535786, 3.4296498, 3.298158, 3.1...</td>
          <td>[4.1579785, 3.9966674, 3.845235, 3.6954503, 3....</td>
          <td>2.103548</td>
          <td>2.312875</td>
          <td>335</td>
          <td>16.867606</td>
        </tr>
        <tr>
          <th>4</th>
          <td>32</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>512</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>512</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>512</td>
          <td>0.002000</td>
          <td>0.100000</td>
          <td>20</td>
          <td>512</td>
          <td>512</td>
          <td>5</td>
          <td>0.050000</td>
          <td>1.000000e-03</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>512</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>512</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>-1.635386</td>
          <td>0</td>
          <td>DONE</td>
          <td>3.111969</td>
          <td>[3.0886695, 1.750991, 1.6709614, 1.6958653, 1....</td>
          <td>[2.9877737, 1.8339363, 1.7619252, 1.7876191, 1...</td>
          <td>[9.036593, 1.8560325, 1.6338795, 1.754334, 1.7...</td>
          <td>[8.499712, 2.2645135, 1.9825038, 2.0629869, 1....</td>
          <td>2.309622</td>
          <td>3.235827</td>
          <td>100</td>
          <td>19.226117</td>
=======
          <td>0.00000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>-1.623808</td>
          <td>5</td>
          <td>DONE</td>
          <td>3.079311</td>
          <td>[2.2356727, 2.1990643, 2.1611142, 2.1306512, 2...</td>
          <td>[2.3715773, 2.321428, 2.2811942, 2.244391, 2.2...</td>
          <td>[3.729117, 3.6063304, 3.473826, 3.3686516, 3.2...</td>
          <td>[4.2356567, 4.062816, 3.9246833, 3.79775, 3.67...</td>
          <td>2.311146</td>
          <td>3.122602e+00</td>
          <td>247.0</td>
          <td>13.158018</td>
        </tr>
        <tr>
          <th>1</th>
          <td>70</td>
          <td>sigmoid</td>
          <td>0.181926</td>
          <td>474</td>
          <td>relu</td>
          <td>0.206947</td>
          <td>102</td>
          <td>gelu</td>
          <td>0.077514</td>
          <td>102</td>
          <td>0.025985</td>
          <td>0.865468</td>
          <td>44</td>
          <td>34</td>
          <td>72</td>
          <td>6</td>
          <td>0.295450</td>
          <td>3.219338e-06</td>
          <td>tanh</td>
          <td>0.013806</td>
          <td>31</td>
          <td>sigmoid</td>
          <td>0.084893</td>
          <td>133</td>
          <td>swish</td>
          <td>0.041425</td>
          <td>342</td>
          <td>relu</td>
          <td>0.00000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>-1.632331</td>
          <td>2</td>
          <td>DONE</td>
          <td>3.075146</td>
          <td>[6.5537024, 1.9579012, 1.665855, 1.6295078, 1....</td>
          <td>[6.8209295, 1.9405267, 1.7315395, 1.684643, 1....</td>
          <td>[13.900175, 2.9138274, 1.5408843, 1.4303554, 1...</td>
          <td>[14.717955, 2.8753903, 1.622345, 1.6306633, 1....</td>
          <td>2.433584</td>
          <td>3.431710e+00</td>
          <td>288.0</td>
          <td>15.570797</td>
        </tr>
        <tr>
          <th>2</th>
          <td>97</td>
          <td>mish</td>
          <td>0.092652</td>
          <td>353</td>
          <td>mish</td>
          <td>0.160574</td>
          <td>209</td>
          <td>gelu</td>
          <td>0.187758</td>
          <td>473</td>
          <td>0.011654</td>
          <td>0.285402</td>
          <td>67</td>
          <td>138</td>
          <td>352</td>
          <td>4</td>
          <td>0.587813</td>
          <td>4.738140e-08</td>
          <td>swish</td>
          <td>0.231107</td>
          <td>320</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.00000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>-10.482303</td>
          <td>8</td>
          <td>DONE</td>
          <td>3.083357</td>
          <td>[4936380000000000.0, 5.681423, 6.664331, 45.25...</td>
          <td>[1811503100000000.0, 6.081249, 6.5736256, 6.43...</td>
          <td>[76.39607, 1464.7188, 3686.473, 2281.423, 3065...</td>
          <td>[162.81638, 1842.9956, 2423.849, 409.41083, 35...</td>
          <td>10.246502</td>
          <td>1.667279e+09</td>
          <td>202.0</td>
          <td>15.596288</td>
        </tr>
        <tr>
          <th>3</th>
          <td>119</td>
          <td>tanh</td>
          <td>0.001329</td>
          <td>69</td>
          <td>swish</td>
          <td>0.184957</td>
          <td>295</td>
          <td>relu</td>
          <td>0.176468</td>
          <td>307</td>
          <td>0.000262</td>
          <td>0.205413</td>
          <td>91</td>
          <td>225</td>
          <td>133</td>
          <td>3</td>
          <td>0.745321</td>
          <td>5.758780e-08</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.00000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>-1.137323</td>
          <td>1</td>
          <td>DONE</td>
          <td>3.073541</td>
          <td>[2.2547195, 2.105179, 1.9712442, 1.85241, 1.76...</td>
          <td>[2.3378735, 2.1741226, 2.0298746, 1.9063791, 1...</td>
          <td>[3.66675, 3.2829812, 2.8488493, 2.375151, 1.89...</td>
          <td>[4.025764, 3.5948925, 3.1349583, 2.635012, 2.1...</td>
          <td>5.430150</td>
          <td>2.168292e+00</td>
          <td>1000.0</td>
          <td>21.298497</td>
        </tr>
        <tr>
          <th>4</th>
          <td>212</td>
          <td>swish</td>
          <td>0.080304</td>
          <td>108</td>
          <td>swish</td>
          <td>0.133348</td>
          <td>180</td>
          <td>swish</td>
          <td>0.214755</td>
          <td>499</td>
          <td>0.000073</td>
          <td>0.744227</td>
          <td>60</td>
          <td>184</td>
          <td>462</td>
          <td>8</td>
          <td>0.236448</td>
          <td>1.947248e-05</td>
          <td>mish</td>
          <td>0.094552</td>
          <td>398</td>
          <td>relu</td>
          <td>0.020214</td>
          <td>69</td>
          <td>silu</td>
          <td>0.165548</td>
          <td>130</td>
          <td>mish</td>
          <td>0.24868</td>
          <td>403</td>
          <td>gelu</td>
          <td>0.175789</td>
          <td>333</td>
          <td>-2.191429</td>
          <td>10</td>
          <td>DONE</td>
          <td>15.566050</td>
          <td>[2.2363968, 2.2094727, 2.1878111, 2.2310803, 2...</td>
          <td>[2.36148, 2.3554432, 2.348463, 2.3400981, 2.32...</td>
          <td>[3.7908025, 3.6970637, 3.6223884, 3.7993572, 3...</td>
          <td>[4.2729263, 4.2616057, 4.2462792, 4.2252903, 4...</td>
          <td>3.207318</td>
          <td>2.229017e+00</td>
          <td>340.0</td>
          <td>29.968976</td>
>>>>>>> master
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
<<<<<<< HEAD
          <th>253</th>
          <td>9</td>
          <td>gelu</td>
          <td>0.147503</td>
          <td>327</td>
          <td>gelu</td>
          <td>0.040974</td>
          <td>382</td>
          <td>relu</td>
          <td>0.111438</td>
          <td>350</td>
          <td>0.000201</td>
          <td>0.646348</td>
          <td>75</td>
          <td>407</td>
          <td>223</td>
          <td>6</td>
          <td>0.355029</td>
          <td>2.807461e-04</td>
          <td>swish</td>
          <td>0.008805</td>
          <td>436</td>
          <td>relu</td>
          <td>0.052158</td>
          <td>168</td>
          <td>swish</td>
          <td>0.048483</td>
          <td>228</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>-1.355191</td>
          <td>246</td>
          <td>DONE</td>
          <td>630.366095</td>
          <td>[1.7959713, 1.612981, 1.5384753, 1.5049683, 1....</td>
          <td>[1.8916413, 1.6986941, 1.633342, 1.585164, 1.5...</td>
          <td>[2.1121533, 1.4583974, 1.2615055, 1.1661158, 1...</td>
          <td>[2.4692147, 1.7586042, 1.5339593, 1.414326, 1....</td>
          <td>5.490296</td>
          <td>2.254771</td>
          <td>150</td>
          <td>679.582077</td>
        </tr>
        <tr>
          <th>254</th>
          <td>9</td>
          <td>gelu</td>
          <td>0.147503</td>
          <td>327</td>
          <td>gelu</td>
          <td>0.040974</td>
          <td>382</td>
          <td>relu</td>
          <td>0.111438</td>
          <td>350</td>
          <td>0.000201</td>
          <td>0.646348</td>
          <td>75</td>
          <td>407</td>
          <td>223</td>
          <td>6</td>
          <td>0.355029</td>
          <td>2.807461e-04</td>
          <td>swish</td>
          <td>0.008805</td>
          <td>436</td>
          <td>relu</td>
          <td>0.052158</td>
          <td>168</td>
          <td>swish</td>
          <td>0.048483</td>
          <td>228</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>-1.338869</td>
          <td>245</td>
          <td>DONE</td>
          <td>630.364619</td>
          <td>[1.7868594, 1.6394458, 1.5969827, 1.5058933, 1...</td>
          <td>[1.881283, 1.6959612, 1.6907848, 1.5996006, 1....</td>
          <td>[2.0721033, 1.5127479, 1.4592592, 1.1749825, 1...</td>
          <td>[2.4467967, 1.7090496, 1.7507932, 1.4495625, 1...</td>
          <td>4.865471</td>
          <td>2.652589</td>
          <td>150</td>
          <td>679.591466</td>
        </tr>
        <tr>
          <th>255</th>
          <td>39</td>
          <td>sigmoid</td>
          <td>0.023327</td>
          <td>492</td>
          <td>gelu</td>
          <td>0.046661</td>
          <td>508</td>
          <td>tanh</td>
          <td>0.054499</td>
          <td>408</td>
          <td>0.000429</td>
          <td>0.456818</td>
          <td>99</td>
          <td>305</td>
          <td>375</td>
          <td>6</td>
          <td>0.083601</td>
          <td>6.648459e-04</td>
          <td>relu</td>
          <td>0.001359</td>
          <td>131</td>
          <td>relu</td>
          <td>0.000434</td>
          <td>91</td>
          <td>mish</td>
          <td>0.006195</td>
          <td>278</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>-1.237021</td>
          <td>254</td>
          <td>DONE</td>
          <td>663.889429</td>
          <td>[2.1483836, 1.6763021, 1.6019645, 1.5603795, 1...</td>
          <td>[2.2329588, 1.7383378, 1.6627777, 1.6332842, 1...</td>
          <td>[3.4518583, 1.6064619, 1.3468125, 1.269646, 1....</td>
          <td>[3.7938585, 1.8709488, 1.5942268, 1.5283567, 1...</td>
          <td>3.218310</td>
          <td>3.013878</td>
          <td>200</td>
          <td>679.601624</td>
        </tr>
        <tr>
          <th>256</th>
          <td>9</td>
          <td>gelu</td>
          <td>0.147503</td>
          <td>327</td>
          <td>gelu</td>
          <td>0.040974</td>
          <td>382</td>
          <td>relu</td>
          <td>0.111438</td>
          <td>350</td>
          <td>0.000201</td>
          <td>0.646348</td>
          <td>75</td>
          <td>407</td>
          <td>223</td>
          <td>6</td>
          <td>0.355029</td>
          <td>2.807461e-04</td>
          <td>swish</td>
          <td>0.008805</td>
          <td>436</td>
          <td>relu</td>
          <td>0.052158</td>
          <td>168</td>
          <td>swish</td>
          <td>0.048483</td>
          <td>228</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>-1.329060</td>
          <td>252</td>
          <td>DONE</td>
          <td>630.374816</td>
          <td>[1.87602, 1.6645529, 1.5612121, 1.4900389, 1.4...</td>
          <td>[1.939153, 1.7124308, 1.6570098, 1.6031138, 1....</td>
          <td>[2.3585367, 1.6309453, 1.300604, 1.1493164, 1....</td>
          <td>[2.6316204, 1.8015269, 1.6143771, 1.4616898, 1...</td>
          <td>10.104279</td>
          <td>3.058449</td>
          <td>150</td>
          <td>679.611899</td>
        </tr>
        <tr>
          <th>257</th>
          <td>40</td>
          <td>swish</td>
          <td>0.021190</td>
          <td>508</td>
          <td>gelu</td>
          <td>0.018175</td>
          <td>251</td>
          <td>tanh</td>
          <td>0.132126</td>
          <td>140</td>
          <td>0.000244</td>
          <td>0.541508</td>
          <td>97</td>
          <td>195</td>
          <td>178</td>
          <td>7</td>
          <td>0.051539</td>
          <td>5.631186e-04</td>
          <td>relu</td>
          <td>0.001663</td>
          <td>263</td>
          <td>relu</td>
          <td>0.001925</td>
          <td>16</td>
          <td>relu</td>
          <td>0.034920</td>
          <td>377</td>
          <td>sigmoid</td>
          <td>0.0</td>
          <td>16</td>
          <td>relu</td>
          <td>0.0</td>
          <td>16</td>
          <td>-1.630955</td>
          <td>257</td>
          <td>DONE</td>
          <td>679.533651</td>
          <td>[2.2289357, 2.1660335, 2.1070788, 1.9747405, 1...</td>
          <td>[2.368112, 2.2929592, 2.195803, 2.0754118, 1.9...</td>
          <td>[3.6846101, 3.5444841, 3.3924818, 2.8852522, 2...</td>
          <td>[4.2091117, 4.049022, 3.7570698, 3.316684, 2.8...</td>
          <td>2.380392</td>
          <td>3.325646</td>
          <td>50</td>
          <td>692.428325</td>
        </tr>
      </tbody>
    </table>
    <p>258 rows × 45 columns</p>
=======
          <th>248</th>
          <td>12</td>
          <td>tanh</td>
          <td>0.165450</td>
          <td>455</td>
          <td>relu</td>
          <td>0.017045</td>
          <td>352</td>
          <td>swish</td>
          <td>0.154026</td>
          <td>236</td>
          <td>0.000677</td>
          <td>0.900386</td>
          <td>75</td>
          <td>156</td>
          <td>97</td>
          <td>3</td>
          <td>0.654945</td>
          <td>2.088938e-07</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.00000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>-1.192449</td>
          <td>239</td>
          <td>DONE</td>
          <td>1165.378879</td>
          <td>[1.6727325, 1.5800053, 1.6157894, 1.5432746, 1...</td>
          <td>[1.7266064, 1.6549356, 1.7140119, 1.637512, 1....</td>
          <td>[1.6016694, 1.3569081, 1.4258856, 1.2578446, 1...</td>
          <td>[1.813346, 1.590621, 1.7395469, 1.5146513, 1.6...</td>
          <td>5.439753</td>
          <td>2.908574e+00</td>
          <td>760.0</td>
          <td>1245.544874</td>
        </tr>
        <tr>
          <th>249</th>
          <td>79</td>
          <td>tanh</td>
          <td>0.161465</td>
          <td>364</td>
          <td>silu</td>
          <td>0.056837</td>
          <td>350</td>
          <td>sigmoid</td>
          <td>0.192261</td>
          <td>415</td>
          <td>0.000176</td>
          <td>0.738764</td>
          <td>65</td>
          <td>510</td>
          <td>105</td>
          <td>3</td>
          <td>0.468523</td>
          <td>2.400839e-04</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.00000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>-1.203743</td>
          <td>250</td>
          <td>DONE</td>
          <td>1212.161439</td>
          <td>[2.122525, 1.9694458, 1.8289667, 1.7220303, 1....</td>
          <td>[2.1825612, 1.9890454, 1.8536297, 1.7302277, 1...</td>
          <td>[3.5389302, 3.000673, 2.0841455, 1.5233757, 1....</td>
          <td>[3.9003167, 3.1071992, 2.2470388, 1.6122707, 1...</td>
          <td>4.303766</td>
          <td>3.376671e+00</td>
          <td>815.0</td>
          <td>1245.564055</td>
        </tr>
        <tr>
          <th>250</th>
          <td>10</td>
          <td>tanh</td>
          <td>0.241329</td>
          <td>460</td>
          <td>tanh</td>
          <td>0.017031</td>
          <td>492</td>
          <td>silu</td>
          <td>0.212373</td>
          <td>431</td>
          <td>0.000131</td>
          <td>0.949106</td>
          <td>96</td>
          <td>453</td>
          <td>127</td>
          <td>4</td>
          <td>0.477518</td>
          <td>1.958822e-07</td>
          <td>relu</td>
          <td>0.116858</td>
          <td>126</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.00000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>-1.187596</td>
          <td>240</td>
          <td>DONE</td>
          <td>1165.380704</td>
          <td>[1.8427774, 1.5654259, 1.54907, 1.5718877, 1.5...</td>
          <td>[1.91599, 1.630422, 1.6431626, 1.6635764, 1.63...</td>
          <td>[2.2706072, 1.2972956, 1.2960027, 1.3628039, 1...</td>
          <td>[2.5700068, 1.5153091, 1.5366507, 1.5885208, 1...</td>
          <td>7.368615</td>
          <td>2.003667e+00</td>
          <td>989.0</td>
          <td>1313.699958</td>
        </tr>
        <tr>
          <th>251</th>
          <td>10</td>
          <td>tanh</td>
          <td>0.070972</td>
          <td>486</td>
          <td>silu</td>
          <td>0.023657</td>
          <td>278</td>
          <td>gelu</td>
          <td>0.046773</td>
          <td>225</td>
          <td>0.000455</td>
          <td>0.443645</td>
          <td>89</td>
          <td>78</td>
          <td>34</td>
          <td>3</td>
          <td>0.587185</td>
          <td>4.961249e-04</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.00000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>-1.159050</td>
          <td>252</td>
          <td>DONE</td>
          <td>1245.439769</td>
          <td>[1.6650637, 1.5602826, 1.5515006, 1.5409552, 1...</td>
          <td>[1.7371452, 1.649412, 1.6291426, 1.6426994, 1....</td>
          <td>[1.6652527, 1.2852505, 1.2749716, 1.2613335, 1...</td>
          <td>[1.898218, 1.5504943, 1.506757, 1.5249182, 1.4...</td>
          <td>1632.896000</td>
          <td>5.692229e+00</td>
          <td>849.0</td>
          <td>1313.717623</td>
        </tr>
        <tr>
          <th>252</th>
          <td>29</td>
          <td>tanh</td>
          <td>0.066118</td>
          <td>491</td>
          <td>silu</td>
          <td>0.022694</td>
          <td>310</td>
          <td>gelu</td>
          <td>0.141575</td>
          <td>376</td>
          <td>0.000453</td>
          <td>0.294295</td>
          <td>92</td>
          <td>95</td>
          <td>55</td>
          <td>3</td>
          <td>0.044767</td>
          <td>2.045635e-04</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.00000</td>
          <td>16</td>
          <td>relu</td>
          <td>0.000000</td>
          <td>16</td>
          <td>-1.122970</td>
          <td>251</td>
          <td>DONE</td>
          <td>1231.533599</td>
          <td>[1.7851361, 1.5758463, 1.5461717, 1.541927, 1....</td>
          <td>[1.8419315, 1.6515197, 1.6262579, 1.624602, 1....</td>
          <td>[2.0054357, 1.3500684, 1.2656512, 1.2646219, 1...</td>
          <td>[2.185182, 1.5942222, 1.5158734, 1.5109658, 1....</td>
          <td>5.322379</td>
          <td>3.880587e+00</td>
          <td>890.0</td>
          <td>1313.733935</td>
        </tr>
      </tbody>
    </table>
    <p>251 rows × 45 columns</p>
>>>>>>> master
    </div>
    </div>
    <br />
    <br />

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 889-890

We look at the learning curves of the best model and observe improvements in both training and validation loss:

.. GENERATED FROM PYTHON SOURCE LINES 890-934
=======
.. GENERATED FROM PYTHON SOURCE LINES 898-899

We look at the learning curves of the best model and observe improvements in both training and validation loss:

.. GENERATED FROM PYTHON SOURCE LINES 899-943
>>>>>>> master

.. code-block:: Python


    # .. dropdown: Make learning curves plot
    x_values = np.arange(1, len(baseline_results["metadata"]["train_loss"]) + 1)
    x_min, x_max = x_values.min(), x_values.max()
    _ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))
    _ = plt.plot(
        x_values,
        baseline_results["metadata"]["train_loss"],
        linestyle=":",
        label="Baseline Training",
    )
    _ = plt.plot(
        x_values,
        baseline_results["metadata"]["val_loss"],
        linestyle=":",
        label="Baseline Validation",
    )

    i_max = hpo_results["objective"].argmax()
    train_loss = json.loads(hpo_results.iloc[i_max]["m:train_loss"])
    val_loss = json.loads(hpo_results.iloc[i_max]["m:val_loss"])
    x_values = np.arange(1, len(train_loss) + 1)
    x_max = max(x_max, x_values.max())
    _ = plt.plot(
        x_values,
        train_loss,
        alpha=0.8,
        linestyle="--",
        label="Best Training",
    )
    _ = plt.plot(
        x_values,
        val_loss,
        alpha=0.8,
        linestyle="--",
        label="Best Validation",
    )
    _ = plt.xlim(x_min, x_max)
    _ = plt.grid(which="both", linestyle=":")
    _ = plt.legend()
    _ = plt.xlabel("Epochs")
    _ = plt.ylabel("NLL")





.. image-sg:: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_005.png
   :alt: plot nas deep ensemble uq regression pytorch
   :srcset: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_005.png
   :class: sphx-glr-single-img





<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 935-936

Finally, we look at predictions of this best model and observe that it manage to predict much better than the baseline one the right range. 

.. GENERATED FROM PYTHON SOURCE LINES 936-956
=======
.. GENERATED FROM PYTHON SOURCE LINES 944-945

Finally, we look at predictions of this best model and observe that it manage to predict much better than the baseline one the right range. 

.. GENERATED FROM PYTHON SOURCE LINES 945-965
>>>>>>> master

.. code-block:: Python

    from deephyper.analysis.hpo import parameters_from_row


    hpo_dir = "nas_regression"
    model_checkpoint_dir = os.path.join(hpo_dir, "models")
    job_id = hpo_results.iloc[i_max]["job_id"]
    file_name = f"model_0.{job_id}.pt"

    weights_path = os.path.join(model_checkpoint_dir, file_name)
    parameters = parameters_from_row(hpo_results.iloc[i_max])

    torch_module = create_model(parameters, y_mu, y_std)

    torch_module.load_state_dict(torch.load(weights_path, weights_only=True))
    torch_module.eval()

    y_pred = torch_module.forward(to_torch(test_X))
    y_pred_mean = to_numpy(y_pred.loc)
    y_pred_std = to_numpy(y_pred.scale)








<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 957-982
=======
.. GENERATED FROM PYTHON SOURCE LINES 966-991
>>>>>>> master

.. dropdown:: Code (Make prediction plot)

    .. code-block:: Python


        kappa = 1.96
        _ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))
        _ = plt.scatter(train_X, train_y, s=5, label="Training")
        _ = plt.scatter(valid_X, valid_y, s=5, label="Validation")
        _ = plt.plot(test_X, test_y, linestyle="--", color="gray", label="Test")

        _ = plt.plot(test_X, y_pred_mean, label=r"$\mu(x)$")
        _ = plt.fill_between(
            test_X.reshape(-1),
            (y_pred_mean - kappa * y_pred_std).reshape(-1),
            (y_pred_mean + kappa * y_pred_std).reshape(-1),
            alpha=0.25,
            label=r"$\sigma_\text{al}(x)$",
        )
        _ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.25)
        _ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.25)
        _ = plt.xlim(-x_lim, x_lim)
        _ = plt.ylim(-y_lim, y_lim)
        _ = plt.legend(ncols=2)
        _ = plt.xlabel(r"$x$")
        _ = plt.ylabel(r"$f(x)$")
        _ = plt.grid(which="both", linestyle=":")




.. image-sg:: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_006.png
   :alt: plot nas deep ensemble uq regression pytorch
   :srcset: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_006.png
   :class: sphx-glr-single-img





<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 983-1000
=======
.. GENERATED FROM PYTHON SOURCE LINES 992-1009
>>>>>>> master

Deep ensemble
-------------

After running the neural architecture search we have an available library of checkpointed models.
From this section, you will learn how to combine these models to form an ensemble that can improve both accuracy and provide disentangled uncertainty quantification.

We start by importing classes from :mod:`deephyper.predictor` and :mod:`deephyper.ensemble`.

The :mod:`deephyper.predictor` module includes subclasses of :class:`deephyper.predictor.Predictor` to wrap predictive models ready for inference. In our case, we will use :class:`deephyper.predictor.torch.TorchPredictor`.
The :mod:`deephyper.ensemble` module includes modular components to build an ensemble of predictive models.
The ensemble module is organized around loss functions, aggregation functions and selection algorithms.
The implementation of these functions is based on Numpy.
In this example, we start by wrapping our torch module within a subclass of :class:`deephyper.predictor.torch.TorchPredictor` that we call ``NormalTorchPredictor``. This predictor class is used to make a torch module compatible with our Numpy-based implementation for ensembles.

The ``pre_process_inputs`` is used to map a Numpy array to a Torch tensor.
The ``post_process_predictions`` is used to map a Torch tensor to a Numpy array.
It also formats the prediction as a dictionnary with ``"loc"`` (for the predictive mean) and ``"scale"`` (for the predictive standard deviation) that is necessary for our aggregation function ``MixedNormalAggregator``.

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1000-1020
=======
.. GENERATED FROM PYTHON SOURCE LINES 1009-1029
>>>>>>> master

.. code-block:: Python

    from deephyper.ensemble import EnsemblePredictor
    from deephyper.ensemble.aggregator import MixedNormalAggregator
    from deephyper.ensemble.loss import NormalNegLogLikelihood
    from deephyper.ensemble.selector import GreedySelector, TopKSelector
    from deephyper.predictor.torch import TorchPredictor

    class NormalTorchPredictor(TorchPredictor):
        def __init__(self, torch_module):
            super().__init__(torch_module.to(device=device, dtype=dtype))

        def pre_process_inputs(self, X):
            return to_torch(X)

        def post_process_predictions(self, y):
            return {
                "loc": to_numpy(y.loc),
                "scale": to_numpy(y.scale),
            }









<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1021-1024
=======
.. GENERATED FROM PYTHON SOURCE LINES 1030-1033
>>>>>>> master

After defining the predictor, we load the checkpointed models to collect their predictions into ``y_predictors``.
These predictions are the inputs of our loss, aggregation and selection functions.
We also collect the job ids of the checkpointed models into ``job_id_predictors``.

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1024-1054
=======
.. GENERATED FROM PYTHON SOURCE LINES 1033-1063
>>>>>>> master

.. code-block:: Python

    model_checkpoint_dir = os.path.join(hpo_dir, "models")

    y_predictors = []
    job_id_predictors = []

    for file_name in tqdm(os.listdir(model_checkpoint_dir)):
        if not file_name.endswith(".pt"):
            continue

        weights_path = os.path.join(model_checkpoint_dir, file_name)
        job_id = int(file_name[6:-3].split(".")[-1])

        row = hpo_results[hpo_results["job_id"] == job_id]
        if len(row) == 0:
            continue
        assert len(row) == 1

        row = row.iloc[0]
        parameters = parameters_from_row(row)
        torch_module = create_model(parameters, y_mu, y_std)
        try:
            torch_module.load_state_dict(torch.load(weights_path, weights_only=True))
        except RuntimeError:
            continue

        predictor = NormalTorchPredictor(torch_module)
        y_pred = predictor.predict(valid_X)
        y_predictors.append(y_pred)
        job_id_predictors.append(job_id)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

<<<<<<< HEAD
      0%|          | 0/258 [00:00<?, ?it/s]      7%|▋         | 19/258 [00:00<00:01, 176.63it/s]     14%|█▍        | 37/258 [00:00<00:01, 175.91it/s]     23%|██▎       | 60/258 [00:00<00:00, 199.92it/s]     32%|███▏      | 83/258 [00:00<00:00, 210.54it/s]     41%|████      | 106/258 [00:00<00:00, 215.32it/s]     50%|████▉     | 128/258 [00:00<00:00, 209.45it/s]     58%|█████▊    | 149/258 [00:00<00:00, 204.08it/s]     66%|██████▌   | 170/258 [00:00<00:00, 204.79it/s]     74%|███████▍  | 192/258 [00:00<00:00, 207.65it/s]     84%|████████▍ | 217/258 [00:01<00:00, 219.26it/s]     93%|█████████▎| 240/258 [00:01<00:00, 220.64it/s]    100%|██████████| 258/258 [00:01<00:00, 202.35it/s]
=======
      0%|          | 0/251 [00:00<?, ?it/s]      7%|▋         | 18/251 [00:00<00:01, 175.85it/s]     14%|█▍        | 36/251 [00:00<00:01, 162.40it/s]     22%|██▏       | 54/251 [00:00<00:01, 166.59it/s]     29%|██▊       | 72/251 [00:00<00:01, 168.33it/s]     35%|███▌      | 89/251 [00:00<00:00, 167.50it/s]     43%|████▎     | 107/251 [00:00<00:00, 171.01it/s]     50%|████▉     | 125/251 [00:00<00:00, 163.75it/s]     57%|█████▋    | 142/251 [00:00<00:00, 161.30it/s]     64%|██████▍   | 161/251 [00:00<00:00, 167.82it/s]     71%|███████   | 178/251 [00:01<00:00, 167.43it/s]     78%|███████▊  | 195/251 [00:01<00:00, 160.69it/s]     84%|████████▍ | 212/251 [00:01<00:00, 163.05it/s]     92%|█████████▏| 230/251 [00:01<00:00, 166.75it/s]     99%|█████████▉| 249/251 [00:01<00:00, 171.94it/s]    100%|██████████| 251/251 [00:01<00:00, 167.37it/s]
>>>>>>> master




<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1055-1067
=======
.. GENERATED FROM PYTHON SOURCE LINES 1064-1076
>>>>>>> master

Ensemble selection
------------------

This is where the ensemble selection logic happens. We use the :class:`deephyper.ensemble.selector.GreedySelector` or :class:`deephyper.ensemble.selector.TopKSelector` class.
The top-k selection, selects the topk-k models according to the given ``los_func`` and weight them equally in the ensemble.
The greedy selection, iteratively selects models from the checkpoints that improves the current ensemble.

The ``aggregator`` is the logic that combines a set of predictors into a single predictor to form the ensemble's prediction.
In our case, we use the :class:`deephyper.ensemble.aggregator.MixedNormalAggregator` that approximates a mixture of normal distribution (each normal distribution is the output of a checkpointed model) as a normal distribution.

To try top-k or greedy selection just uncomment/comment the corresponding code.
This part of the code is fast to compute.

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1067-1101
=======
.. GENERATED FROM PYTHON SOURCE LINES 1076-1110
>>>>>>> master

.. code-block:: Python

    k = 50

    # Top-K Selection
    # selector = TopKSelector(
    #     loss_func=NormalNegLogLikelihood(),
    #     k=k,
    # )

    # Greedy Selection
    selector = GreedySelector(
        loss_func=NormalNegLogLikelihood(),
        aggregator=MixedNormalAggregator(),
        k=k,
        max_it=k,
        k_init=3,
        early_stopping=True,
        with_replacement=True,
        bagging=True,
        verbose=True,
    )

    selected_predictors_indexes, selected_predictors_weights = selector.select(
        valid_y,
        y_predictors,
    )

    print(f"{selected_predictors_indexes=}")
    print(f"{selected_predictors_weights=}")

    selected_predictors_job_ids = np.array(job_id_predictors)[selected_predictors_indexes]
    selected_predictors_job_ids

    print(f"{selected_predictors_job_ids=}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

<<<<<<< HEAD
    Ensemble initialized with [190, 174, 58]
    Step 1, ensemble is [190, 174, 58, 169]
    Step 2, ensemble is [190, 174, 58, 169, 190]
    Step 3, ensemble selection stopped
    After 3 steps, the final ensemble is [ 58 169 174 190] with weights [0.2 0.2 0.2 0.4]
    selected_predictors_indexes=[58, 169, 174, 190]
    selected_predictors_weights=[0.2, 0.2, 0.2, 0.4]
    selected_predictors_job_ids=array([211,  37,  42, 206])
=======
    Ensemble initialized with [142, 14, 92]
    Step 1, ensemble is [142, 14, 92, 134]
    Step 2, ensemble selection stopped
    After 2 steps, the final ensemble is [ 14  92 134 142] with weights [0.25 0.25 0.25 0.25]
    selected_predictors_indexes=[14, 92, 134, 142]
    selected_predictors_weights=[0.25, 0.25, 0.25, 0.25]
    selected_predictors_job_ids=array([251, 158,  32, 199])
>>>>>>> master




<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1102-1117
=======
.. GENERATED FROM PYTHON SOURCE LINES 1111-1126
>>>>>>> master

Evaluation of the ensemble
--------------------------

Now that we have a set of predictors with their corresponding weights in the ensemble we can look at the predictions.
For this, we use the :class:`deephyper.ensemble.EnsemblePredictor` class.
This class can use the :class:`deephyper.evaluator.Evaluator` to parallelize the inference of ensemble members.
Then, we need to give it the list of ``predictors``, ``weights`` and the ``aggregator``.
For inference, we set ``decomposed_scale=True`` for the :class:`deephyper.ensemble.aggregator.MixedNormalAggregator` as we want
to predict disentangled epistemic and aleatoric uncertainty using the law of total variance:

.. math::

    V_Y[Y|X=x] = \underbrace{E_\Theta\left[V_Y[Y|X=x;\Theta\right]}_\text{Aleatoric Uncertainty} + \underbrace{V_\Theta\left[E_Y[Y|X=x;\Theta]\right]}_\text{Epistemic Uncertainty}

where :math:`\Theta` is the random variable that represents a concatenation of weights and hyperparameters, :math:`Y`` is the random variable representing a target prediction, and :math:`X` is the random variable representing an observed input.

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1117-1142
=======
.. GENERATED FROM PYTHON SOURCE LINES 1126-1151
>>>>>>> master

.. code-block:: Python

    predictors = []

    hpo_dir = "nas_regression"
    model_checkpoint_dir = os.path.join(hpo_dir, "models")

    for job_id in selected_predictors_job_ids:
        file_name = f"model_0.{job_id}.pt"

        weights_path = os.path.join(model_checkpoint_dir, file_name)

        row = hpo_results[hpo_results["job_id"] == job_id].iloc[0]
        parameters = parameters_from_row(row)
        torch_module = create_model(parameters, y_mu, y_std)
        torch_module.load_state_dict(torch.load(weights_path, weights_only=True))
        predictor = NormalTorchPredictor(torch_module)
        predictors.append(predictor)

    ensemble = EnsemblePredictor(
        predictors=predictors,
        weights=selected_predictors_weights,
        aggregator=MixedNormalAggregator(decomposed_scale=True),
    )

    y_pred = ensemble.predict(test_X)








<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1143-1147
=======
.. GENERATED FROM PYTHON SOURCE LINES 1152-1156
>>>>>>> master

In the visualization, we can first observe that the mean prediction is close to the true function.

Then, to visualize both uncertainties together we plot the variance.
The goal is to observe the epistemic component vanish in areas where we observed data.

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1148-1186
=======
.. GENERATED FROM PYTHON SOURCE LINES 1157-1195
>>>>>>> master

.. dropdown:: Code (Make uncertainty plot)

    .. code-block:: Python


        _ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))
        _ = plt.scatter(train_X, train_y, s=5, label="Training")
        _ = plt.scatter(valid_X, valid_y, s=5, label="Validation")
        _ = plt.plot(test_X, test_y, linestyle="--", color="gray", label="Test")
        _ = plt.plot(test_X, y_pred["loc"], label=r"$\mu(x)$")
        _ = plt.fill_between(
            test_X.reshape(-1),
            (y_pred["loc"] - y_pred["scale_aleatoric"]**2).reshape(-1),
            (y_pred["loc"] + y_pred["scale_aleatoric"]**2).reshape(-1),
            alpha=0.25,
            label=r"$\sigma_\text{al}^2(x)$",
        )
        _ = plt.fill_between(
            test_X.reshape(-1),
            (y_pred["loc"] - y_pred["scale_aleatoric"]**2).reshape(-1),
            (y_pred["loc"] - y_pred["scale_aleatoric"]**2 - y_pred["scale_epistemic"]**2).reshape(-1),
            alpha=0.25,
            color="red",
            label=r"$\sigma_\text{ep}^2(x)$",
        )
        _ = plt.fill_between(
            test_X.reshape(-1),
            (y_pred["loc"] + y_pred["scale_aleatoric"]**2).reshape(-1),
            (y_pred["loc"] + y_pred["scale_aleatoric"]**2 + y_pred["scale_epistemic"]**2).reshape(-1),
            alpha=0.25,
            color="red",
        )
        _ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.25)
        _ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.25)
        _ = plt.xlim(-x_lim, x_lim)
        _ = plt.ylim(-y_lim, y_lim)
        _ = plt.legend(ncols=2)
        _ = plt.xlabel(r"$x$")
        _ = plt.ylabel(r"$f(x)$")
        _ = plt.grid(which="both", linestyle=":")




.. image-sg:: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_007.png
   :alt: plot nas deep ensemble uq regression pytorch
   :srcset: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_007.png
   :class: sphx-glr-single-img





<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1188-1192
=======
.. GENERATED FROM PYTHON SOURCE LINES 1197-1201
>>>>>>> master

Aleatoric Uncertainty
~~~~~~~~~~~~~~~~~~~~~

Now, if we isolate the aleatoric uncertainty we observe that we somewhat correctly estimated the lower aleatoric uncertainty on the left side, and larger on the right side.

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1192-1216
=======
.. GENERATED FROM PYTHON SOURCE LINES 1201-1225
>>>>>>> master

.. dropdown:: Code (Make aleatoric uncertainty plot)

    .. code-block:: Python


        kappa = 1.96
        _ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))
        _ = plt.scatter(train_X, train_y, s=5, label="Training")
        _ = plt.scatter(valid_X, valid_y, s=5, label="Validation")
        _ = plt.plot(test_X, test_y, linestyle="--", color="gray", label="Test")
        _ = plt.plot(test_X, y_pred["loc"], label=r"$\mu(x)$")
        _ = plt.fill_between(
            test_X.reshape(-1),
            (y_pred["loc"] - kappa * y_pred["scale_aleatoric"]).reshape(-1),
            (y_pred["loc"] + kappa * y_pred["scale_aleatoric"]).reshape(-1),
            alpha=0.25,
            label=r"$\sigma_\text{al}(x)$",
        )
        _ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.25)
        _ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.25)
        _ = plt.xlim(-x_lim, x_lim)
        _ = plt.ylim(-y_lim, y_lim)
        _ = plt.legend(ncols=2)
        _ = plt.xlabel(r"$x$")
        _ = plt.ylabel(r"$f(x)$")
        _ = plt.grid(which="both", linestyle=":")




.. image-sg:: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_008.png
   :alt: plot nas deep ensemble uq regression pytorch
   :srcset: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_008.png
   :class: sphx-glr-single-img





<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1217-1221
=======
.. GENERATED FROM PYTHON SOURCE LINES 1226-1230
>>>>>>> master

Epistemic uncertainty
~~~~~~~~~~~~~~~~~~~~~

Finally, if we isole the epistemic uncertainty we observe that it vanishes in the grey areas where we observed data and grows in areas were we did not have data.

<<<<<<< HEAD
.. GENERATED FROM PYTHON SOURCE LINES 1221-1245
=======
.. GENERATED FROM PYTHON SOURCE LINES 1230-1254
>>>>>>> master

.. dropdown:: Code (Make epistemic uncertainty plot)

    .. code-block:: Python


        kappa = 1.96
        _ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))
        _ = plt.scatter(train_X, train_y, s=5, label="Training")
        _ = plt.scatter(valid_X, valid_y, s=5, label="Validation")
        _ = plt.plot(test_X, test_y, linestyle="--", color="gray", label="Test")
        _ = plt.plot(test_X, y_pred["loc"], label=r"$\mu(x)$")
        _ = plt.fill_between(
            test_X.reshape(-1),
            (y_pred["loc"] - kappa * y_pred["scale_epistemic"]).reshape(-1),
            (y_pred["loc"] + kappa * y_pred["scale_epistemic"]).reshape(-1),
            alpha=0.25,
            color="red",
            label=r"$\sigma_\text{ep}(x)$",
        )
        _ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.25)
        _ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color="gray", alpha=0.25)
        _ = plt.xlim(-x_lim, x_lim)
        _ = plt.ylim(-y_lim, y_lim)
        _ = plt.legend(ncols=2)
        _ = plt.xlabel(r"$x$")
        _ = plt.ylabel(r"$f(x)$")
        _ = plt.grid(which="both", linestyle=":")



.. image-sg:: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_009.png
   :alt: plot nas deep ensemble uq regression pytorch
   :srcset: /examples/examples_uq/images/sphx_glr_plot_nas_deep_ensemble_uq_regression_pytorch_009.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

<<<<<<< HEAD
   **Total running time of the script:** (0 minutes 19.320 seconds)
=======
   **Total running time of the script:** (0 minutes 19.953 seconds)
>>>>>>> master


.. _sphx_glr_download_examples_examples_uq_plot_nas_deep_ensemble_uq_regression_pytorch.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_nas_deep_ensemble_uq_regression_pytorch.ipynb <plot_nas_deep_ensemble_uq_regression_pytorch.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_nas_deep_ensemble_uq_regression_pytorch.py <plot_nas_deep_ensemble_uq_regression_pytorch.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_nas_deep_ensemble_uq_regression_pytorch.zip <plot_nas_deep_ensemble_uq_regression_pytorch.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
