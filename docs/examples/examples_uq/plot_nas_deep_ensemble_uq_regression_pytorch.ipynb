{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Neural Architecture Search and Deep Ensemble with Uncertainty Quantification for Regression (Pytorch)\n\n**Author(s)**: Romain Egele, Brett Eiffert.\n\nIn this tutorial, you will learn how to perform **Neural Architecture Search (NAS)** and use it to construct a diverse deep ensemble with disentangled **aleatoric** and **epistemic uncertainty**.\n \nNAS is the idea of automatically optimizing the architecture of deep neural networks to solve a given task. Here, we will use **hyperparameter optimization (HPO)** algorithms to guide the NAS process.\n\nSpecifically, in this tutorial you will learn how to:\n\n1.      **Define a customizable PyTorch module** that exposes neural architecture hyperparameters.\n2.      **Define constraints** on the neural architecture hyperparameters to reduce redundancies and improve efficiency of the optimization.\n\nThis tutorial will provide a hands-on approach to leveraging NAS for robust regression models with well-calibrated uncertainty estimates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation and imports\n\nInstalling dependencies with the `pip installation <install-pip>` is recommended. It requires **Python >= 3.10**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\npip install \"deephyper[ray,torch]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Import statements\nimport json\nimport os\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nWIDTH_PLOTS = 8\nHEIGHT_PLOTS = WIDTH_PLOTS / 1.618"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Synthetic data generation\n\nWe generate synthetic data from a 1D scalar function $Y = f(X) + \\epsilon(X)$, where $X,Y$ are random variables with support $\\mathbb{R}$.\n\nThe training data are drown uniformly from $X \\sim U([-30,-15] \\cup [15,30])$ with:\n\n\\begin{align}f(x) = \\cos(x/2) + 2 \\cdot \\sin(x/10) + x/100\\end{align}\n\nand $\\epsilon(X) \\sim \\mathcal{N}(0, \\sigma(X))$ with:\n\n- $\\sigma(x) = 0.5$ if $x \\in [-30,-15]$\n- $\\sigma(x) = 1.0$ if $x \\in [15,30]$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Loading synthetic data\ndef load_data(\n    developement_size=500,\n    test_size=200,\n    random_state=42,\n    x_min=-50,\n    x_max=50,\n):\n    rs = np.random.RandomState(random_state)\n\n    def f(x):\n        return np.cos(x / 2) + 2 * np.sin(x / 10) + x / 100\n\n    x_1 = rs.uniform(low=-30, high=-15.0, size=developement_size // 2)\n    eps_1 = rs.normal(loc=0.0, scale=0.5, size=developement_size // 2)\n    y_1 = f(x_1) + eps_1\n\n    x_2 = rs.uniform(low=15.0, high=30.0, size=developement_size // 2)\n    eps_2 = rs.normal(loc=0.0, scale=1.0, size=developement_size // 2)\n    y_2 = f(x_2) + eps_2\n\n    x = np.concatenate([x_1, x_2], axis=0)\n    y = np.concatenate([y_1, y_2], axis=0)\n\n    test_X = np.linspace(x_min, x_max, test_size)\n    test_y = f(test_X)\n\n    x = x.reshape(-1, 1)\n    y = y.reshape(-1, 1)\n\n    train_X, valid_X, train_y, valid_y = train_test_split(\n        x, y, test_size=0.33, random_state=random_state\n    )\n\n    test_X = test_X.reshape(-1, 1)\n    test_y = test_y.reshape(-1, 1)\n\n    return (train_X, train_y), (valid_X, valid_y), (test_X, test_y)\n\n\n(train_X, train_y), (valid_X, valid_y), (test_X, test_y) = load_data()\n\ny_mu, y_std = np.mean(train_y), np.std(train_y)\n\nx_lim, y_lim = 50, 7\n_ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\n_ = plt.scatter(train_X, train_y, s=5, label=\"Training\")\n_ = plt.scatter(valid_X, valid_y, s=5, label=\"Validation\")\n_ = plt.plot(test_X, test_y, linestyle=\"--\", color=\"gray\", label=\"Test\")\n_ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.25)\n_ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.25)\n_ = plt.xlim(-x_lim, x_lim)\n_ = plt.ylim(-y_lim, y_lim)\n_ = plt.legend()\n_ = plt.xlabel(r\"$x$\")\n_ = plt.ylabel(r\"$f(x)$\")\n_ = plt.grid(which=\"both\", linestyle=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configurable neural network with uncertainty\n\nWe define a configurable Pytorch module to be able to explore:\n\n- the number of layers\n- the number of units per layer\n- the activation function per layer\n- the dropout rate\n- the output layer\n\nThe output of this module will be a Gaussian distribution $\\mathcal{N}(\\mu_\\theta(x), \\sigma_\\theta(x))$, where $\\theta$ represent the concatenation of the weights and the hyperparameters of our model.\n\nThe uncertainty $\\sigma_\\theta(x)$ estimated by the network is an estimator of $V_Y[Y|X=x]$ therefore corresponding\nto aleatoric uncertainty (a.k.a., intrinsic noise).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nclass DeepNormalRegressor(nn.Module):\n    def __init__(\n        self,\n        n_inputs,\n        layers,\n        n_units_mean=64,\n        n_units_std=64,\n        std_offset=1e-3,\n        softplus_factor=0.05,\n        loc=0,\n        scale=1.0,\n    ):\n        super().__init__()\n\n        layers_ = []\n        prev_n_units = n_inputs\n        for n_units, activation, dropout_rate in layers:\n            linear_layer = nn.Linear(prev_n_units, n_units)\n            if activation == \"relu\":\n                activation_layer = nn.ReLU()\n            elif activation == \"sigmoid\":\n                activation_layer = nn.Sigmoid()\n            elif activation == \"tanh\":\n                activation_layer = nn.Tanh()\n            elif activation == \"swish\":\n                activation_layer = nn.SiLU()\n            elif activation == \"mish\":\n                activation_layer = nn.Mish()\n            elif activation == \"gelu\":\n                activation_layer = nn.GELU()\n            elif activation == \"silu\":\n                activation_layer = nn.SiLU()\n            dropout_layer = nn.Dropout(dropout_rate)\n\n            layers_.extend([linear_layer, activation_layer, dropout_layer])\n\n            prev_n_units = n_units\n\n        # Shared parameters\n        self.shared_layer = nn.Sequential(\n            *layers_,\n        )\n\n        # Mean parameters\n        self.mean_layer = nn.Sequential(\n            nn.Linear(prev_n_units, n_units_mean),\n            nn.ReLU(),\n            nn.Linear(n_units_mean, 1),\n        )\n\n        # Standard deviation parameters\n        self.std_layer = nn.Sequential(\n            nn.Linear(prev_n_units, n_units_std),\n            nn.ReLU(),\n            nn.Linear(n_units_std, 1),\n            nn.Softplus(beta=1.0, threshold=20.0),  # enforces positivity\n        )\n\n        self.std_offset = std_offset\n        self.softplus_factor = softplus_factor\n        self.loc = loc\n        self.scale = scale\n\n    def forward(self, x):\n        # Shared embedding\n        shared = self.shared_layer(x)\n\n        # Parametrization of the mean\n        mu = self.mean_layer(shared) + self.loc\n\n        # Parametrization of the standard deviation\n        sigma = self.std_offset + self.std_layer(self.softplus_factor * shared) * self.scale\n\n        return torch.distributions.Normal(mu, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter search space\n\nWe define the hyperparameter space that includes both **neural architecture** and **training hyperparameters**.\n\nWithout having a good heuristic on training hyperparameters given the neural architecture hyperparameter search space \nit is important to define them jointly with the neural architecture hyperparameters as they can have strong interactions. \n\nIn the definition of the hyperparameter space, we add constraints using :class:`ConfigSpace.GreaterThanCondition` to\nrepresent when an hyperparameter is active. In this example, \"active\" means it actually influence the code execution of\nthe trained model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ConfigSpace import GreaterThanCondition\nfrom deephyper.hpo import HpProblem\n\n\ndef create_hpo_problem(min_num_layers=3, max_num_layers=8, max_num_units=512):\n    problem = HpProblem()\n\n    # Neural Architecture Hyperparameters\n    num_layers = problem.add_hyperparameter((min_num_layers, max_num_layers), \"num_layers\", default_value=5)\n\n    conditions = []\n    for i in range(max_num_layers):\n\n        # Adding the hyperparameters that impact each layer of the model\n        layer_i_units = problem.add_hyperparameter((16, max_num_units), f\"layer_{i}_units\", default_value=max_num_units)\n        layer_i_activation = problem.add_hyperparameter(\n            [\"relu\", \"sigmoid\", \"tanh\", \"swish\", \"mish\", \"gelu\", \"silu\"],\n            f\"layer_{i}_activation\",\n            default_value=\"relu\",\n        )\n        layer_i_dropout_rate = problem.add_hyperparameter(\n            (0.0, 0.25), f\"layer_{i}_dropout_rate\", default_value=0.0\n        )\n\n        # Adding the constraints to define when these hyperparameters are active\n        if i + 1 > min_num_layers:\n            conditions.extend(\n                [\n                    GreaterThanCondition(layer_i_units, num_layers, i),\n                    GreaterThanCondition(layer_i_activation, num_layers, i),\n                    GreaterThanCondition(layer_i_dropout_rate, num_layers, i),\n                ]\n            )\n\n    problem.add_conditions(conditions)\n\n    # Hyperparameters of the output layers\n    problem.add_hyperparameter((16, max_num_units), \"n_units_mean\", default_value=max_num_units)\n    problem.add_hyperparameter((16, max_num_units), \"n_units_std\", default_value=max_num_units)\n    problem.add_hyperparameter((1e-8, 1e-2, \"log-uniform\"), \"std_offset\", default_value=1e-3)\n    problem.add_hyperparameter((0.01, 1.0), \"softplus_factor\", default_value=0.05)\n\n    # Training Hyperparameters\n    problem.add_hyperparameter((1e-5, 1e-1, \"log-uniform\"), \"learning_rate\", default_value=2e-3)\n    problem.add_hyperparameter((8, 256, \"log-uniform\"), \"batch_size\", default_value=32)\n    problem.add_hyperparameter((0.01, 0.99), \"lr_scheduler_factor\", default_value=0.1)\n    problem.add_hyperparameter((10, 100), \"lr_scheduler_patience\", default_value=20)\n\n    return problem\n\nproblem = create_hpo_problem()\nproblem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss and Metric\n\nFor the loss we will use the Gaussian negative log-likelihood to evalute the quality of the \npredicted distribution $\\mathcal{N}(\\mu_\\theta(x), \\sigma_\\theta(x))$ using with formula:\n\n\\begin{align}L_\\text{NLL}(x, y;\\theta) = \\frac{1}{2}\\left(\\log\\left(\\sigma_\\theta^{2}(x)\\right) + \\frac{\\left(y-\\mu_{\\theta}(x)\\right)^{2}}{\\sigma_{\\theta}^{2}(x)}\\right) + \\text{cst}\\end{align}\n\nAs complementary metric, we use the squared error to evaluate the quality of the mean predictions $\\mu_\\theta(x)$:\n\n\\begin{align}L_\\text{SE}(x, y;\\theta) = (\\mu_\\theta(x)-y)^2\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def nll(y, rv_y):\n    \"\"\"Negative log likelihood for Pytorch distribution.\n\n    Args:\n        y: true data.\n        rv_y: learned (predicted) probability distribution.\n    \"\"\"\n    return -rv_y.log_prob(y)\n\n\ndef squared_error(y_true, rv_y):\n    \"\"\"Squared error for Pytorch distribution.\n\n    Args:\n        y: true data.\n        rv_y: learned (predicted) probability distribution.\n    \"\"\"\n    y_pred = rv_y.mean\n    return (y_true - y_pred) ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n\nIn our training loop, we make sure to collect training and validation learning curves for better analysis.\n\nWe also add a mechanism to checkpoint weights of the model based on the best observed validation loss.\n\nFinally, we add an early stopping mechanism to save computing resources.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Training loop\ndef train_one_step(model, optimizer, x_batch, y_batch):\n    model.train()\n    optimizer.zero_grad()\n    y_dist = model(x_batch)\n\n    loss = torch.mean(nll(y_batch, y_dist))\n    mse = torch.mean(squared_error(y_batch, y_dist))\n\n    loss.backward()\n    optimizer.step()\n\n    return loss, mse\n\n\ndef train(\n    job,\n    model,\n    optimizer,\n    x_train,\n    x_val,\n    y_train,\n    y_val,\n    n_epochs,\n    batch_size,\n    scheduler=None,\n    patience=200,\n    progressbar=True,\n):\n    data_train = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n\n    checkpointed_state_dict = model.state_dict()\n    checkpointed_val_loss = np.inf\n\n    train_loss, val_loss = [], []\n    train_mse, val_mse = [], []\n\n    tqdm_bar = tqdm(total=n_epochs, disable=not progressbar)\n\n    for epoch in range(n_epochs):\n        batch_losses_t, batch_losses_v, batch_mse_t, batch_mse_v = [], [], [], []\n\n        for batch_x, batch_y in data_train:\n            b_train_loss, b_train_mse = train_one_step(model, optimizer, batch_x, batch_y)\n\n            model.eval()\n            y_dist = model(x_val)\n            b_val_loss = torch.mean(nll(y_val, y_dist))\n            b_val_mse = torch.mean(squared_error(y_val, y_dist))\n\n            batch_losses_t.append(to_numpy(b_train_loss))\n            batch_mse_t.append(to_numpy(b_train_mse))\n            batch_losses_v.append(to_numpy(b_val_loss))\n            batch_mse_v.append(to_numpy(b_val_mse))\n\n        train_loss.append(np.mean(batch_losses_t))\n        val_loss.append(np.mean(batch_losses_v))\n        train_mse.append(np.mean(batch_mse_t))\n        val_mse.append(np.mean(batch_mse_v))\n\n        if scheduler is not None:\n            scheduler.step(val_loss[-1])\n\n        tqdm_bar.update(1)\n        tqdm_bar.set_postfix(\n            {\n                \"train_loss\": f\"{train_loss[-1]:.3f}\",\n                \"val_loss\": f\"{val_loss[-1]:.3f}\",\n                \"train_mse\": f\"{train_mse[-1]:.3f}\",\n                \"val_mse\": f\"{val_mse[-1]:.3f}\",\n            }\n        )\n\n        # Checkpoint weights if they improve\n        if val_loss[-1] < checkpointed_val_loss:\n            checkpointed_val_loss = val_loss[-1]\n            checkpointed_state_dict = model.state_dict()\n\n        # Early discarding\n        job.record(budget=epoch+1, objective=-val_loss[-1])\n        if job.stopped():\n            break\n\n        if len(val_loss) > (patience + 1) and val_loss[-patience - 1] < min(val_loss[-patience:]):\n            break\n\n    # Reload the best weights\n    model.load_state_dict(checkpointed_state_dict)\n\n    return train_loss, val_loss, train_mse, val_mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run time\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n\ndtype = torch.float32\nif torch.cuda.is_available():\n    device = \"cuda\"\n    device_count = 1\nelse:\n    device = \"cpu\"\n    device_count = multiprocessing.cpu_count()\n\nprint(f\"Runtime with {device=}, {device_count=}, {dtype=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Conversion utility functions\n\ndef to_torch(array):\n    return torch.from_numpy(array).to(device=device, dtype=dtype)\n\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation function\n\nWe start by defining a function that will create the Torch module from a dictionnary of hyperparameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_model(parameters: dict, y_mu=0, y_std=1):\n    num_layers = parameters[\"num_layers\"]\n    torch_module = DeepNormalRegressor(\n        n_inputs=1,\n        layers=[\n            (\n                parameters[f\"layer_{i}_units\"],\n                parameters[f\"layer_{i}_activation\"],\n                parameters[f\"layer_{i}_dropout_rate\"],\n            )\n            for i in range(num_layers)\n        ],\n        n_units_mean=parameters[\"n_units_mean\"],\n        n_units_std=parameters[\"n_units_std\"],\n        std_offset=parameters[\"std_offset\"],\n        softplus_factor=parameters[\"softplus_factor\"],\n        loc=y_mu,\n        scale=y_std,\n    ).to(device=device, dtype=dtype)\n    return torch_module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The evaluation function (often called ``run``-function in DeepHyper) is the function that \nreceives suggested parameters as inputs ``job.parameters`` and returns an ``\"objective\"`` \nthat we want to maximize.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_n_epochs = 1_000\n\n\ndef run(job, model_checkpoint_dir=\".\", verbose=False):\n    (x, y), (vx, vy), (tx, ty) = load_data()\n\n    # Create the model based on neural architecture hyperparameters\n    model = create_model(job.parameters, y_mu, y_std)\n\n    if verbose:\n        print(model)\n\n    # Initialize training loop based on training hyperparameters\n    optimizer = torch.optim.Adam(model.parameters(), lr=job.parameters[\"learning_rate\"])\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer,\n        factor=job.parameters[\"lr_scheduler_factor\"],\n        patience=job.parameters[\"lr_scheduler_patience\"],\n    )\n\n    x, vx, tx = to_torch(x), to_torch(vx), to_torch(tx)\n    y, vy, ty = to_torch(y), to_torch(vy), to_torch(ty)\n\n    try:\n        train_losses, val_losses, train_mse, val_mse = train(\n            job,\n            model,\n            optimizer,\n            x,\n            vx,\n            y,\n            vy,\n            n_epochs=max_n_epochs,\n            batch_size=job.parameters[\"batch_size\"],\n            scheduler=scheduler,\n            progressbar=verbose,\n        )\n    except Exception:\n        return \"F_fit\"\n\n    ty_pred = model(tx)\n    test_loss = to_numpy(torch.mean(nll(ty, ty_pred)))\n    test_mse = to_numpy(torch.mean(squared_error(ty, ty_pred)))\n\n    # Saving the model's state (i.e., weights)\n    torch.save(model.state_dict(), os.path.join(model_checkpoint_dir, f\"model_{job.id}.pt\"))\n\n    return {\n        \"objective\": -val_losses[-1],\n        \"metadata\": {\n            \"train_loss\": train_losses,\n            \"val_loss\": val_losses,\n            \"train_mse\": train_mse,\n            \"val_mse\": val_mse,\n            \"test_loss\": test_loss,\n            \"test_mse\": test_mse,\n            \"budget\": len(val_losses),\n        },\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation of the baseline\n\nWe evaluate the default configuration of hyperparameters that we call \"baseline\" using the same evaluation function.\nThis allows to test the evaluation function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.evaluator import RunningJob\n\nbaseline_dir = \"nas_baseline_regression\"\n\ndef evaluate_baseline(problem):\n    model_checkpoint_dir = os.path.join(baseline_dir, \"models\")\n    pathlib.Path(model_checkpoint_dir).mkdir(parents=True, exist_ok=True)\n\n    default_parameters = problem.default_configuration\n    print(f\"{default_parameters=}\\n\")\n\n    result = run(\n        RunningJob(parameters=default_parameters),\n        model_checkpoint_dir=model_checkpoint_dir,\n        verbose=True,\n    )\n    return result\n\nbaseline_results = evaluate_baseline(problem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we look at the learning curves of our baseline model returned by the evaluation function.\n\nThese curves display a good learning behaviour:\n\n- the training and validation curves follow each other closely and are decreasing.\n- a clear convergence plateau is reached at the end of the training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Make learning curves plot\n_ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\n\nx_values = np.arange(1, len(baseline_results[\"metadata\"][\"train_loss\"]) + 1)\n_ = plt.plot(\n    x_values,\n    baseline_results[\"metadata\"][\"train_loss\"],\n    label=\"Training\",\n)\n_ = plt.plot(\n    x_values,\n    baseline_results[\"metadata\"][\"val_loss\"],\n    label=\"Validation\",\n)\n\n_ = plt.xlim(x_values.min(), x_values.max())\n_ = plt.grid(which=\"both\", linestyle=\":\")\n_ = plt.legend()\n_ = plt.xlabel(\"Epochs\")\n_ = plt.ylabel(\"NLL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition, we look at the predictions by reloading the checkpointed weights.\n\nWe first need to recreate the torch module and then we update its state using the checkpointed weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights_path = os.path.join(baseline_dir, \"models\",  \"model_0.0.pt\")\nparameters = problem.default_configuration\ntorch_module = create_model(parameters, y_mu, y_std)\ntorch_module.load_state_dict(torch.load(weights_path, weights_only=True))\ntorch_module.eval()\n\ny_pred = torch_module.forward(to_torch(test_X))\ny_pred_mean = to_numpy(y_pred.loc)\ny_pred_std = to_numpy(y_pred.scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Make prediction plot\n_ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\n_ = plt.scatter(train_X, train_y, s=5, label=\"Training\")\n_ = plt.scatter(valid_X, valid_y, s=5, label=\"Validation\")\n_ = plt.plot(test_X, test_y, linestyle=\"--\", color=\"gray\", label=\"Test\")\n\n_ = plt.plot(test_X, y_pred_mean, label=r\"$\\mu(x)$\")\nkappa = 1.96\n_ = plt.fill_between(\n    test_X.reshape(-1),\n    (y_pred_mean - kappa * y_pred_std).reshape(-1),\n    (y_pred_mean + kappa * y_pred_std).reshape(-1),\n    alpha=0.25,\n    label=r\"$\\sigma_\\text{al}(x)$\",\n)\n\n_ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.15)\n_ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.15)\n_ = plt.xlim(-x_lim, x_lim)\n_ = plt.ylim(-y_lim, y_lim)\n_ = plt.legend(ncols=2)\n_ = plt.xlabel(r\"$x$\")\n_ = plt.ylabel(r\"$f(x)$\")\n_ = plt.grid(which=\"both\", linestyle=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural architecture search\n\nWe will now use Bayesian opimization to perform neural architecture search. \nThe sequential Bayesian optimization algorithm can be described by the following pseudo-code:\n\n### Sequential Bayesian optimization\n\n**Algorithm**: Bayesian Optimization (a.k.a., Efficient Global Optimization (EGO))\n\n  Inputs\n    $\\texttt{thetaSpace}$: a hyperparameter space\n\n    $\\texttt{nInitial}$: the number of initial hyperparameter configurations\n\n    $\\texttt{f}$: a function that returns the objective of the learning workflow\n\n  Outputs\n    $\\texttt{thetaStar}$ the recommended hyperparameter configuration\n\n  $\\texttt{thetaArray}, \\texttt{objArray} \\gets$ New empty arrays of hyperparameter configurations and objectives\n  $\\texttt{model} \\gets$ New surrogate model\n\n  Loop until stopping criteria is not valid\n\n    If Length of $\\texttt{thetaArray} < \\texttt{nInitial}$ then\n\n      $\\texttt{theta} \\gets$ Sample hyperparameter configuration from $\\texttt{thetaSpace}$\n\n    Else\n\n      Update $\\texttt{model}$ with $\\texttt{thetaArray}, \\texttt{objArray}$\n\n      $\\texttt{theta} \\gets$ Returns $\\texttt{theta}$ in $\\texttt{thetaSpace}$ that maximizes \n      the acquisition function for the current $\\texttt{model}$\n\n    $\\texttt{obj} \\gets$ Returns the objective of learning workflow $\\texttt{f}(\\texttt{theta})$\n\n    $\\texttt{thetaArray}  \\gets$ Concatenate $\\texttt{thetaArray}$ with $[\\texttt{theta}]$\n\n    $\\texttt{objArray}  \\gets$ Concatenate $\\texttt{objArray}$ with $[\\texttt{obj}]$\n\n    $\\texttt{thetaStar} \\gets$ Update recommendation\n\n### Parallel Bayesian optimization\n\nIn DeepHyper, instead of just performing sequential Bayesian optimization we provide asynchronous parallelisation for\nBayesian optimization (and other methods). This allows to execute multiple evaluation function in parallel to collect observations of objectives\nfaster.\n\nIn this example, we will focus on using centralized Bayesian optimization (CBO). In this setting, we have one main process that runs the\nBayesian optimization algorithm and we have multiple worker processes that run evaluation functions. The class we use for this is\n:class:`deephyper.hpo.CBO`.\n\nLet us start by explaining import configuration parameters of :class:`deephyper.hpo.CBO`:\n\n- ``initial_points``: is a list of initial hyperparameter configurations to test, we add the baseline hyperparameters as we want to be at least better than this configuration.\n- ``surrogate_model_*``: are parameters related to the surrogate model we use, here ``\"ET\"`` is an alias for the Extremely Randomized Trees regression model.\n- ``multi_point_strategy``: is the strategy we use for parallel suggestion of hyperparameters, here we use the ``qUCBd`` that will sample for each new parallel configuration a different $\\kappa^j_i$ value from an exponential with mean $\\kappa_i$ where $j$ is the index in the current generated parallel batch and $i$ is the iteration of the Bayesian optimization loop. ``UCB`` corresponds to the Upper Confidence Bound acquisition function:\n\n\\begin{align}\\alpha_\\text{UCB}(\\theta;\\kappa) = \\mu_\\text{ET}(\\theta) + \\kappa \\cdot \\sigma_\\text{ET}(\\theta)\\end{align}\n\nwhere $\\mu_\\text{ET}(\\theta)$ and $\\sigma_\\text{ET}^2(\\theta)$ are respectively estimators of $E_C[C|\\Theta=\\theta]$ and $V_C[C|\\Theta=\\theta]$ with $C$ the random variable describing the objective (or cost) and $\\Theta$ the random variable describing the hyperparameters alone.\n\nFinally the ``\"d\"`` postfix in ``qUCBd`` means that we will only consider the epistemic component of the uncertainty returned by the surrogate model.\nThanks to the law of total variance we have the following decomposition:\n\n\\begin{align}V_C[C|\\Theta=\\theta] = E_\\text{tree}\\left[V_C[C|\\Theta=\\theta;\\text{tree}\\right] + V_\\text{tree}\\left[E_C[C|\\Theta=\\theta;\\text{tree}]\\right]\\end{align}\n\nThen, we define $\\sigma_{\\text{ET},\\text{ep}}(\\theta)$ as the empirical estimate of $V_\\text{tree}\\left[E_C[C|\\Theta=\\theta;\\text{tree}]\\right]$.\nThen, we define $\\alpha_\\text{qUCBd}(\\theta;\\kappa^j_i)$ as:\n\n\\begin{align}\\alpha_\\text{qUCBd}(\\theta;\\kappa^j_i) = \\mu_\\text{ET}(\\theta) + \\kappa^j_i \\cdot \\sigma_{\\text{ET},\\text{ep}}(\\theta)\\end{align}\n\nInterestingly the same trick will be used later to decompose the uncertainty of the deep ensemble.\n\n- ``acq_optimizer_*``: are parameters related to optimization of the previously defined acquisition function.\n- ``kappa`` and ``scheduler``: are the parameters that define the schedule of $\\kappa^j_i$ previously mentionned.\n- ``objective_scaler``: is a parameter that can be used to rescale the observed objectives (e.g., identity, min-max, log).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search_kwargs = {\n    \"initial_points\": [problem.default_configuration],\n    \"n_initial_points\": 2 * len(problem) + 1,  # Number of initial random points\n    \"surrogate_model\": \"ET\",  # Use Extra Trees as surrogate model\n    \"surrogate_model_kwargs\": {\n        \"n_estimators\": 50,  # Relatively small number of trees in the surrogate to make it \"fast\"\n        \"min_samples_split\": 8,  # Larger number to avoid small leaf nodes (smoothing the objective response)\n    },\n    \"multi_point_strategy\": \"qUCBd\",  # Multi-point strategy for asynchronous batch generations (explained later)\n    \"acq_optimizer\": \"mixedga\",  # Use continuous Genetic Algorithm for the acquisition function optimizer\n    \"acq_optimizer_freq\": 1,  # Frequency of the acquisition function optimizer (1 = each new batch generation) increasing this value can help amortize the computational cost of acquisition function optimization\n    \"filter_duplicated\": False,  # Deactivate filtration of duplicated new points\n    \"kappa\": 10.0,  # Initial value of exploration-exploitation parameter for the acquisition function\n    \"scheduler\": {  # Scheduler for the exploration-exploitation parameter \"kappa\"\n        \"type\": \"periodic-exp-decay\",  # Periodic exponential decay\n        \"period\": 50,  # Period over which the decay is applied. It is useful to escape local solutions.\n        \"kappa_final\": 0.001,  # Value of kappa at the end of each \"period\"\n    },\n    \"objective_scaler\": \"identity\",\n    \"random_state\": 42,  # Random seed\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we create the search instance.\n\nFor this we pass the hyperparameter ``problem``, the ``evaluator`` and also a ``stopper`` (optional).\n\nThe ``problem`` is the instance of :class:`deephyper.hpo.HpProblem` that we defined in previous sections.\n\nThe ``evaluator`` is a subclass of :class:`deephyper.evaluator.Evaluator` that provides a ``.submit(...)`` method and a ``.gather(...)`` method to\nsubmit and gather asynchronous evaluations.\n\nThe ``stopper`` is an optional parameter that allows to use an early-discarding (a.k.a., multi-fidelity) strategy to stop early low performing evaluations.\nIn our case we will use the median early-discarding strategy. \nThis strategy consists in early stopping the training if the observed objective at the current budget is worse than the median objective for the same budget.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.evaluator import Evaluator\nfrom deephyper.evaluator.callback import TqdmCallback\nfrom deephyper.hpo import CBO\nfrom deephyper.stopper import MedianStopper\n\n\nhpo_dir = \"nas_regression\"\n\n\ndef run_neural_architecture_search(problem, max_evals):\n    model_checkpoint_dir = os.path.join(hpo_dir, \"models\")\n    pathlib.Path(model_checkpoint_dir).mkdir(parents=True, exist_ok=True)\n\n    method_kwargs = {\n        \"run_function_kwargs\": {\n            \"model_checkpoint_dir\": model_checkpoint_dir,\n            \"verbose\": False,\n        },\n        \"callbacks\": [TqdmCallback()],\n    }\n\n    if device == \"cuda\":\n        method_kwargs.update({\n            \"num_cpus\": device_count,\n            \"num_gpus\": device_count,\n            \"num_cpus_per_task\": 1,\n            \"num_gpus_per_task\": 1,\n        })\n    else:\n        method_kwargs.update({\n            \"num_cpus\": device_count,\n            \"num_cpus_per_task\": 1,\n        })\n    \n\n    evaluator = Evaluator.create(\n        run,\n        method=\"ray\",  \n        method_kwargs=method_kwargs,\n    )\n\n    stopper = None\n    \n    # Uncomment the following to speed-up the search\n    # stopper = MedianStopper(min_steps=50, max_steps=max_n_epochs, interval_steps=50)\n\n    search = CBO(problem, evaluator, log_dir=hpo_dir, stopper=stopper, **search_kwargs)\n\n    results = search.search(max_evals=max_evals)\n\n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can download precomputed results if you want to skip the slow neural architecture search. We provide the following two set of precomputed results:\n\n- Link to precomputed results without stopper: ``https://drive.google.com/uc?id=1VOV-UM0ws0lopHvoYT_9RAiRdT1y4Kus``\n- Link to precomputed results with median stopper: ``https://drive.google.com/uc?id=1I09-ZaH4BzQfBOw6YmhzgKLFWBsdrvpg``\n\nThen run the following commands and adapt the url:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\npip install gdown  # Install if necessary\ngdown \"https://drive.google.com/uc?id=1VOV-UM0ws0lopHvoYT_9RAiRdT1y4Kus\"\ntar -xvf nas_regression.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to remove previously computed results run the following command:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\nrm -rf nas_regression/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the search can take some time to finalize we provide a mechanism that checks if results were already computed and skip \nthe neural architecture search if it is the case.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_evals = 250\n\nhpo_results = None\nhpo_results_path = os.path.join(hpo_dir, \"results.csv\")\nif os.path.exists(hpo_results_path):\n    print(\"Reloading results...\")\n    hpo_results = pd.read_csv(hpo_results_path)\n\nif hpo_results is None or len(hpo_results) < max_evals:\n    print(\"Running neural architecture search...\")\n    hpo_results = run_neural_architecture_search(problem, max_evals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of the results\n\nWe will now look at the results of the search globally in term of evolution of the objective and worker's activity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.analysis.hpo import plot_search_trajectory_single_objective_hpo\nfrom deephyper.analysis.hpo import plot_worker_utilization\n\n\nfig, axes = plt.subplots(\n    nrows=2,\n    ncols=1,\n    sharex=True,\n    figsize=(WIDTH_PLOTS, HEIGHT_PLOTS),\n)\n\n_ = plot_search_trajectory_single_objective_hpo(\n    hpo_results,\n    mode=\"min\",\n    x_units=\"seconds\",\n    ax=axes[0],\n)\naxes[0].set_yscale(\"log\")\n\n_ = plot_worker_utilization(\n    hpo_results,\n    profile_type=\"submit/gather\",\n    ax=axes[1],\n)\n\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we split results between successful and failed results if there are some.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.analysis.hpo import filter_failed_objectives\n\n\nhpo_results, hpo_results_failed = filter_failed_objectives(hpo_results)\n\nhpo_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We look at the learning curves of the best model and observe improvements in both training and validation loss:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown: Make learning curves plot\nx_values = np.arange(1, len(baseline_results[\"metadata\"][\"train_loss\"]) + 1)\nx_min, x_max = x_values.min(), x_values.max()\n_ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\n_ = plt.plot(\n    x_values,\n    baseline_results[\"metadata\"][\"train_loss\"],\n    linestyle=\":\",\n    label=\"Baseline Training\",\n)\n_ = plt.plot(\n    x_values,\n    baseline_results[\"metadata\"][\"val_loss\"],\n    linestyle=\":\",\n    label=\"Baseline Validation\",\n)\n\ni_max = hpo_results[\"objective\"].argmax()\ntrain_loss = json.loads(hpo_results.iloc[i_max][\"m:train_loss\"])\nval_loss = json.loads(hpo_results.iloc[i_max][\"m:val_loss\"])\nx_values = np.arange(1, len(train_loss) + 1)\nx_max = max(x_max, x_values.max())\n_ = plt.plot(\n    x_values,\n    train_loss,\n    alpha=0.8,\n    linestyle=\"--\",\n    label=\"Best Training\",\n)\n_ = plt.plot(\n    x_values,\n    val_loss,\n    alpha=0.8,\n    linestyle=\"--\",\n    label=\"Best Validation\",\n)\n_ = plt.xlim(x_min, x_max)\n_ = plt.grid(which=\"both\", linestyle=\":\")\n_ = plt.legend()\n_ = plt.xlabel(\"Epochs\")\n_ = plt.ylabel(\"NLL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we look at predictions of this best model and observe that it manage to predict much better than the baseline one the right range. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.analysis.hpo import parameters_from_row\n\n\nhpo_dir = \"nas_regression\"\nmodel_checkpoint_dir = os.path.join(hpo_dir, \"models\")\njob_id = hpo_results.iloc[i_max][\"job_id\"]\nfile_name = f\"model_0.{job_id}.pt\"\n\nweights_path = os.path.join(model_checkpoint_dir, file_name)\nparameters = parameters_from_row(hpo_results.iloc[i_max])\n\ntorch_module = create_model(parameters, y_mu, y_std)\n\ntorch_module.load_state_dict(torch.load(weights_path, weights_only=True))\ntorch_module.eval()\n\ny_pred = torch_module.forward(to_torch(test_X))\ny_pred_mean = to_numpy(y_pred.loc)\ny_pred_std = to_numpy(y_pred.scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Make prediction plot\nkappa = 1.96\n_ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\n_ = plt.scatter(train_X, train_y, s=5, label=\"Training\")\n_ = plt.scatter(valid_X, valid_y, s=5, label=\"Validation\")\n_ = plt.plot(test_X, test_y, linestyle=\"--\", color=\"gray\", label=\"Test\")\n\n_ = plt.plot(test_X, y_pred_mean, label=r\"$\\mu(x)$\")\n_ = plt.fill_between(\n    test_X.reshape(-1),\n    (y_pred_mean - kappa * y_pred_std).reshape(-1),\n    (y_pred_mean + kappa * y_pred_std).reshape(-1),\n    alpha=0.25,\n    label=r\"$\\sigma_\\text{al}(x)$\",\n)\n_ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.25)\n_ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.25)\n_ = plt.xlim(-x_lim, x_lim)\n_ = plt.ylim(-y_lim, y_lim)\n_ = plt.legend(ncols=2)\n_ = plt.xlabel(r\"$x$\")\n_ = plt.ylabel(r\"$f(x)$\")\n_ = plt.grid(which=\"both\", linestyle=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep ensemble\n\nAfter running the neural architecture search we have an available library of checkpointed models.\nFrom this section, you will learn how to combine these models to form an ensemble that can improve both accuracy and provide disentangled uncertainty quantification.\n\nWe start by importing classes from :mod:`deephyper.predictor` and :mod:`deephyper.ensemble`.\n\nThe :mod:`deephyper.predictor` module includes subclasses of :class:`deephyper.predictor.Predictor` to wrap predictive models ready for inference. In our case, we will use :class:`deephyper.predictor.torch.TorchPredictor`.\nThe :mod:`deephyper.ensemble` module includes modular components to build an ensemble of predictive models.\nThe ensemble module is organized around loss functions, aggregation functions and selection algorithms.\nThe implementation of these functions is based on Numpy.\nIn this example, we start by wrapping our torch module within a subclass of :class:`deephyper.predictor.torch.TorchPredictor` that we call ``NormalTorchPredictor``. This predictor class is used to make a torch module compatible with our Numpy-based implementation for ensembles.\n\nThe ``pre_process_inputs`` is used to map a Numpy array to a Torch tensor.\nThe ``post_process_predictions`` is used to map a Torch tensor to a Numpy array.\nIt also formats the prediction as a dictionnary with ``\"loc\"`` (for the predictive mean) and ``\"scale\"`` (for the predictive standard deviation) that is necessary for our aggregation function ``MixedNormalAggregator``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.ensemble import EnsemblePredictor\nfrom deephyper.ensemble.aggregator import MixedNormalAggregator\nfrom deephyper.ensemble.loss import NormalNegLogLikelihood\nfrom deephyper.ensemble.selector import GreedySelector, TopKSelector\nfrom deephyper.predictor.torch import TorchPredictor\n\nclass NormalTorchPredictor(TorchPredictor):\n    def __init__(self, torch_module):\n        super().__init__(torch_module.to(device=device, dtype=dtype))\n\n    def pre_process_inputs(self, X):\n        return to_torch(X)\n\n    def post_process_predictions(self, y):\n        return {\n            \"loc\": to_numpy(y.loc),\n            \"scale\": to_numpy(y.scale),\n        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After defining the predictor, we load the checkpointed models to collect their predictions into ``y_predictors``.\nThese predictions are the inputs of our loss, aggregation and selection functions.\nWe also collect the job ids of the checkpointed models into ``job_id_predictors``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_checkpoint_dir = os.path.join(hpo_dir, \"models\")\n\ny_predictors = []\njob_id_predictors = []\n\nfor file_name in tqdm(os.listdir(model_checkpoint_dir)):\n    if not file_name.endswith(\".pt\"):\n        continue\n\n    weights_path = os.path.join(model_checkpoint_dir, file_name)\n    job_id = int(file_name[6:-3].split(\".\")[-1])\n\n    row = hpo_results[hpo_results[\"job_id\"] == job_id]\n    if len(row) == 0:\n        continue\n    assert len(row) == 1\n\n    row = row.iloc[0]\n    parameters = parameters_from_row(row)\n    torch_module = create_model(parameters, y_mu, y_std)\n    try:\n        torch_module.load_state_dict(torch.load(weights_path, weights_only=True))\n    except RuntimeError:\n        continue\n\n    predictor = NormalTorchPredictor(torch_module)\n    y_pred = predictor.predict(valid_X)\n    y_predictors.append(y_pred)\n    job_id_predictors.append(job_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble selection\n\nThis is where the ensemble selection logic happens. We use the :class:`deephyper.ensemble.selector.GreedySelector` or :class:`deephyper.ensemble.selector.TopKSelector` class.\nThe top-k selection, selects the topk-k models according to the given ``los_func`` and weight them equally in the ensemble.\nThe greedy selection, iteratively selects models from the checkpoints that improves the current ensemble.\n\nThe ``aggregator`` is the logic that combines a set of predictors into a single predictor to form the ensemble's prediction.\nIn our case, we use the :class:`deephyper.ensemble.aggregator.MixedNormalAggregator` that approximates a mixture of normal distribution (each normal distribution is the output of a checkpointed model) as a normal distribution.\n\nTo try top-k or greedy selection just uncomment/comment the corresponding code.\nThis part of the code is fast to compute.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "k = 50\n\n# Top-K Selection\n# selector = TopKSelector(\n#     loss_func=NormalNegLogLikelihood(),\n#     k=k,\n# )\n\n# Greedy Selection\nselector = GreedySelector(\n    loss_func=NormalNegLogLikelihood(),\n    aggregator=MixedNormalAggregator(),\n    k=k,\n    max_it=k,\n    k_init=3,\n    early_stopping=True,\n    with_replacement=True,\n    bagging=True,\n    verbose=True,\n)\n\nselected_predictors_indexes, selected_predictors_weights = selector.select(\n    valid_y,\n    y_predictors,\n)\n\nprint(f\"{selected_predictors_indexes=}\")\nprint(f\"{selected_predictors_weights=}\")\n\nselected_predictors_job_ids = np.array(job_id_predictors)[selected_predictors_indexes]\nselected_predictors_job_ids\n\nprint(f\"{selected_predictors_job_ids=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation of the ensemble\n\nNow that we have a set of predictors with their corresponding weights in the ensemble we can look at the predictions.\nFor this, we use the :class:`deephyper.ensemble.EnsemblePredictor` class.\nThis class can use the :class:`deephyper.evaluator.Evaluator` to parallelize the inference of ensemble members.\nThen, we need to give it the list of ``predictors``, ``weights`` and the ``aggregator``.\nFor inference, we set ``decomposed_scale=True`` for the :class:`deephyper.ensemble.aggregator.MixedNormalAggregator` as we want\nto predict disentangled epistemic and aleatoric uncertainty using the law of total variance:\n\n\\begin{align}V_Y[Y|X=x] = \\underbrace{E_\\Theta\\left[V_Y[Y|X=x;\\Theta\\right]}_\\text{Aleatoric Uncertainty} + \\underbrace{V_\\Theta\\left[E_Y[Y|X=x;\\Theta]\\right]}_\\text{Epistemic Uncertainty}\\end{align}\n\nwhere $\\Theta$ is the random variable that represents a concatenation of weights and hyperparameters, $Y$` is the random variable representing a target prediction, and $X$ is the random variable representing an observed input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predictors = []\n\nhpo_dir = \"nas_regression\"\nmodel_checkpoint_dir = os.path.join(hpo_dir, \"models\")\n\nfor job_id in selected_predictors_job_ids:\n    file_name = f\"model_0.{job_id}.pt\"\n\n    weights_path = os.path.join(model_checkpoint_dir, file_name)\n\n    row = hpo_results[hpo_results[\"job_id\"] == job_id].iloc[0]\n    parameters = parameters_from_row(row)\n    torch_module = create_model(parameters, y_mu, y_std)\n    torch_module.load_state_dict(torch.load(weights_path, weights_only=True))\n    predictor = NormalTorchPredictor(torch_module)\n    predictors.append(predictor)\n\nensemble = EnsemblePredictor(\n    predictors=predictors,\n    weights=selected_predictors_weights,\n    aggregator=MixedNormalAggregator(decomposed_scale=True),\n)\n\ny_pred = ensemble.predict(test_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the visualization, we can first observe that the mean prediction is close to the true function.\n\nThen, to visualize both uncertainties together we plot the variance.\nThe goal is to observe the epistemic component vanish in areas where we observed data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Make uncertainty plot\n_ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\n_ = plt.scatter(train_X, train_y, s=5, label=\"Training\")\n_ = plt.scatter(valid_X, valid_y, s=5, label=\"Validation\")\n_ = plt.plot(test_X, test_y, linestyle=\"--\", color=\"gray\", label=\"Test\")\n_ = plt.plot(test_X, y_pred[\"loc\"], label=r\"$\\mu(x)$\")\n_ = plt.fill_between(\n    test_X.reshape(-1),\n    (y_pred[\"loc\"] - y_pred[\"scale_aleatoric\"]**2).reshape(-1),\n    (y_pred[\"loc\"] + y_pred[\"scale_aleatoric\"]**2).reshape(-1),\n    alpha=0.25,\n    label=r\"$\\sigma_\\text{al}^2(x)$\",\n)\n_ = plt.fill_between(\n    test_X.reshape(-1),\n    (y_pred[\"loc\"] - y_pred[\"scale_aleatoric\"]**2).reshape(-1),\n    (y_pred[\"loc\"] - y_pred[\"scale_aleatoric\"]**2 - y_pred[\"scale_epistemic\"]**2).reshape(-1),\n    alpha=0.25,\n    color=\"red\",\n    label=r\"$\\sigma_\\text{ep}^2(x)$\",\n)\n_ = plt.fill_between(\n    test_X.reshape(-1),\n    (y_pred[\"loc\"] + y_pred[\"scale_aleatoric\"]**2).reshape(-1),\n    (y_pred[\"loc\"] + y_pred[\"scale_aleatoric\"]**2 + y_pred[\"scale_epistemic\"]**2).reshape(-1),\n    alpha=0.25,\n    color=\"red\",\n)\n_ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.25)\n_ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.25)\n_ = plt.xlim(-x_lim, x_lim)\n_ = plt.ylim(-y_lim, y_lim)\n_ = plt.legend(ncols=2)\n_ = plt.xlabel(r\"$x$\")\n_ = plt.ylabel(r\"$f(x)$\")\n_ = plt.grid(which=\"both\", linestyle=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aleatoric Uncertainty\n\nNow, if we isolate the aleatoric uncertainty we observe that we somewhat correctly estimated the lower aleatoric uncertainty on the left side, and larger on the right side.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Make aleatoric uncertainty plot\nkappa = 1.96\n_ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\n_ = plt.scatter(train_X, train_y, s=5, label=\"Training\")\n_ = plt.scatter(valid_X, valid_y, s=5, label=\"Validation\")\n_ = plt.plot(test_X, test_y, linestyle=\"--\", color=\"gray\", label=\"Test\")\n_ = plt.plot(test_X, y_pred[\"loc\"], label=r\"$\\mu(x)$\")\n_ = plt.fill_between(\n    test_X.reshape(-1),\n    (y_pred[\"loc\"] - kappa * y_pred[\"scale_aleatoric\"]).reshape(-1),\n    (y_pred[\"loc\"] + kappa * y_pred[\"scale_aleatoric\"]).reshape(-1),\n    alpha=0.25,\n    label=r\"$\\sigma_\\text{al}(x)$\",\n)\n_ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.25)\n_ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.25)\n_ = plt.xlim(-x_lim, x_lim)\n_ = plt.ylim(-y_lim, y_lim)\n_ = plt.legend(ncols=2)\n_ = plt.xlabel(r\"$x$\")\n_ = plt.ylabel(r\"$f(x)$\")\n_ = plt.grid(which=\"both\", linestyle=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Epistemic uncertainty\n\nFinally, if we isole the epistemic uncertainty we observe that it vanishes in the grey areas where we observed data and grows in areas were we did not have data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Make epistemic uncertainty plot\nkappa = 1.96\n_ = plt.figure(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\n_ = plt.scatter(train_X, train_y, s=5, label=\"Training\")\n_ = plt.scatter(valid_X, valid_y, s=5, label=\"Validation\")\n_ = plt.plot(test_X, test_y, linestyle=\"--\", color=\"gray\", label=\"Test\")\n_ = plt.plot(test_X, y_pred[\"loc\"], label=r\"$\\mu(x)$\")\n_ = plt.fill_between(\n    test_X.reshape(-1),\n    (y_pred[\"loc\"] - kappa * y_pred[\"scale_epistemic\"]).reshape(-1),\n    (y_pred[\"loc\"] + kappa * y_pred[\"scale_epistemic\"]).reshape(-1),\n    alpha=0.25,\n    color=\"red\",\n    label=r\"$\\sigma_\\text{ep}(x)$\",\n)\n_ = plt.fill_between([-30, -15], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.25)\n_ = plt.fill_between([15, 30], [-y_lim, -y_lim], [y_lim, y_lim], color=\"gray\", alpha=0.25)\n_ = plt.xlim(-x_lim, x_lim)\n_ = plt.ylim(-y_lim, y_lim)\n_ = plt.legend(ncols=2)\n_ = plt.xlabel(r\"$x$\")\n_ = plt.ylabel(r\"$f(x)$\")\n_ = plt.grid(which=\"both\", linestyle=\":\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}