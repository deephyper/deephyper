{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Scaling Bayesian Optimization with Heterogeneous Parallelism\n\n**Author(s)**: Romain Egele.\n\nThis example demonstrates the advantages of mixing centralized parallelism (1 optimization process with N workers)\nwith decentralized parallelism (N optimization processes) to scale Bayesian optimization. For this we will have\na total of 1000 local workers simulated with threads and timeouts.\n\nIn this example, we will start by demonstrating the behaviour of an efficient centralized bayesian optimization using 1000 workers.\nThen, we will run a mixed decentralized optimization with 10 replications of a centralized optimization each with 100 workers for a\ntotal of 1000 workers as well.\n\nTherefore, we start by defining a black-box ``run``-function that implements the Ackley function:\n\n<img src=\"https://www.sfu.ca/~ssurjano/ackley.png\" width=\"400\" alt=\"Ackley Function in 2D\">\n\nTo help illustrate the parallelization gain, we will simulate a computational cost\nby using ``time.sleep``. We also use the ``@profile`` decorator to collect starting/ending\ntimes of each call to the ``run``-function. When using this decorator, the ``run``-function will\nreturn a dictionnary including ``\"metadata\"`` with 2 new keys ``\"timestamp_start\"`` and\n``\"timestamp_end\"``. The ``run``-function is defined in a separate Python module\nfor better serialization (through ``pickle``) in case other parallel backends such as ``\"process\"`` would be used\n\n.. literalinclude:: ../../examples/black_box_util.py\n   :language: python\n\nAfter defining the ``run``-function we can continue with the definition of our optimization script:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few useful imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import black_box_util as black_box\nimport matplotlib.pyplot as plt\nimport scipy.stats as ss\n\nfrom multiprocessing import Pool\n\nfrom deephyper.analysis import figure_size\nfrom deephyper.analysis.hpo import plot_search_trajectory_single_objective_hpo\nfrom deephyper.analysis.hpo import plot_worker_utilization\nfrom deephyper.evaluator import Evaluator\nfrom deephyper.evaluator.callback import TqdmCallback\nfrom deephyper.evaluator.storage import SharedMemoryStorage\nfrom deephyper.hpo import HpProblem, CBO\n\nfrom dbo_util import execute_centralized_bo_with_share_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define the variable(s) we want to optimize. For this problem we\noptimize Ackley in a N-dimensional search space. Each dimension in the continuous range\n[-32.768, 32.768]. The true minimum is located at ``(0, ..., 0)``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nb_dim = 5\nproblem = HpProblem()\nfor i in range(nb_dim):\n    problem.add_hyperparameter((-32.768, 32.768), f\"x{i}\")\nproblem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define some default search parameters for the Bayesian Optimization algorithm.\nIt is important to note the `\"qUCBd\"` parameter for the multi-point strategy. Using the\nclassic constant-liar strategy (a.k.a, Krigging Believer) `\"cl_min/max/mean` in our setting\nwould totally freeze the execution (you can try!).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search_kwargs = {\n    \"n_initial_points\": 2 * nb_dim + 1,  # Number of initial random points\n    \"surrogate_model\": \"ET\",  # Use Extra Trees as surrogate model\n    \"surrogate_model_kwargs\": {\n        \"n_estimators\": 25,  # Relatively small number of trees in the surrogate to make it \"fast\"\n        \"min_samples_split\": 8,  # Larger number to avoid small leaf nodes (smoothing the response)\n    },\n    \"multi_point_strategy\": \"qUCBd\",  # Multi-point strategy for asynchronous batch generations (explained later)\n    \"acq_optimizer\": \"ga\",  # Use continuous Genetic Algorithm for the acquisition function optimizer\n    \"acq_optimizer_freq\": 1,  # Frequency of the acquisition function optimizer (1 = each new batch generation) increasing this value can help amortize the computational cost of acquisition function optimization\n    \"filter_duplicated\": False,  # Deactivate filtration of duplicated new points\n    \"kappa\": 10.0,  # Initial value of exploration-exploitation parameter for the acquisition function\n    \"scheduler\": {  # Scheduler for the exploration-exploitation parameter \"kappa\"\n        \"type\": \"periodic-exp-decay\",  # Periodic exponential decay\n        \"period\": 50,  # Period over which the decay is applied. It is useful to escape local solutions.\n        \"kappa_final\": 0.001,  # Value of kappa at the end of each \"period\"\n    },\n    \"random_state\": 42,  # Random seed\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define the time budget for the optimization. The time budget is defined in seconds.\nThe `sleep_loc` and `sleep_scale` parameters simulate the distribution of duration of evaluated\nblack-box functions sampled from a normal law with mean `sleep_loc` and standard deviation `sleep_scale`.\nWe also define here the total number of workers to 1000.\nUsing so many workers for Bayesian optimization is quite rare. Usually it is limited to ~200 sequential\niterations and a dozen of parallel workers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "timeout = 30\nnum_workers = 1_000\nrun_function_kwargs = dict(sleep_loc=1, sleep_scale=0.25)\nresults = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a utility function to preprocess our results before plotting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def preprocess_results(results):\n    results = results.dropna().copy()\n    offset = results[\"m:timestamp_start\"].min()\n    results[\"m:timestamp_start\"] -= offset\n    results[\"m:timestamp_end\"] -= offset\n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can create a centralized parallel search with .\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def execute_centralized_bo(\n    problem, run_function, run_function_kwargs, num_workers, log_dir, search_kwargs, timeout\n):\n    evaluator = Evaluator.create(\n        run_function,\n        method=\"thread\",\n        method_kwargs={\n            \"num_workers\": num_workers,  # For the parallel evaluations\n            \"callbacks\": [TqdmCallback()],\n            \"run_function_kwargs\": run_function_kwargs,\n        },\n    )\n    search = CBO(\n        problem,\n        evaluator,\n        log_dir=log_dir,\n        **search_kwargs,\n    )\n    results = search.search(timeout=timeout)\n\n    results = preprocess_results(results)\n\n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To execute the search, we use the `if __name__ == \"__main__\":` statement. It is important to avoid triggering searches\nrecursively when launching child processes latter in the example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    results[\"centralized\"] = execute_centralized_bo(\n        problem=problem,\n        run_function=black_box.run_ackley,\n        run_function_kwargs=run_function_kwargs,\n        num_workers=num_workers,\n        log_dir=\"search_centralized\",\n        search_kwargs=search_kwargs,\n        timeout=timeout,\n    )\n    results[\"centralized\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot the results of the centralized search. The first plot shows the evolution of the objective.\nThe second plot shows the utilization of the worker over time.\nThe drops of the function show the regular re-fit of the surrogate model and optimization of the acquisition function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    fig, axes = plt.subplots(\n        nrows=2,\n        ncols=1,\n        sharex=True,\n        figsize=figure_size(width=600),\n    )\n\n    plot_search_trajectory_single_objective_hpo(\n        results[\"centralized\"], mode=\"min\", x_units=\"seconds\", ax=axes[0]\n    )\n\n    plot_worker_utilization(\n        results[\"centralized\"], num_workers=None, profile_type=\"start/end\", ax=axes[1]\n    )\n\n    plt.tight_layout()\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we move to the decentralized optimization. We defined it in a separate module for Pickling to work.\nThis function will be launched in child processes to trigger each sub-instances of the decentralized search.\n\n.. literalinclude:: ../../examples/dbo_util.py\n   :language: python\n\nThis function is very similar to our previous centralized optimization. However, importantly you can see that we\noverride two methods of the `CBO` with `dummy` function.\n\nNow we can define the decentralized search using that launches each centralized instance with `multiprocessing.Pool`.\nImportantly we use `SharedMemoryStorage` so that centralized sub-instances can communicate globally.\nWe also create explicitely the `search_id` to make sure they communicate about the search Search instance.\nThe `kappa` value is sampled from an Exponential distribution to enable a diverse set of exploration-exploitation trade-offs\nleading to better results. You can try to fix it to `1.96` instead (default parameter in DeepHyper).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def execute_decentralized_bo(\n    problem,\n    run_function,\n    run_function_kwargs,\n    num_workers,\n    log_dir,\n    search_kwargs,\n    timeout,\n    n_processes,\n):\n    storage = SharedMemoryStorage()\n    search_id = storage.create_new_search()\n    kappa = ss.expon.rvs(\n        size=n_processes, scale=search_kwargs[\"kappa\"], random_state=search_kwargs[\"random_state\"]\n    )\n    with Pool(processes=n_processes) as pool:\n        results = pool.starmap(\n            execute_centralized_bo_with_share_memory,\n            [\n                (\n                    problem,\n                    run_function,\n                    run_function_kwargs,\n                    storage,\n                    search_id,\n                    i,\n                    log_dir,\n                    num_workers // n_processes,\n                    i == 0,\n                    kappa[i],\n                    search_kwargs,\n                    timeout,\n                )\n                for i in range(n_processes)\n            ],\n        )\n\n    results = preprocess_results(results[0])\n\n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now execute the decentralized optimization with 10 processes each using 100 workers:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    results[\"decentralized\"] = execute_decentralized_bo(\n        problem=problem,\n        run_function=black_box.run_ackley,\n        run_function_kwargs=run_function_kwargs,\n        num_workers=num_workers,\n        log_dir=\"search_centralized\",\n        search_kwargs=search_kwargs,\n        timeout=timeout,\n        n_processes=10,\n    )\n    results[\"decentralized\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the results we can see a better objective and less intance drops in worker utilization:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    fig, axes = plt.subplots(\n        nrows=2,\n        ncols=1,\n        sharex=True,\n        figsize=figure_size(width=600),\n    )\n\n    plot_search_trajectory_single_objective_hpo(\n        results[\"decentralized\"], mode=\"min\", x_units=\"seconds\", ax=axes[0]\n    )\n\n    plot_worker_utilization(\n        results[\"decentralized\"], num_workers=None, profile_type=\"start/end\", ax=axes[1]\n    )\n\n    plt.tight_layout()\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we compare the objective curves side by side we can see the improvement of decentralized\noptimization even better.\nEven if the total number of evaluations is less, the quality is much better as we infered more\noften the surrogate model (combining re-fitting and optimization of the acquisition function):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    # sphinx_gallery_thumbnail_number = 3\n    fig, ax = plt.subplots(figsize=figure_size(width=600))\n    labels = {\n        \"centralized\": \"Centralized Bayesian Optimization\",\n        \"decentralized\": \"Decentralized Bayesian Optimization\",\n    }\n\n    x_min = float(\"inf\")\n    x_max = -float(\"inf\")\n    for i, (key, label) in enumerate(labels.items()):\n        df = results[key]\n        plot_search_trajectory_single_objective_hpo(\n            df,\n            show_failures=False,\n            mode=\"min\",\n            x_units=\"seconds\",\n            ax=ax,\n            label=label,\n            plot_kwargs={\"color\": f\"C{i}\"},\n            scatter_success_kwargs={\"color\": f\"C{i}\", \"alpha\": 0.5},\n        )\n        x_min = min(df[\"m:timestamp_start\"].min(), x_min)\n        x_max = max(df[\"m:timestamp_end\"].max(), x_max)\n\n    ax.set_xlim(x_min, x_max)\n\n    plt.xlabel(\"Time (sec.)\")\n    plt.ylabel(\"Objective\")\n    plt.yscale(\"log\")\n    plt.grid(visible=True, which=\"minor\", linestyle=\":\")\n    plt.grid(visible=True, which=\"major\", linestyle=\"-\")\n    plt.legend()\n    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}