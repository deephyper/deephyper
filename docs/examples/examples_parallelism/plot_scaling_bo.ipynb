{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Scaling Bayesian Optimization with Heterogeneous Parallelism\n\n**Author(s)**: Romain Egele.\n\nThis example demonstrates the advantages of mixing centralized parallelism (1 optimization process with N workers)\nwith decentralized parallelism (N optimization processes) to scale Bayesian optimization. For this we will have\na total of 1000 local workers simulated with threads and timeouts.\n\nIn this example, we will start by demonstrating the behaviour of an efficient centralized bayesian optimization using 1000 workers.\nThen, we will run a mixed decentralized optimization with 10 replications of a centralized optimization each with 100 workers for a\ntotal of 1000 workers as well.\n\nTherefore, we start by defining a black-box ``run``-function that implements the Ackley function:\n\n<img src=\"https://www.sfu.ca/~ssurjano/ackley.png\" width=\"400\" alt=\"Ackley Function in 2D\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Import statements\nimport multiprocessing\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as ss\n\nfrom loky import get_reusable_executor\n\nfrom deephyper.analysis import figure_size\nfrom deephyper.analysis.hpo import plot_search_trajectory_single_objective_hpo\nfrom deephyper.analysis.hpo import plot_worker_utilization\nfrom deephyper.evaluator import Evaluator, profile\nfrom deephyper.evaluator.callback import TqdmCallback\nfrom deephyper.evaluator.storage import SharedMemoryStorage\nfrom deephyper.hpo import HpProblem, CBO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the Ackley function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Ackley function\ndef ackley(x, a=20, b=0.2, c=2 * np.pi):\n    d = len(x)\n    s1 = np.sum(x**2)\n    s2 = np.sum(np.cos(c * x))\n    term1 = -a * np.exp(-b * np.sqrt(s1 / d))\n    term2 = -np.exp(s2 / d)\n    y = term1 + term2 + a + np.exp(1)\n    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the ``time.sleep`` function to simulate a budget of 2 secondes of execution in average \nwhich helps illustrate the advantage of parallel evaluations. The ``@profile`` decorator is useful \nto collect starting/ending time of the ``run``-function execution which help us know exactly when \nwe are inside the black-box. This decorator is necessary when profiling the worker utilization. When \nusing this decorator, the ``run``-function will return a dictionnary with 2 new keys ``\"timestamp_start\"`` \nand ``\"timestamp_end\"``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@profile\ndef run_ackley(config, sleep_loc=2, sleep_scale=0.5):\n    # to simulate the computation of an expensive black-box\n    if sleep_loc > 0:\n        t_sleep = np.random.normal(loc=sleep_loc, scale=sleep_scale)\n        t_sleep = max(t_sleep, 0)\n        time.sleep(t_sleep)\n\n    x = np.array([config[k] for k in config if \"x\" in k])\n    x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n    return -ackley(x)  # maximisation is performed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define the variable(s) we want to optimize. For this problem we\noptimize Ackley in a N-dimensional search space. Each dimension in the continuous range\n[-32.768, 32.768]. The true minimum is located at ``(0, ..., 0)``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nb_dim = 10\nproblem = HpProblem()\nfor i in range(nb_dim):\n    problem.add_hyperparameter((-32.768, 32.768), f\"x{i}\")\nproblem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define some default search parameters for the Bayesian Optimization algorithm.\nIt is important to note the `\"qUCBd\"` parameter for the multi-point strategy. Using the\nclassic constant-liar strategy (a.k.a, Krigging Believer) `\"cl_min/max/mean` in our setting\nwould totally freeze the execution (you can try!).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search_kwargs = {\n    \"acq_func_kwargs\": {\n        \"kappa\": 2.0,\n    },\n    \"acq_optimizer\": \"ga\",  # Use continuous Genetic Algorithm for the acquisition function optimizer\n    \"acq_optimizer_kwargs\": {\n        \"filter_duplicated\": False, # Deactivate filtration of duplicated new points\n    },\n    \"multi_point_strategy\": \"qUCBd\",  # Multi-point strategy for asynchronous batch generations (explained later)\n    \"random_state\": 42,  # Random seed\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define the time budget for the optimization. The time budget is defined in seconds.\nThe `sleep_loc` and `sleep_scale` parameters simulate the distribution of duration of evaluated\nblack-box functions sampled from a normal law with mean `sleep_loc` and standard deviation `sleep_scale`.\nWe also define here the total number of workers to 1000.\nUsing so many workers for Bayesian optimization is quite rare. Usually it is limited to ~200 sequential\niterations and a dozen of parallel workers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "timeout = 30\nnum_workers = 1_000\nrun_function_kwargs = dict(sleep_loc=1, sleep_scale=0.25)\nresults = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a utility function to preprocess our results before plotting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def preprocess_results(results):\n    results = results.dropna().copy()\n    offset = results[\"m:timestamp_start\"].min()\n    results[\"m:timestamp_start\"] -= offset\n    results[\"m:timestamp_end\"] -= offset\n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can create a centralized parallel search with .\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def execute_centralized_bo(\n    problem, run_function, run_function_kwargs, num_workers, log_dir, search_kwargs, timeout\n):\n    evaluator = Evaluator.create(\n        run_function,\n        method=\"thread\",\n        method_kwargs={\n            \"num_workers\": num_workers,  # For the parallel evaluations\n            \"callbacks\": [TqdmCallback()],\n            \"run_function_kwargs\": run_function_kwargs,\n        },\n    )\n    search = CBO(\n        problem,\n        evaluator,\n        log_dir=log_dir,\n        **search_kwargs,\n    )\n    results = search.search(timeout=timeout)\n\n    results = preprocess_results(results)\n\n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To execute the search, we use the ``if __name__ == \"__main__\":`` statement. It is important to avoid triggering searches\nrecursively when launching child processes latter in the example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    results[\"centralized\"] = execute_centralized_bo(\n        problem=problem,\n        run_function=run_ackley,\n        run_function_kwargs=run_function_kwargs,\n        num_workers=num_workers,\n        log_dir=\"search_centralized\",\n        search_kwargs=search_kwargs,\n        timeout=timeout,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot the results of the centralized search. The first plot shows the evolution of the objective.\nThe second plot shows the utilization of the worker over time.\nThe drops of the function show the regular re-fit of the surrogate model and optimization of the acquisition function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Make plot\nif __name__ == \"__main__\":\n    fig, axes = plt.subplots(\n        nrows=2,\n        ncols=1,\n        sharex=True,\n        figsize=figure_size(width=600),\n        tight_layout=True,\n    )\n\n    fig, ax = plot_search_trajectory_single_objective_hpo(\n        results[\"centralized\"], mode=\"min\", x_units=\"seconds\", ax=axes[0]\n    )\n\n    fig, ax = plot_worker_utilization(\n        results[\"centralized\"], num_workers=None, profile_type=\"start/end\", ax=axes[1]\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we move to the decentralized optimization. We defined it in a separate module for Pickling to work.\nThis function will be launched in child processes to trigger each sub-instances of the decentralized search.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def execute_centralized_bo_with_share_memory(\n    problem,\n    run_function,\n    run_function_kwargs,\n    storage,\n    search_id,\n    search_random_state,\n    log_dir,\n    num_workers,\n    is_master,\n    kappa,\n    search_kwargs,\n    timeout,\n):\n    evaluator = Evaluator.create(\n        run_function,\n        method=\"thread\",\n        method_kwargs={\n            \"num_workers\": num_workers,\n            \"storage\": storage,\n            \"search_id\": search_id,\n            \"callbacks\": [TqdmCallback()] if is_master else [],\n            \"run_function_kwargs\": run_function_kwargs,\n        },\n    )\n\n    search_kwargs[\"acq_func_kwargs\"][\"kappa\"] = kappa\n    search_kwargs[\"random_state\"] = search_random_state\n    search = CBO(problem, evaluator, log_dir=log_dir, **search_kwargs)\n\n    def dummy(*args, **kwargs):\n        pass\n\n    results = None\n    if is_master:\n        results = search.search(timeout=timeout)\n    else:\n        # for concurrency reasons this is important to override these functions\n        evaluator.dump_jobs_done_to_csv = dummy\n        search.extend_results_with_pareto_efficient = dummy\n\n        search.search(timeout=timeout)\n\n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function is very similar to our previous centralized optimization. However, importantly you can see that we\noverride two methods of the ``CBO`` with ``dummy`` function.\n\nNow we can define the decentralized search by launching centralized instances with an paralle executor.\nImportantly we use ``SharedMemoryStorage`` so that centralized sub-instances can communicate globally.\nWe also create explicitely the ``search_id`` to make sure they communicate about the search Search instance.\nThe ``kappa`` value is sampled from an Exponential distribution to enable a diverse set of exploration-exploitation trade-offs\nleading to better results. You can try to fix it to ``1.96`` instead (default parameter in DeepHyper).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def execute_decentralized_bo(\n    problem,\n    run_function,\n    run_function_kwargs,\n    num_workers,\n    log_dir,\n    search_kwargs,\n    timeout,\n    n_processes,\n):\n    storage = SharedMemoryStorage()\n    search_id = storage.create_new_search()\n    kappa = ss.expon.rvs(\n        size=n_processes, \n        scale=search_kwargs[\"acq_func_kwargs\"][\"kappa\"], \n        random_state=search_kwargs[\"random_state\"],\n    )\n\n    executor = get_reusable_executor(max_workers=n_processes, context=\"spawn\")\n    futures = []\n    for i in range(n_processes):\n        future = executor.submit(execute_centralized_bo_with_share_memory, *(\n            problem,\n            run_function,\n            run_function_kwargs,\n            storage,\n            search_id,\n            i,\n            log_dir,\n            num_workers // n_processes,\n            i == 0,\n            kappa[i],\n            search_kwargs,\n            timeout\n            )\n        )\n        futures.append(future)\n\n    results = preprocess_results(futures[0].result())\n\n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now execute the decentralized optimization with 10 processes each using 100 workers:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    results[\"decentralized\"] = execute_decentralized_bo(\n        problem=problem,\n        run_function=run_ackley,\n        run_function_kwargs=run_function_kwargs,\n        num_workers=num_workers,\n        log_dir=\"search_decentralized\",\n        search_kwargs=search_kwargs,\n        timeout=timeout,\n        n_processes=multiprocessing.cpu_count(),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the results we can see a better objective and less intance drops in worker utilization:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Make plot\nif __name__ == \"__main__\":\n    fig, axes = plt.subplots(\n        nrows=2,\n        ncols=1,\n        sharex=True,\n        figsize=figure_size(width=600),\n        tight_layout=True,\n    )\n\n    fig, ax = plot_search_trajectory_single_objective_hpo(\n        results[\"decentralized\"], mode=\"min\", x_units=\"seconds\", ax=axes[0]\n    )\n\n    fig, ax = plot_worker_utilization(\n        results[\"decentralized\"], num_workers=None, profile_type=\"start/end\", ax=axes[1]\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we compare the objective curves side by side we can see the improvement of decentralized\noptimization even better.\nEven if the total number of evaluations is less, the quality is much better as we infered more\noften the surrogate model (combining re-fitting and optimization of the acquisition function):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. dropdown:: Make plot\nif __name__ == \"__main__\":\n    fig, ax = plt.subplots(figsize=figure_size(width=600), tight_layout=True)\n    labels = {\n        \"centralized\": \"Centralized Bayesian Optimization\",\n        \"decentralized\": \"Decentralized Bayesian Optimization\",\n    }\n\n    x_min = float(\"inf\")\n    x_max = -float(\"inf\")\n    for i, (key, label) in enumerate(labels.items()):\n        df = results[key]\n        plot_search_trajectory_single_objective_hpo(\n            df,\n            show_failures=False,\n            mode=\"min\",\n            x_units=\"seconds\",\n            ax=ax,\n            label=label,\n            plot_kwargs={\"color\": f\"C{i}\"},\n            scatter_success_kwargs={\"color\": f\"C{i}\", \"alpha\": 0.5},\n        )\n        x_min = min(df[\"m:timestamp_start\"].min(), x_min)\n        x_max = max(df[\"m:timestamp_end\"].max(), x_max)\n\n    ax.set_xlim(x_min, x_max)\n\n    ax.set_xlabel(\"Time (sec.)\")\n    ax.set_ylabel(\"Objective\")\n    ax.set_yscale(\"log\")\n    ax.grid(visible=True, which=\"minor\", linestyle=\":\")\n    ax.grid(visible=True, which=\"major\", linestyle=\"-\")\n    ax.legend()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}