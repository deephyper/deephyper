{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Notify Failures in Hyperparameter optimization \n\n**Author(s)**: Romain Egele.\n\nThis example demonstrates how to handle failure of objectives in hyperparameter search. In many cases such as software auto-tuning (where we minimize the run-time of a software application) some configurations can create run-time errors and therefore no scalar objective is returned. A default choice could be to return in this case the worst case objective if known and it can be done inside the ``run``-function. Other possibilites are to ignore these configurations or to replace them with the running mean/min objective. To illustrate such a use-case we define an artificial ``run``-function which will fail when one of its input parameters is greater than 0.5. To define a failure, it is possible to return a \"string\" value with ``\"F\"`` as prefix such as:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run(config: dict) -> float:\n    if config[\"y\"] > 0.5:\n        return \"F_postfix\"\n    else:\n        return config[\"x\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define the corresponding hyperparameter problem where ``x`` is the value to maximize and ``y`` is a value impact the appearance of failures.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.problem import HpProblem\n\nproblem = HpProblem()\nproblem.add_hyperparameter([1, 2, 4, 8, 16, 32], \"x\")\nproblem.add_hyperparameter((0.0, 1.0), \"y\")\n\nprint(problem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define a centralized Bayesian optimization (CBO) search (i.e., master-worker architecture) which uses the Random-Forest regressor as default surrogate model. We will compare the ``ignore`` strategy which filters-out failed configurations, the ``mean`` strategy which replaces a failure by the running mean of collected objectives and the ``min`` strategy which replaces by the running min of collected objectives.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deephyper.search.hps import CBO\nfrom deephyper.evaluator import Evaluator\nfrom deephyper.evaluator.callback import TqdmCallback\n\nresults = {}\nmax_evals = 30\nfor failure_strategy in [\"ignore\", \"mean\", \"min\"]:\n    # for failure_strategy in [\"min\"]:\n    print(f\"Executing failure strategy: {failure_strategy}\")\n    evaluator = Evaluator.create(\n        run, method=\"serial\", method_kwargs={\"callbacks\": [TqdmCallback(max_evals)]}\n    )\n    search = CBO(\n        problem,\n        evaluator,\n        filter_failures=failure_strategy,\n        log_dir=f\"search_{failure_strategy}\",\n        random_state=42,\n    )\n    results[failure_strategy] = search.search(max_evals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we plot the collected results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.figure()\n\nfor i, (failure_strategy, df) in enumerate(results.items()):\n    plt.subplot(3, 1, i + 1)\n    if df.objective.dtype != np.float64:\n        x = np.arange(len(df))\n        mask_failed = np.where(df.objective.str.startswith(\"F\"))[0]\n        mask_success = np.where(~df.objective.str.startswith(\"F\"))[0]\n        x_success, x_failed = x[mask_success], x[mask_failed]\n        y_success = df[\"objective\"][mask_success].astype(float)\n    plt.scatter(x_success, y_success, label=failure_strategy)\n    plt.scatter(x_failed, np.zeros(x_failed.shape), marker=\"v\", color=\"red\")\n\n    plt.xlabel(r\"Iterations\")\n    plt.ylabel(r\"Objective\")\n    plt.legend()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}